{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643cdcf2",
   "metadata": {},
   "source": [
    "# Face recognition learning pipeline for VGGface dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0b71f",
   "metadata": {},
   "source": [
    "## Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "e4ae3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.onnx\n",
    "from torchinfo import summary\n",
    "from typing import Tuple\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c83f4",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229e273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить функцию по скачиванию изображений VGGface, сделать многопоточное скачивание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3909601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить функцию по выделению лиц из скачанных изображений\n",
    "# можно расширить bbox, чтобы больше лица влезло в кадр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделать обучение на vggface2\n",
    "# перебрать гиперпараметры\n",
    "# сделать learning rate annealing\n",
    "# разморозить больше слоев в сети и обучить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разобраться как работают num_workers у Dataliader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ba8cb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(research_data_loader: DataLoader) -> Tuple[tuple]:\n",
    "    '''Функция получает на вход объект класса DataLoader для \n",
    "    вычисления mean и std для всего датасета поканально\n",
    "    \n",
    "    Входные параметры:\n",
    "    research_data_loader: DataLoader - объект для загрузки изображений лиц\n",
    "    Возвращаемые значения:\n",
    "    (mean, std): Tuple[tuple] - кортеж, содержащий кортежи со значениями mean и std поканально\n",
    "    ((mean_r, mean_g, mean_b), (std_r, std_g, std_b))'''\n",
    "    nimages = 0\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for batch in research_data_loader:\n",
    "        # Приводим тензор из [B, C, W, H] к [B, C, W * H]\n",
    "        batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "        # Суммируем общее количество иобработанных изображений\n",
    "        nimages += batch.size(0)\n",
    "        # Вычисляем mean и std \n",
    "        mean += batch.mean(2).sum(0) \n",
    "        std += batch.std(2).sum(0)\n",
    "\n",
    "    mean /= nimages\n",
    "    std /= nimages\n",
    "    return tuple(mean.cpu().numpy()), tuple(std.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "6db15fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(faces_directory_path: str = None) -> pd.DataFrame:\n",
    "    '''Функция получает на вход путь к директории с лицами\n",
    "    и генерирует датафрейм, содержащий полные имена изображений \n",
    "  \n",
    "    Входные параметры:\n",
    "    faces_directory_path: str - путь к директории с лицами,\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data - dataframe, содержащий полные имена изображений'''\n",
    "\n",
    "    if not os.path.isdir(faces_directory_path):\n",
    "        raise OSError('Directory is not exist')\n",
    "\n",
    "    data = {}\n",
    "    data['face_path'] = []\n",
    "    data['face_path'] = list(glob.glob(faces_directory_path + \"/*/*\"))\n",
    "    data = pd.DataFrame(data)\n",
    "    data['person_name'] = data['face_path'].apply(lambda x: x.split('/')[-2])\n",
    "    #data = data.sort_values(by='person_name')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "53df2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_test(source_df: pd.DataFrame, separate_feature: str = None, \n",
    "                         valid_size: float = 0.2, test_size: float = 0.2) -> pd.DataFrame:\n",
    "    '''Функция разделяет source_df на три части с коэффициентами valid_size и test_size\n",
    "    по уникальным значениям separate_feature так, чтобы в новых датафреймах\n",
    "    не было строк с одинаковыми значениями из separate_feature\n",
    "\n",
    "    Входные параметры:\n",
    "    source_df: pd.DataFrame - датафрейм для разделения на train, valid и test\n",
    "    separate_feature: str - поле, по которому датафрейм будет разделен\n",
    "    valid_size: float - коэффициент разделения для valid\n",
    "    test_size: float - коэффициент разделения для test\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data_train - датафрейм для тренировки\n",
    "    pd.DataFrame: data_valid - датафрейм для валидации\n",
    "    pd.DataFrame: data_test - датафрейм для тестирования'''\n",
    "  \n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_faces, test_faces = train_test_split(source_df[separate_feature].unique(), \n",
    "                                                   test_size=(valid_size + test_size), random_state=42)\n",
    "        valid_faces, test_faces = train_test_split(test_faces, \n",
    "                                                   test_size=(test_size/(valid_size + test_size)), random_state=42)\n",
    "        \n",
    "        data_train = source_df[np.isin(source_df[separate_feature].values, train_faces)]\n",
    "        data_valid = source_df[np.isin(source_df[separate_feature].values, valid_faces)]\n",
    "        data_test = source_df[np.isin(source_df[separate_feature].values, test_faces)]\n",
    "        \n",
    "        assert source_df.shape[0] == (data_train.shape[0] + data_valid.shape[0] + data_test.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "        assert np.isin(data_train[separate_feature].values, data_test[separate_feature].values).sum() == 0\n",
    "        assert np.isin(data_valid[separate_feature].values, data_test[separate_feature].values).sum() == 0\n",
    "        \n",
    "    else:\n",
    "        data_train, data_test = train_test_split(source_df, test_size=(valid_size + test_size))\n",
    "        data_valid, data_test = train_test_split(data_test, test_size=(test_size/(valid_size + test_size)))\n",
    "        \n",
    "\n",
    "    return (data_train, data_valid, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b7398",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "8e45d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchDataset(Dataset):\n",
    "    '''Класс для создания датасета для вычисления среднего и дисперсии по каналам изображений лиц'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения и маски\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTrain'''\n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Количество изображений\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        image_tensor: torch.Tensor - тензорное представление изображения лица'''\n",
    "        \n",
    "        image_numpy = cv2.imread(self.data_info['face_path'][index])\n",
    "        image_numpy = cv2.cvtColor(image_numpy, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        image_tensor = torch.from_numpy(image_numpy)\n",
    "        image_tensor = image_tensor.to(self.device).float()\n",
    "        image_tensor = image_tensor.permute(2, 0, 1)\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d72a1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str, transform: object):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения и маски\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTrain'''\n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # \n",
    "        self.face_path_arr = self.data_info.iloc[:,0]\n",
    "        # \n",
    "        self.person_name_arr = self.data_info.iloc[:,1]\n",
    "        # Количество изображений\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения лица'''\n",
    "        \n",
    "        anchor_image = cv2.imread(self.data_info['face_path'][index])\n",
    "        anchor_person_name = self.data_info['person_name'][index]\n",
    "        \n",
    "        positive_indices = self.data_info[self.data_info['person_name'] == anchor_person_name].index.values\n",
    "        negative_indices = self.data_info[self.data_info['person_name'] != anchor_person_name].index.values\n",
    "        \n",
    "        while True:\n",
    "            positive_index = np.random.choice(positive_indices)\n",
    "            if positive_index != index:\n",
    "                break\n",
    "        \n",
    "        negative_index = np.random.choice(negative_indices)\n",
    "        \n",
    "        \n",
    "        positive_image = cv2.imread(self.face_path_arr[positive_index])\n",
    "        negative_image = cv2.imread(self.face_path_arr[negative_index])\n",
    "        \n",
    "        anchor_image = cv2.cvtColor(anchor_image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        positive_image = cv2.cvtColor(positive_image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        negative_image = cv2.cvtColor(negative_image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        tr_anchor_image = self.transform(image=anchor_image)['image'].to(self.device).float()\n",
    "        tr_positive_image = self.transform(image=positive_image)['image'].to(self.device).float()\n",
    "        tr_negative_image = self.transform(image=negative_image)['image'].to(self.device).float()\n",
    "\n",
    "        return tr_anchor_image, tr_positive_image, tr_negative_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e1120078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    '''Класс для вычисления Triplet loss для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self, margin: float):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor_embs: torch.Tensor, positive_embs: torch.Tensor, negative_embs: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        anchor_embs: torch.Tensor - тензор предсказанных эмбэддингов для якорей\n",
    "        positive_embs: torch.Tensor - тензор предсказанных эмбэддингов для позитивных примеров\n",
    "        negative_embs: torch.Tensor - тензор предсказанных эмбэддингов для негативных примеров\n",
    "        все тензоры имеют размерность B х E, где B - размер батча, E - размер вектора эмбэддинга\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        score: float - значение Triplet loss для наборов предсказанных эмбэддингов'''\n",
    "        \n",
    "        anchor_positive_distances = torch.sum((anchor_embs - positive_embs)**2, dim=1)\n",
    "        anchor_negative_distances = torch.sum((anchor_embs - negative_embs)**2, dim=1)\n",
    "        \n",
    "        print(anchor_positive_distances)\n",
    "        \n",
    "        score = torch.max(0, anchor_positive_distances - anchor_negative_distances + self.margin)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "aca5cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''Класс для работы с нейронной сетью для распознавания лиц на основе VGGface'''\n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: nn.Module - последовательность слоев или модель, через которую будут проходить данные\n",
    "        Возвращаемые значения: \n",
    "        объект класса NeuralNetwork'''\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        '''Функция прямого прохода через объкт класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения\n",
    "        Возвращаемые значения: \n",
    "        input_data: torch.Tensor - тензорное представление маски изображения'''\n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    \n",
    "    def fit(self, criterion: object, metric: object, optimizer: object, \n",
    "                  train_data_loader: DataLoader, valid_data_loader: DataLoader=None, \n",
    "                  epochs: int=1, verbose: int=50):\n",
    "        '''Метод для обучения модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        optimizer: object - оптимизатор\n",
    "        train_data_loader: DataLoader - загрузчик данных для обучения\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        epochs: int - количество эпох обучения\n",
    "        verbose: int - вывод информации через каждые verbose итераций\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        result = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss = 0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, data in enumerate(train_data_loader):\n",
    "                # data - список из 3-х тезоров с размерностями [B, C, W, H]. Сделаем конкатенацию\n",
    "                # в один тензор с размерностью [3*B, C, W, H] для одновременного прогона через сеть\n",
    "                data = torch.cat(data, dim=0)\n",
    "                data = Variable(data)       \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                # После прогона данных, разделяем результат на три соответствующих тензора\n",
    "                batch_size = int(outputs.shape[0]/3)\n",
    "                anchor_embs = outputs[:batch_size, :]\n",
    "                positive_embs = outputs[batch_size : 2*batch_size, :]\n",
    "                negative_embs = outputs[2*batch_size :, :]\n",
    "                \n",
    "                loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % verbose == 0:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {(running_loss/verbose):.6f}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spended time for {verbose} batches ({int((verbose*data.shape[0])/3)} triplets', end=\"\") \n",
    "                    print(f' or {int(verbose*data.shape[0])} images) : {(time2-time1):.6f} sec')\n",
    "                    \n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)        \n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                valid_result = self.valid(criterion, metric, valid_data_loader)\n",
    "                valid_loss = valid_result['valid_loss']\n",
    "                valid_metric = valid_result['valid_metric']\n",
    "            \n",
    "                print('='*80)\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}, valid_loss: {(valid_loss):.6f}, ', end=\"\")\n",
    "                print(f'valid_metric: {(valid_metric):.6f}')\n",
    "                print('='*80)\n",
    "            else:\n",
    "                print('='*80)\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}')\n",
    "                print('='*80)\n",
    "                valid_loss = None\n",
    "                valid_metric = None\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        result['epoch_train_losses'] = epoch_train_losses\n",
    "        result['epoch_valid_losses'] = epoch_valid_losses\n",
    "        result['epoch_valid_metrics'] = epoch_valid_metrics\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def valid(self, criterion: object, metric: object, valid_data_loader: DataLoader):\n",
    "        '''Метод для валидации модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss и метрики при валидации'''\n",
    "        self.model.eval()\n",
    "        valid_metrics = []\n",
    "        valid_losses = []\n",
    "        result = {}\n",
    "        for batch_idx, data in enumerate(valid_data_loader):\n",
    "            data = torch.cat(data, dim=0)\n",
    "            data = Variable(data)\n",
    "            \n",
    "            outputs = self.model(data)\n",
    "                    \n",
    "            batch_size = int(outputs.shape[0]/3)\n",
    "            anchor_embs = outputs[:batch_size, :]\n",
    "            positive_embs = outputs[batch_size : 2*batch_size, :]\n",
    "            negative_embs = outputs[2*batch_size :, :]\n",
    "            \n",
    "            loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "            valid_losses.append(loss.item())\n",
    "                    \n",
    "            metric_value = metric(anchor_embs, positive_embs, negative_embs)\n",
    "            valid_metrics.append(metric_value.item())\n",
    "                \n",
    "        valid_loss    = np.mean(valid_losses)\n",
    "        valid_metric  = np.mean(valid_metrics)\n",
    "        result['valid_loss'] = valid_loss\n",
    "        result['valid_metric'] = valid_metric\n",
    "        return result\n",
    "\n",
    "    \n",
    "    \n",
    "    def save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения весов модели\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения состояния модели'''\n",
    "        torch.save(self.model.state_dict(), path_to_save)\n",
    "    \n",
    "    def trace_save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения модели через torchscript\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.rand(1, 3, 512, 512).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "            \n",
    "        traced_model = torch.jit.trace((self.model).eval(), example_forward_input)\n",
    "        torch.jit.save(traced_model, path_to_save)\n",
    "    \n",
    "    def onnx_save(self, path_to_save: str='./carvana_model.onnx'):\n",
    "        '''Метод сохранения модели в формате ONNX\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.randn(1, 3, 1024, 1024, requires_grad=True).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "\n",
    "        torch.onnx.export(self.model,\n",
    "                          example_forward_input,\n",
    "                          path_to_save,\n",
    "                          export_params=True,\n",
    "                          opset_version=10,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'],\n",
    "                          dynamic_axes={'input' : {0 : 'batch_size'},    # Модель будет работать с произвольным\n",
    "                                        'output' : {0 : 'batch_size'}})  # размером батча\n",
    "    \n",
    "    def load(self, path_to_model: str='./model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "b7732612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собственная модель с нуля\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.alobal_average = nn.AvgPool2d(5)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "        \n",
    "        x = self.alobal_average(x)\n",
    "        \n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.dropout1(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "cfffdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    '''Класс необходим для реализации transfer learning.\n",
    "    Если создать объект этого класса и присвоить его последнему слою классификатора\n",
    "    обученной нейросети, то классификатор перезапишется, сигнал будет проходить без изменений, \n",
    "    и в дальнейшем можно будет добавлять пользовательские слои'''\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "07ba78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постороение модели на основе mobilenet_v3_small\n",
    "class MobileImagenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.backbone.classifier[-1] = Identity()\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "\n",
    "    def get_layers_names(self):\n",
    "         return dict(self.backbone.named_modules())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = F.relu(self.dropout1(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf10be",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "f5411c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "21706550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "80e5e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 20 15:50:52 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.82.00    Driver Version: 470.82.00    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   40C    P8     9W / 170W |   2279MiB / 12045MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       945      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1650      G   /usr/lib/xorg/Xorg                153MiB |\r\n",
      "|    0   N/A  N/A      1777      G   /usr/bin/gnome-shell               26MiB |\r\n",
      "|    0   N/A  N/A      2542      G   /usr/lib/firefox/firefox          135MiB |\r\n",
      "|    0   N/A  N/A      2716      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      2777      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      2806      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      2818      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      2823      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      2830      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      2839      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      5511      C   ...dima/anaconda3/bin/python     1897MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "60163a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "num_epochs = 30\n",
    "margin = 0.1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "b65bd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_directory_path = '/home/dima/datasets/vgg_face_dataset/faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "0a7ee883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15228, 2)"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_data_csv(faces_directory_path)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "751d08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = get_train_valid_test(source_df=dataset, separate_feature='person_name')\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "test_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "2812543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique person names in train: 120\n",
      "Unique person names in valid: 40\n",
      "Unique person names in test: 40\n",
      "Total images in train: 9014\n",
      "Total images in valid: 3146\n",
      "Total images in test: 3068\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique person names in train: {train_df['person_name'].nunique()}\")\n",
    "print(f\"Unique person names in valid: {valid_df['person_name'].nunique()}\")\n",
    "print(f\"Unique person names in test: {test_df['person_name'].nunique()}\")\n",
    "print(f\"Total images in train: {train_df.shape[0]}\")\n",
    "print(f\"Total images in valid: {valid_df.shape[0]}\")\n",
    "print(f\"Total images in test: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "9ff19011",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_data = ResearchDataset(train_df, device)\n",
    "research_data_loader = DataLoader(research_data, batch_size=64, shuffle=True)\n",
    "mean, std = get_mean_std(research_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "ed33bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean channel values in train: (0.654159, 0.49055982, 0.41564453)\n",
      "Std channel values in train: (0.20767309, 0.1787051, 0.16775697)\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean channel values in train: {mean}')\n",
    "print(f'Std channel values in train: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "3dc971b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробовать обучить модель со стандартизацией и без и сравнить качество\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=mean, std=std, max_pixel_value=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_test_transform = A.Compose([\n",
    "    A.Normalize(mean=mean, std=std, max_pixel_value=1.0),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "a8a8affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнить производительность при разных num_workers\n",
    "train_data = CustomDataset(train_df, device, train_transform)\n",
    "valid_data = CustomDataset(valid_df, device, valid_test_transform)\n",
    "test_data = CustomDataset(test_df, device, valid_test_transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "0161aaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "NeuralNetwork                                           --                        --\n",
       "├─MobileImagenet: 1-1                                   [32, 128]                 --\n",
       "│    └─MobileNetV3: 2-1                                 [32, 1024]                --\n",
       "│    │    └─Sequential: 3-1                             [32, 576, 5, 5]           (927,008)\n",
       "│    │    └─AdaptiveAvgPool2d: 3-2                      [32, 576, 1, 1]           --\n",
       "│    │    └─Sequential: 3-3                             [32, 1024]                (590,848)\n",
       "│    └─Linear: 2-2                                      [32, 256]                 262,400\n",
       "│    └─Dropout: 2-3                                     [32, 256]                 --\n",
       "│    └─Linear: 2-4                                      [32, 128]                 32,896\n",
       "=========================================================================================================\n",
       "Total params: 1,813,152\n",
       "Trainable params: 295,296\n",
       "Non-trainable params: 1,517,856\n",
       "Total mult-adds (M): 932.30\n",
       "=========================================================================================================\n",
       "Input size (MB): 9.83\n",
       "Forward/backward pass size (MB): 370.15\n",
       "Params size (MB): 7.25\n",
       "Estimated Total Size (MB): 387.23\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model from scratch\n",
    "#model = Model().to(device)\n",
    "\n",
    "# transfer learning with mobilenet v3 (imagenet)\n",
    "#model = MobileImagenet().to(device)\n",
    "\n",
    "\n",
    "network = NeuralNetwork(model=model)\n",
    "summary(network, input_size=(32, 3, 160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "e3a7dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = TripletLoss(margin=margin)\n",
    "#metric = TripletLoss(margin=margin)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "metric = nn.TripletMarginLoss(margin=1.0, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "f682da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.830728\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.329509 sec\n",
      "Train Epoch: 1, Loss: 0.621594\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.395612 sec\n",
      "Train Epoch: 1, Loss: 0.552309\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.382660 sec\n",
      "Train Epoch: 1, Loss: 0.541826\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.415146 sec\n",
      "Train Epoch: 1, Loss: 0.473931\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.341105 sec\n",
      "================================================================================\n",
      "Epoch 1, train loss: 0.588289, valid_loss: 0.576978, valid_metric: 0.576978\n",
      "================================================================================\n",
      "Train Epoch: 2, Loss: 0.458152\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.367517 sec\n",
      "Train Epoch: 2, Loss: 0.444278\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.369329 sec\n",
      "Train Epoch: 2, Loss: 0.413906\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.255110 sec\n",
      "Train Epoch: 2, Loss: 0.437775\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.243630 sec\n",
      "Train Epoch: 2, Loss: 0.392071\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.359866 sec\n",
      "================================================================================\n",
      "Epoch 2, train loss: 0.429903, valid_loss: 0.524629, valid_metric: 0.524629\n",
      "================================================================================\n",
      "Train Epoch: 3, Loss: 0.432708\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.270117 sec\n",
      "Train Epoch: 3, Loss: 0.396835\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.319750 sec\n",
      "Train Epoch: 3, Loss: 0.379479\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.289057 sec\n",
      "Train Epoch: 3, Loss: 0.362228\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.291529 sec\n",
      "Train Epoch: 3, Loss: 0.373464\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.232668 sec\n",
      "================================================================================\n",
      "Epoch 3, train loss: 0.388804, valid_loss: 0.472406, valid_metric: 0.472406\n",
      "================================================================================\n",
      "Train Epoch: 4, Loss: 0.397810\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.410184 sec\n",
      "Train Epoch: 4, Loss: 0.366750\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.322946 sec\n",
      "Train Epoch: 4, Loss: 0.392652\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.259996 sec\n",
      "Train Epoch: 4, Loss: 0.393495\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.263047 sec\n",
      "Train Epoch: 4, Loss: 0.374362\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.254674 sec\n",
      "================================================================================\n",
      "Epoch 4, train loss: 0.376874, valid_loss: 0.455530, valid_metric: 0.455530\n",
      "================================================================================\n",
      "Train Epoch: 5, Loss: 0.347745\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.257472 sec\n",
      "Train Epoch: 5, Loss: 0.365508\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.322301 sec\n",
      "Train Epoch: 5, Loss: 0.339488\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.239879 sec\n",
      "Train Epoch: 5, Loss: 0.361557\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.308492 sec\n",
      "Train Epoch: 5, Loss: 0.337174\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.324846 sec\n",
      "================================================================================\n",
      "Epoch 5, train loss: 0.343518, valid_loss: 0.506638, valid_metric: 0.506638\n",
      "================================================================================\n",
      "Train Epoch: 6, Loss: 0.333522\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.286819 sec\n",
      "Train Epoch: 6, Loss: 0.356189\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.237678 sec\n",
      "Train Epoch: 6, Loss: 0.334226\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.249480 sec\n",
      "Train Epoch: 6, Loss: 0.334434\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.295747 sec\n",
      "Train Epoch: 6, Loss: 0.337817\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.398249 sec\n",
      "================================================================================\n",
      "Epoch 6, train loss: 0.342181, valid_loss: 0.507194, valid_metric: 0.507194\n",
      "================================================================================\n",
      "Train Epoch: 7, Loss: 0.324914\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.270488 sec\n",
      "Train Epoch: 7, Loss: 0.339803\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.308818 sec\n",
      "Train Epoch: 7, Loss: 0.328572\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.300445 sec\n",
      "Train Epoch: 7, Loss: 0.335214\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.254964 sec\n",
      "Train Epoch: 7, Loss: 0.359212\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.242840 sec\n",
      "================================================================================\n",
      "Epoch 7, train loss: 0.338569, valid_loss: 0.476250, valid_metric: 0.476250\n",
      "================================================================================\n",
      "Train Epoch: 8, Loss: 0.273957\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.311199 sec\n",
      "Train Epoch: 8, Loss: 0.302789\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.345737 sec\n",
      "Train Epoch: 8, Loss: 0.311795\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.264570 sec\n",
      "Train Epoch: 8, Loss: 0.322776\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.299508 sec\n",
      "Train Epoch: 8, Loss: 0.279578\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.246320 sec\n",
      "================================================================================\n",
      "Epoch 8, train loss: 0.300468, valid_loss: 0.476985, valid_metric: 0.476985\n",
      "================================================================================\n",
      "Train Epoch: 9, Loss: 0.318052\n",
      "Spended time for 50 batches (1600 triplets or 4800 images) : 5.245173 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5511/2224270366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result_train = network.fit(criterion,\n\u001b[0m\u001b[1;32m      2\u001b[0m              \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5511/2175206537.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, criterion, metric, optimizer, train_data_loader, valid_data_loader, epochs, verbose)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# data - список из 3-х тезоров с размерностями [B, C, W, H]. Сделаем конкатенацию\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# в один тензор с размерностью [3*B, C, W, H] для одновременного прогона через сеть\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5511/2479000937.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0manchor_person_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mpositive_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manchor_person_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mnegative_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0manchor_person_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3444\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3448\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3627\u001b[0m         \"\"\"\n\u001b[0;32m-> 3628\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3629\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3613\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3615\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3616\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3617\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m    678\u001b[0m             )\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             new_blocks = [\n\u001b[0m\u001b[1;32m    681\u001b[0m                 blk.take_nd(\n\u001b[1;32m    682\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             new_blocks = [\n\u001b[0;32m--> 681\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m    682\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1146\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mflip_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mflip_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_train = network.fit(criterion,\n",
    "             metric,\n",
    "             optimizer,\n",
    "             train_data_loader,\n",
    "             valid_data_loader,\n",
    "             epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7756c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "c99324b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.28777003751108143, 'valid_metric': 0.28777003751108143}"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_valid = network.valid(criterion, metric, valid_data_loader)\n",
    "result_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "893bec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.4207154152294, 'valid_metric': 0.4207154152294}"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test = network.valid(criterion, metric, test_data_loader)\n",
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "78328996",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save(path_to_save='./my_model_21ep_0.3test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "5d3fb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork(model=model)\n",
    "network.load(path_to_model = './my_model_21ep_0.3test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8acba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16c309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
