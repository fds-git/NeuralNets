{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643cdcf2",
   "metadata": {},
   "source": [
    "# Face recognition learning pipeline for VGGface dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0b71f",
   "metadata": {},
   "source": [
    "## Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ae3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.onnx\n",
    "from torchinfo import summary\n",
    "from typing import Tuple\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from facenet_pytorch import MTCNN\n",
    "import glob\n",
    "import urllib\n",
    "from multiprocessing import Pool\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c83f4",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8cb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(research_data_loader: DataLoader) -> Tuple[tuple]:\n",
    "    '''Функция получает на вход объект класса DataLoader для \n",
    "    вычисления mean и std для всего датасета поканально\n",
    "    Входные параметры:\n",
    "    research_data_loader: DataLoader - объект для загрузки изображений лиц\n",
    "    Возвращаемые значения:\n",
    "    (mean, std): Tuple[tuple] - кортеж, содержащий кортежи со значениями mean и std поканально\n",
    "    ((mean_r, mean_g, mean_b), (std_r, std_g, std_b))'''\n",
    "    \n",
    "    nimages = 0\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for batch in research_data_loader:\n",
    "        # Приводим тензор из [B, C, W, H] к [B, C, W * H]\n",
    "        batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "        # Суммируем общее количество иобработанных изображений\n",
    "        nimages += batch.size(0)\n",
    "        # Вычисляем mean и std\n",
    "        mean += batch.mean(2).sum(0)\n",
    "        std += batch.std(2).sum(0)\n",
    "\n",
    "    mean /= nimages\n",
    "    std /= nimages\n",
    "    return tuple(mean.cpu().numpy()), tuple(std.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db15fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(faces_directory_path: str = None) -> pd.DataFrame:\n",
    "    '''Функция получает на вход путь к директории с лицами\n",
    "    и генерирует датафрейм, содержащий полные имена изображений \n",
    "    Входные параметры:\n",
    "    faces_directory_path: str - путь к директории с лицами,\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data - dataframe, содержащий полные имена изображений'''\n",
    "\n",
    "    if not os.path.isdir(faces_directory_path):\n",
    "        raise OSError('Directory is not exist')\n",
    "\n",
    "    data = {}\n",
    "    data['face_path'] = []\n",
    "    data['face_path'] = list(glob.glob(faces_directory_path + \"/*/*\"))\n",
    "    data = pd.DataFrame(data)\n",
    "    data['person_name'] = data['face_path'].apply(lambda x: x.split('/')[-2])\n",
    "    #data = data.sort_values(by='person_name')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53df2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_test(source_df: pd.DataFrame, separate_feature: str = None, \n",
    "                         valid_size: float = 0.2, test_size: float = 0.2) -> pd.DataFrame:\n",
    "    '''Функция разделяет source_df на три части с коэффициентами valid_size и test_size\n",
    "    по уникальным значениям separate_feature так, чтобы в новых датафреймах\n",
    "    не было строк с одинаковыми значениями из separate_feature\n",
    "    Входные параметры:\n",
    "    source_df: pd.DataFrame - датафрейм для разделения на train, valid и test\n",
    "    separate_feature: str - поле, по которому датафрейм будет разделен\n",
    "    valid_size: float - коэффициент разделения для valid\n",
    "    test_size: float - коэффициент разделения для test\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data_train - датафрейм для тренировки\n",
    "    pd.DataFrame: data_valid - датафрейм для валидации\n",
    "    pd.DataFrame: data_test - датафрейм для тестирования'''\n",
    "\n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_faces, test_faces = train_test_split(source_df[separate_feature].unique(), \n",
    "                                                   test_size=(valid_size + test_size), random_state=42)\n",
    "        valid_faces, test_faces = train_test_split(test_faces, \n",
    "                                                   test_size=(test_size/(valid_size + test_size)), random_state=42)\n",
    "        \n",
    "        data_train = source_df[np.isin(source_df[separate_feature].values, train_faces)]\n",
    "        data_valid = source_df[np.isin(source_df[separate_feature].values, valid_faces)]\n",
    "        data_test = source_df[np.isin(source_df[separate_feature].values, test_faces)]\n",
    "        \n",
    "        assert source_df.shape[0] == (data_train.shape[0] + data_valid.shape[0] + data_test.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "        assert np.isin(data_train[separate_feature].values, data_test[separate_feature].values).sum() == 0\n",
    "        assert np.isin(data_valid[separate_feature].values, data_test[separate_feature].values).sum() == 0\n",
    "\n",
    "    else:\n",
    "        data_train, data_test = train_test_split(source_df, test_size=(valid_size + test_size))\n",
    "        data_valid, data_test = train_test_split(data_test, test_size=(test_size/(valid_size + test_size)))\n",
    "\n",
    "\n",
    "    return (data_train, data_valid, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b7398",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5184e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGface2Data(object):\n",
    "    '''Класс для предобработки данных датасета VGGFace2'''\n",
    "    def __init__(self, images_directory: str, faces_directory: str):\n",
    "        '''Конструктор класса VGGface2Data\n",
    "        Входные параметры:\n",
    "        images_directory: str - директория, в которой будут храниться скачанные изображения людей\n",
    "        faces_directory: str - директория, в которой будут храниться лица, выделенный из изображений\n",
    "        Возвращаемые значения:\n",
    "        объект класса VGGface1Data'''\n",
    "        \n",
    "        if ((not os.path.isdir(images_directory)) or (not os.path.isdir(faces_directory))):\n",
    "            raise OSError('Directory is not exist')\n",
    "    \n",
    "        self.images_directory = images_directory\n",
    "        self.faces_directory = faces_directory\n",
    "\n",
    "\n",
    "    def get_faces_from_mtcnn(self, out_face_size: tuple = (160, 160), in_faces_size_treashold: tuple = (150, 150), \n",
    "                        face_prob_treashold: float = 0.99, face_ratio_treashold: float = 1.5):\n",
    "        '''Метод получения из изображений с людьми в images_directory изображений лиц и сохранения их\n",
    "        в faces_directory с помощью детектора лиц mtcnn\n",
    "        Входные параметры:\n",
    "        images_directory: str - директория с изображениями людей (включает в себя поддиректории - каждая для\n",
    "        отдельного человека)\n",
    "        faces_directory: str - директория с изображениями лиц людей (включает в себя поддиректории - каждая для\n",
    "        отдельного человека)\n",
    "        out_face_size: tuple - размер, к которому будут приведены изображения лиц перед сохранением в faces_directory\n",
    "        in_faces_size_treashold: tuple - порог размера лица на исходном изображении, по которому будут фильтроваться\n",
    "        изображения лиц перед сохранением в faces_directory\n",
    "        face_prob_treashold: float - порог уверенности сети в обнаружении лица на исходном изображении, \n",
    "        по которому будут фильтроваться изображения лиц перед сохранением в faces_directory\n",
    "        face_ratio_treashold: float - порог соотношения сторон изображения лица, по которому будут фильтроваться \n",
    "        изображения лиц перед сохранением в faces_directory'''\n",
    "\n",
    "        persons_list = os.listdir(self.images_directory)\n",
    "        mtcnn = MTCNN(keep_all=True, device='cuda:0')\n",
    "        for person in persons_list:\n",
    "            os.mkdir(self.faces_directory + person)\n",
    "            images_list = list(glob.glob(self.images_directory + person + \"/*\"))\n",
    "            for image in images_list:\n",
    "                try:\n",
    "                    image_numpy = cv2.imread(image)\n",
    "                    rgb_image_numpy = cv2.cvtColor(image_numpy, cv2.COLOR_BGR2RGB)\n",
    "                    boxes, probs = mtcnn.detect(rgb_image_numpy, landmarks=False)\n",
    "                    # Если на изображении несколько лиц или сеть не уверена в обнаружении лица, \n",
    "                    # не обрабатываем это изображение\n",
    "                    if (len(boxes) != 1) or (probs[0] < face_prob_treashold):\n",
    "                        continue\n",
    "                    box = boxes[0]\n",
    "                    left, top, right, bottom = (int(box[0])), (int(box[1])), (int(box[2])), (int(box[3]))\n",
    "                    face_numpy = image_numpy[top:bottom, left:right, :]\n",
    "\n",
    "                    # Если размер лица на исходном изображении мал, то не обрабатываем это изображение\n",
    "                    # (избавляемся от шакалистых лиц)\n",
    "                    if ((face_numpy.shape[0] < in_faces_size_treashold[0]) or \n",
    "                                        (face_numpy.shape[1] < in_faces_size_treashold[1])):\n",
    "                        continue\n",
    "\n",
    "                    # Бывает, сеть принимает за лицо рандомные прямоугольники, избавляемся от них\n",
    "                    if ((face_numpy.shape[0]/face_numpy.shape[1] > face_ratio_treashold) or \n",
    "                                    (face_numpy.shape[1]/face_numpy.shape[0] > face_ratio_treashold)):\n",
    "                        continue\n",
    "\n",
    "                    face_numpy = cv2.resize(face_numpy, out_face_size, interpolation = cv2.INTER_LINEAR).astype('float')\n",
    "\n",
    "                    cv2.imwrite(self.faces_directory + person + '/' + image.split('/')[-1], face_numpy)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4500e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGface1Data(VGGface2Data):\n",
    "    '''Класс для загрузки и предобработки данных датасета VGGFace1 на основе класса VGGface2Data\n",
    "    так как в новом классе нам нужен метод get_faces_from_mtcnn'''\n",
    "    def __init__(self, urls_directory: str, images_directory: str, faces_directory: str):\n",
    "        '''Конструктор класса VGGface1Data\n",
    "        Входные параметры:\n",
    "        urls_directory: str - директория, в которой находятся текстовые файлы (для каждого человека свой),\n",
    "        в которых хранятся url изображений соответствующих людей\n",
    "        images_directory: str - директория, в которой будут храниться скачанные изображения людей\n",
    "        faces_directory: str - директория, в которой будут храниться лица, выделенный из изображений\n",
    "        Возвращаемые значения:\n",
    "        объект класса VGGface1Data'''\n",
    "        \n",
    "        VGGface2Data.__init__(self, images_directory, faces_directory)\n",
    "        \n",
    "        if not os.path.isdir(urls_directory):\n",
    "            raise OSError('Directory is not exist')\n",
    "                             \n",
    "        self.urls_directory = urls_directory\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def __checkurl(params: list):\n",
    "        '''Статический метод для запроса изображения по url и сохранения его в целевую директорию.\n",
    "        Используется при вызове функции map модуля multiprocessing\n",
    "        Входные параметры:\n",
    "        params[0]: str - url, по которому находится изображение человека\n",
    "        params[1]: str - директория, в которую необходимо сохранить скачанное изображение человека\n",
    "        params[2]: str - имя человека, для которого скачивается изображение\n",
    "        params[3]: str - индекс изображения для данного человека\n",
    "        params[4]: str - координаты bbox'а лица на изображении данного человека'''\n",
    "        \n",
    "        image_url = params[0]\n",
    "        images_directory = params[1]\n",
    "        person_name = params[2]\n",
    "        index = params[3]\n",
    "        bbox = params[4]\n",
    "\n",
    "        try:\n",
    "            print(f'Trying download {image_url}')\n",
    "            with urllib.request.urlopen(image_url, timeout=5) as f:\n",
    "                image = f.read()\n",
    "            with open(images_directory + person_name + '/' + str(index) + ' ' + str(bbox) + '.jpg', 'wb') as binary_file:\n",
    "                binary_file.write(image)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "              \n",
    "                \n",
    "    def download(self, total_persons: int = 200, total_images_for_person: int = 200, processes: int = 20):\n",
    "        '''Метод скачивания изображений людей в self.images_directory с помощью запросов к url,\n",
    "        которые хранятся в self.urls_directory и сагрегированы по txt файлам (для каждого человека свой файл,\n",
    "        хранящий 1000 url к соответствующим изображениям)\n",
    "        Входные параметры:\n",
    "        total_persons: int - общее количесво людей, для которых будут скачиваться изображения\n",
    "        total_images_for_person: int - общее количество запросов, которые будут выполнены для каждого человека\n",
    "        processes: int - количество процессов для загрузки изображений\n",
    "        Возвращаемые значения:\n",
    "        total_time: float - время, затраченное на загрузку и сохранение изображений'''\n",
    "\n",
    "        total_persons = int(total_persons)\n",
    "        if total_persons < 1:\n",
    "            total_persons = 1\n",
    "        if total_persons > 1000:\n",
    "            total_persons = 1000\n",
    "\n",
    "        total_images_for_person = int(total_images_for_person)\n",
    "        if total_images_for_person < 1:\n",
    "            total_images_for_person = 1\n",
    "        if total_images_for_person > 1000:\n",
    "            total_images_for_person = 1000\n",
    "\n",
    "        persons_list = list(glob.glob(self.urls_directory + \"*\"))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(total_persons):\n",
    "            # Создаем папку с именем человека, в которой будем хранить его изображения\n",
    "            person_name = persons_list[i].split('.')[0].split('/')[-1]\n",
    "            os.mkdir(self.images_directory + person_name)\n",
    "            # Считываем содержимое текстового файла и парсим его\n",
    "            file_with_urls = open(persons_list[i], \"r\")\n",
    "            \n",
    "            info = file_with_urls.readlines()\n",
    "            urls = list(map(lambda x: x.split(' ')[1], info))\n",
    "            indexes = list(map(lambda x: x.split(' ')[0], info))\n",
    "            bboxes = list(map(lambda x: x.split(' ')[2:], info))\n",
    "            bboxes = list(map(lambda x: ' '.join(x), bboxes))\n",
    "\n",
    "            urls = urls[: total_images_for_person]\n",
    "            indexes = indexes[: total_images_for_person]\n",
    "            bboxes = bboxes[: total_images_for_person]\n",
    "\n",
    "            # Список из одинаковых элементов необходим для правильной работы функции p.map\n",
    "            images_directory_arr = [self.images_directory] * total_images_for_person\n",
    "            person_name_arr = [person_name] * total_images_for_person\n",
    "\n",
    "            p = Pool(processes=processes)\n",
    "            result = p.map(VGGface1Data.__checkurl, list(zip(urls, images_directory_arr, person_name_arr, \n",
    "                                                           indexes, bboxes)))\n",
    "\n",
    "            file_with_urls.close\n",
    "\n",
    "        stop_time = time.time()\n",
    "        total_time = stop_time - start_time\n",
    "\n",
    "        return total_time\n",
    "\n",
    "    \n",
    "    def get_faces_from_bbox(self, out_face_size: tuple = (160, 160), in_faces_size_treashold: tuple = (150, 150), \n",
    "                  face_ratio_treashold: float = 1.5):\n",
    "        '''Метод получения из изображений с людьми в images_directory изображений лиц с использованием\n",
    "        известных координат bbox и сохранения изображений лиц в faces_directory\n",
    "        Входные параметры:\n",
    "        out_face_size: tuple - размер, к которому будут приведены изображения лиц перед сохранением в faces_directory\n",
    "        in_faces_size_treashold: tuple - порог размера лица на исходном изображении, по которому будут фильтроваться\n",
    "        изображения лиц перед сохранением в faces_directory\n",
    "        face_ratio_treashold: float - порог соотношения сторон изображения лица, по которому будут фильтроваться \n",
    "        изображения лиц перед сохранением в faces_directory'''\n",
    "\n",
    "        persons_list = os.listdir(self.images_directory)\n",
    "        for person in persons_list:\n",
    "            os.mkdir(self.faces_directory + person)\n",
    "            images_list = list(glob.glob(self.images_directory + person + \"/*\"))\n",
    "            for image in images_list:\n",
    "                try:\n",
    "                    image_numpy = cv2.imread(image)\n",
    "                    rgb_image_numpy = cv2.cvtColor(image_numpy, cv2.COLOR_BGR2RGB)\n",
    "                    left = int(float(image.split(' ')[1]))\n",
    "                    top = int(float(image.split(' ')[2]))\n",
    "                    right = int(float(image.split(' ')[3]))\n",
    "                    bottom = int(float(image.split(' ')[4]))\n",
    "\n",
    "                    face_numpy = image_numpy[top:bottom, left:right, :]\n",
    "\n",
    "                    # Если размер лица на исходном изображении мал, то не обрабатываем это изображение\n",
    "                    # (избавляемся от шакалистых лиц)\n",
    "                    if ((face_numpy.shape[0] < in_faces_size_treashold[0]) or \n",
    "                                        (face_numpy.shape[1] < in_faces_size_treashold[1])):\n",
    "                        continue\n",
    "\n",
    "                    # Бывает, сеть принимает за лицо рандомные прямоугольники, избавляемся от них\n",
    "                    if ((face_numpy.shape[0]/face_numpy.shape[1] > face_ratio_treashold) or \n",
    "                                    (face_numpy.shape[1]/face_numpy.shape[0] > face_ratio_treashold)):\n",
    "                        continue \n",
    "\n",
    "                    face_numpy = cv2.resize(face_numpy, out_face_size, interpolation = cv2.INTER_LINEAR).astype('float')\n",
    "\n",
    "                    cv2.imwrite(self.faces_directory + person + '/' + image.split('/')[-1], face_numpy)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e45d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchDataset(Dataset):\n",
    "    '''Класс для создания датасета для вычисления среднего и дисперсии по каналам изображений лиц'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTrain'''\n",
    "        \n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Количество изображений\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        index: int - индекс для обращения к элементам датафрейма self.data_info\n",
    "        Возвращаемые значения:\n",
    "        image_tensor: torch.Tensor - тензорное представление изображения лица'''\n",
    "        \n",
    "        image_numpy = cv2.imread(self.data_info['face_path'][index])\n",
    "        image_numpy = cv2.cvtColor(image_numpy, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        image_tensor = torch.from_numpy(image_numpy)\n",
    "        image_tensor = image_tensor.to(self.device).float()\n",
    "        image_tensor = image_tensor.permute(2, 0, 1)\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72a1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str, transform: object):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения и маски\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDataset'''\n",
    "        \n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        self.face_path_arr = self.data_info.iloc[:,0]\n",
    "        self.person_name_arr = self.data_info.iloc[:,1]\n",
    "        # Количество изображений\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        index: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        Tuple[torch.Tensor] - кортеж из тензорных представлений изображений лиц (якоря, позитивного\n",
    "        и негативного примеров)'''\n",
    "        \n",
    "        anchor_image = cv2.imread(self.data_info['face_path'][index])\n",
    "        anchor_person_name = self.data_info['person_name'][index]\n",
    "        positive_indices = self.data_info[self.data_info['person_name'] == anchor_person_name].index.values\n",
    "        negative_indices = self.data_info[self.data_info['person_name'] != anchor_person_name].index.values\n",
    "        \n",
    "        while True:\n",
    "            positive_index = np.random.choice(positive_indices)\n",
    "            if positive_index != index:\n",
    "                break\n",
    "        \n",
    "        positive_index = np.random.choice(positive_indices)\n",
    "        negative_index = np.random.choice(negative_indices)\n",
    "        \n",
    "        positive_image = cv2.imread(self.face_path_arr[positive_index])\n",
    "        negative_image = cv2.imread(self.face_path_arr[negative_index])\n",
    "        \n",
    "        anchor_image = cv2.cvtColor(anchor_image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        positive_image = cv2.cvtColor(positive_image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        negative_image = cv2.cvtColor(negative_image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        tr_anchor_image = self.transform(image=anchor_image)['image'].to(self.device).float()\n",
    "        tr_positive_image = self.transform(image=positive_image)['image'].to(self.device).float()\n",
    "        tr_negative_image = self.transform(image=negative_image)['image'].to(self.device).float()\n",
    "\n",
    "        return tr_anchor_image, tr_positive_image, tr_negative_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1120078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    '''Класс для вычисления Triplet loss для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self, margin: float, device: str):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, anchor_embs: torch.Tensor, positive_embs: torch.Tensor, negative_embs: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        anchor_embs: torch.Tensor - тензор предсказанных эмбэддингов для якорей\n",
    "        positive_embs: torch.Tensor - тензор предсказанных эмбэддингов для позитивных примеров\n",
    "        negative_embs: torch.Tensor - тензор предсказанных эмбэддингов для негативных примеров\n",
    "        все тензоры имеют размерность B х E, где B - размер батча, E - размер вектора эмбэддинга\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение Triplet loss для наборов предсказанных эмбэддингов'''\n",
    "        \n",
    "        anchor_positive_distances = torch.sum((anchor_embs - positive_embs)**2, dim=1)\n",
    "        anchor_negative_distances = torch.sum((anchor_embs - negative_embs)**2, dim=1)\n",
    "        \n",
    "        score = torch.max(torch.tensor([0.]).to(self.device), \n",
    "                        anchor_positive_distances - anchor_negative_distances + \n",
    "                        torch.tensor([self.margin]).to(self.device))\n",
    "        \n",
    "        score = torch.mean(score)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14ce54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionMetric(nn.Module):\n",
    "    '''Класс для вычисления метрики для задачи face recognition для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self):\n",
    "        super(RecognitionMetric, self).__init__()\n",
    "\n",
    "        \n",
    "    def forward(self, anchor_embs: torch.Tensor, positive_embs: torch.Tensor, \n",
    "                negative_embs: torch.Tensor) -> List[float]:\n",
    "        '''Входные параметры:\n",
    "        anchor_embs: torch.Tensor - тензор предсказанных эмбэддингов для якорей\n",
    "        positive_embs: torch.Tensor - тензор предсказанных эмбэддингов для позитивных примеров\n",
    "        negative_embs: torch.Tensor - тензор предсказанных эмбэддингов для негативных примеров\n",
    "        все тензоры имеют размерность B х E, где B - размер батча, E - размер вектора эмбэддинга\n",
    "        Возвращаемые значения:\n",
    "        score: List[float] - список, содержащий два значения:\n",
    "            1 - среднее по батчу l2 расстояние от anchor_embs до positive_embs\n",
    "            2 - среднее по батчу l2 расстояние от anchor_embs до negative_embs'''\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # для каждого триплета в батче получаем l2 расстояния от якоря до позитивного и от \n",
    "            # якоря до негативного элемента\n",
    "            anchor_positive_distances = (torch.sum((anchor_embs - positive_embs)**2, dim=1))**0.5\n",
    "            anchor_negative_distances = (torch.sum((anchor_embs - negative_embs)**2, dim=1))**0.5\n",
    "            # Проводим усреднения по батчу\n",
    "        return (torch.mean(anchor_positive_distances).item(), torch.mean(anchor_negative_distances).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aca5cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(nn.Module):\n",
    "    '''Класс, реализующий функционал для работы с нейронной сетью для распознавания лиц на основе VGGface'''\n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: nn.Module - последовательность слоев или модель, через которую будут проходить данные\n",
    "        Возвращаемые значения: \n",
    "        объект класса ModelWrapper'''\n",
    "        \n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        '''Метод прямого прохода через объект класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения лица\n",
    "        Возвращаемые значения: \n",
    "        output_data: torch.Tensor - эмбэддинг лица в тензорном формате'''\n",
    "        \n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    \n",
    "    def fit(self, criterion: object, metric: object, optimizer: object, scheduler: object,\n",
    "                  train_data_loader: DataLoader, valid_data_loader: DataLoader=None, \n",
    "                  epochs: int=1, verbose: int=50):\n",
    "        '''Метод для обучения модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        optimizer: object - оптимизатор\n",
    "        scheduler: object - \n",
    "        train_data_loader: DataLoader - загрузчик данных для обучения\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        epochs: int - количество эпох обучения\n",
    "        verbose: int - вывод информации через каждые verbose итераций\n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        result = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss = 0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, data in enumerate(train_data_loader):\n",
    "                # data - список из 3-х тезоров с размерностями [B, C, W, H]. Сделаем конкатенацию\n",
    "                # в один тензор с размерностью [3*B, C, W, H] для одновременного прогона через сеть\n",
    "                data = torch.cat(data, dim=0)\n",
    "                data = Variable(data)       \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                # После прогона данных, разделяем результат на три соответствующих тензора\n",
    "                batch_size = int(outputs.shape[0]/3)\n",
    "                anchor_embs = outputs[:batch_size, :]\n",
    "                positive_embs = outputs[batch_size : 2*batch_size, :]\n",
    "                negative_embs = outputs[2*batch_size :, :]\n",
    "                loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % verbose == 0:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {(running_loss/verbose):.6f}, ', end=\"\")\n",
    "                    print(f'Learning rate: {scheduler.get_last_lr()[0]}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spended time for {verbose} batches ({int((verbose*data.shape[0])/3)} triplets', end=\"\") \n",
    "                    print(f' or {int(verbose*data.shape[0])} images) : {(time2-time1):.6f} sec')\n",
    "                    \n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)\n",
    "            scheduler.step()\n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                valid_result = self.valid(criterion, metric, valid_data_loader)\n",
    "                valid_loss = valid_result['valid_loss']\n",
    "                valid_metric = valid_result['valid_metric']\n",
    "            \n",
    "                print('='*80)\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}, valid loss: {(valid_loss):.6f}, ', end=\"\")\n",
    "                print(f'valid anc pos dist: {(valid_metric[0]):.6f}, valid anc neg dist: {(valid_metric[1]):.6f}')\n",
    "                print('='*80)\n",
    "            else:\n",
    "                print('='*80)\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}')\n",
    "                print('='*80)\n",
    "                valid_loss = None\n",
    "                valid_metric = None\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        result['epoch_train_losses'] = epoch_train_losses\n",
    "        result['epoch_valid_losses'] = epoch_valid_losses\n",
    "        result['epoch_valid_metrics'] = epoch_valid_metrics\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def valid(self, criterion: object, metric: object, valid_data_loader: DataLoader):\n",
    "        '''Метод для валидации модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss и метрики при валидации'''\n",
    "        \n",
    "        self.model.eval()\n",
    "        valid_metrics = []\n",
    "        valid_losses = []\n",
    "        result = {}\n",
    "        for batch_idx, data in enumerate(valid_data_loader):\n",
    "            data = torch.cat(data, dim=0)\n",
    "            data = Variable(data)\n",
    "            \n",
    "            outputs = self.model(data)\n",
    "                    \n",
    "            batch_size = int(outputs.shape[0]/3)\n",
    "            anchor_embs = outputs[:batch_size, :]\n",
    "            positive_embs = outputs[batch_size : 2*batch_size, :]\n",
    "            negative_embs = outputs[2*batch_size :, :]\n",
    "            \n",
    "            loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "            valid_losses.append(loss.item())\n",
    "                    \n",
    "            metric_value = metric(anchor_embs, positive_embs, negative_embs)\n",
    "            valid_metrics.append(metric_value)\n",
    "                \n",
    "        valid_loss    = np.mean(valid_losses)\n",
    "        valid_metric  = np.mean(valid_metrics, axis=0)\n",
    "        result['valid_loss'] = valid_loss\n",
    "        result['valid_metric'] = valid_metric\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def save(self, path_to_save: str = './model.pth'):\n",
    "        '''Метод сохранения весов модели\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения состояния модели'''\n",
    "        \n",
    "        torch.save(self.model.state_dict(), path_to_save)\n",
    "    \n",
    "    \n",
    "    def trace_save(self, path_to_save: str = './model.pth'):\n",
    "        '''Метод сохранения модели через torchscript\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        \n",
    "        example_forward_input = torch.rand(1, 3, 160, 160).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "            \n",
    "        traced_model = torch.jit.trace((self.model).eval(), example_forward_input)\n",
    "        torch.jit.save(traced_model, path_to_save)\n",
    "    \n",
    "    \n",
    "    def onnx_save(self, path_to_save: str = './face_recognition_model.onnx'):\n",
    "        '''Метод сохранения модели в формате ONNX\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        \n",
    "        example_forward_input = torch.randn(1, 3, 160, 160, requires_grad=True).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "\n",
    "        torch.onnx.export(self.model,\n",
    "                          example_forward_input,\n",
    "                          path_to_save,\n",
    "                          export_params=True,\n",
    "                          opset_version=10,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'],\n",
    "                          dynamic_axes={'input' : {0 : 'batch_size'},    # Модель будет работать с произвольным\n",
    "                                        'output' : {0 : 'batch_size'}})  # размером батча\n",
    "    \n",
    "    \n",
    "    def load(self, path_to_model: str = './model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7732612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собственная модель с нуля\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.alobal_average = nn.AvgPool2d(5)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "        \n",
    "        x = self.alobal_average(x)\n",
    "        \n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.dropout1(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfffdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    '''Класс необходим для реализации transfer learning.\n",
    "    Если создать объект этого класса и присвоить его последнему слою классификатора\n",
    "    обученной нейросети, то классификатор перезапишется, сигнал будет проходить без изменений, \n",
    "    и в дальнейшем можно будет добавлять пользовательские слои'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ba78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постороение модели на основе mobilenet_v3_small\n",
    "class MobileImagenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True)\n",
    "        # Заменяем последние 2 полносвязных слоя на имеющие нужную размерность\n",
    "        self.backbone.classifier[0] = nn.Linear(576, 256)\n",
    "        self.backbone.classifier[3] = nn.Linear(256, 128)\n",
    "\n",
    "        \n",
    "    def get_layers_names(self):\n",
    "         return dict(self.backbone.named_modules())\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2416e55",
   "metadata": {},
   "source": [
    "## Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14b99c",
   "metadata": {},
   "source": [
    "VGGface2 test https://www.kaggle.com/greatgamedota/vggface2-test - изображения людей\n",
    "\n",
    "VGGface https://www.robots.ox.ac.uk/~vgg/data/vgg_face/ - ссылки на изображения людей в txt файлах\n",
    "(надо скачивать через request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26260cdd",
   "metadata": {},
   "source": [
    "Для работы необходимо создать директории:\n",
    "\n",
    "    Для VGGface1:\n",
    "    - /home/dima/datasets/vgg_face_dataset/files/ - для хранения текстовых файлов со ссылками на \n",
    "    изображения VGGface1\n",
    "    (должны изначально находиться в этой директории)\n",
    "    - /home/dima/datasets/vgg_face_dataset/full_images/ - для хранения изображений VGGface1, которые будут \n",
    "    скачаны\n",
    "    - /home/dima/datasets/vgg_face_dataset/faces_from_mtcnn/ - для хранения лиц, которые будут извлечены mtcnn\n",
    "    - /home/dima/datasets/vgg_face_dataset/faces_from_bbox/ - для хранения лиц, которые будут извлечены через \n",
    "    известные bbox'ы\n",
    "    \n",
    "    Для VGGface2:\n",
    "    - /home/dima/datasets/vgg_face2_dataset/faces/ - для хранения изображений VGGface2 \n",
    "    (должны изначально находиться в этой директории)\n",
    "    - /home/dima/datasets/vgg_face2_dataset/full_images/ - для хранения лиц, которые будут извлечены mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "564ed4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG1_URLS_DIRECTORY = \"/home/dima/datasets/vgg_face_dataset/files/\"\n",
    "VGG1_IMAGES_DIRECTORY = '/home/dima/datasets/vgg_face_dataset/full_images/'\n",
    "VGG1_FACES_DIRECTORY_MTCNN = '/home/dima/datasets/vgg_face_dataset/faces_from_mtcnn/'\n",
    "VGG1_FACES_DIRECTORY_BBOX = '/home/dima/datasets/vgg_face_dataset/faces_from_bbox/'\n",
    "\n",
    "VGG2_FACES_DIRECTORY = '/home/dima/datasets/vgg_face2_dataset/faces/'\n",
    "VGG2_IMAGES_DIRECTORY = '/home/dima/datasets/vgg_face2_dataset/full_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка VGGface1\n",
    "vggface1data = VGGface1Data(urls_directory=VGG1_URLS_DIRECTORY,\n",
    "                            images_directory=VGG1_IMAGES_DIRECTORY, \n",
    "                            faces_directory=VGG1_FACES_DIRECTORY_BBOX)\n",
    "\n",
    "total_time = vggface1data.download(total_persons=1000, total_images_for_person=400, processes=20)\n",
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение лиц для VGGface1 на основе известных bbox\n",
    "vggface1data.get_faces_from_bbox(out_face_size=(160, 160), in_faces_size_treashold=(100, 100), \n",
    "                  face_ratio_treashold=1.5)\n",
    "\n",
    "# Меняем целевую директорию для лиц\n",
    "vggface1data.faces_directory = VGG1_FACES_DIRECTORY_MTCNN\n",
    "\n",
    "# Получение лиц для VGGface1 с использованием mtcnn\n",
    "vggface1data.get_faces_from_mtcnn(out_face_size=(160, 160), in_faces_size_treashold=(100, 100), \n",
    "                        face_prob_treashold=0.99, face_ratio_treashold=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1dd9e",
   "metadata": {},
   "source": [
    "Выделение лиц для VGGface1 происходило двумя способами: с помощью предоставленных bbox'ов и через mtcnn.\n",
    "В первом случае получился датасет из 122000 изображений лиц, во втором - из 77000. В обоих случаях\n",
    "отфильтровывались те лица, исходный размер которых на изображениях был меньше 100х100.\n",
    "Разница в размере произошла из-за того, что алгоритм выделения лиц с помощью mtcnn отбраковывал изображения, \n",
    "на которых присутствуют несколько человек, плюс еще есть дополнительные фильтры.\n",
    "Лучше использовать первый метод, если нужно больше данных, но нужна чистка данных, так как иногда bbox'ы \n",
    "размечены неправильно. Если мало времени, используем второй метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117850ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение лиц для VGGface2 с использованием mtcnn\n",
    "vggface2data = VGGface2Data(images_directory=VGG2_IMAGES_DIRECTORY, faces_directory=VGG2_FACES_DIRECTORY)\n",
    "vggface2data.get_faces_mtcnn(out_face_size=(160, 160),\n",
    "                             in_faces_size_treashold=(100, 100), \n",
    "                             face_prob_treashold=0.99, \n",
    "                             face_ratio_treashold=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf10be",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5411c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21706550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80e5e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  5 16:04:35 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   44C    P8    10W / 170W |    485MiB / 12045MiB |     12%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       987      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1707      G   /usr/lib/xorg/Xorg                202MiB |\r\n",
      "|    0   N/A  N/A      1842      G   /usr/bin/gnome-shell               32MiB |\r\n",
      "|    0   N/A  N/A      3655      G   ...AAAAAAAAA= --shared-files       47MiB |\r\n",
      "|    0   N/A  N/A      4039      G   /usr/lib/firefox/firefox          142MiB |\r\n",
      "|    0   N/A  N/A      4195      G   /usr/lib/firefox/firefox            4MiB |\r\n",
      "|    0   N/A  N/A      4207      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      4262      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      6590      G   /usr/bin/nvidia-settings            0MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60163a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "margin = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a7ee883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122114, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_vggface1 = get_data_csv(VGG1_FACES_DIRECTORY_BBOX)\n",
    "big_vggface1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e96b9024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77199, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_vggface1 = get_data_csv(VGG1_FACES_DIRECTORY_MTCNN)\n",
    "small_vggface1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f7c245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57331, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggface2 = get_data_csv(VGG2_FACES_DIRECTORY)\n",
    "vggface2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f2205",
   "metadata": {},
   "source": [
    "Эксперименты показали, что работать с vggface2 (в нашем случае только тестовая часть) не имеет смысла, так\n",
    "как данных в нем относительно мало (а именно мало различных людей - 500), и даже легковесная модель быстро \n",
    "переобучается. Поэтому основная работа будет проведена с big_vggface1 и small_vggface1: с лицами, извлеченными \n",
    "из vggface1 на основе bbox и с использованием mtcnn соответственно - в обоих случаях разлиных людей 1000, а \n",
    "различия заключаются лишь в общем количестве изображений лиц людей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "751d08d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique person names in train: 600\n",
      "Unique person names in valid: 200\n",
      "Unique person names in test: 200\n",
      "Total images in train: 73006\n",
      "Total images in valid: 24832\n",
      "Total images in test: 24276\n"
     ]
    }
   ],
   "source": [
    "big_train_df, big_valid_df, big_test_df = get_train_valid_test(source_df=big_vggface1, \n",
    "                                                               separate_feature='person_name')\n",
    "big_train_df.reset_index(inplace=True, drop=True)\n",
    "big_valid_df.reset_index(inplace=True, drop=True)\n",
    "big_test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Unique person names in train: {big_train_df['person_name'].nunique()}\")\n",
    "print(f\"Unique person names in valid: {big_valid_df['person_name'].nunique()}\")\n",
    "print(f\"Unique person names in test: {big_test_df['person_name'].nunique()}\")\n",
    "print(f\"Total images in train: {big_train_df.shape[0]}\")\n",
    "print(f\"Total images in valid: {big_valid_df.shape[0]}\")\n",
    "print(f\"Total images in test: {big_test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2812543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique person names in train: 600\n",
      "Unique person names in valid: 200\n",
      "Unique person names in test: 200\n",
      "Total images in train: 46117\n",
      "Total images in valid: 15513\n",
      "Total images in test: 15569\n"
     ]
    }
   ],
   "source": [
    "small_train_df, small_valid_df, small_test_df = get_train_valid_test(source_df=small_vggface1, \n",
    "                                                                     separate_feature='person_name')\n",
    "small_train_df.reset_index(inplace=True, drop=True)\n",
    "small_valid_df.reset_index(inplace=True, drop=True)\n",
    "small_test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Unique person names in train: {small_train_df['person_name'].nunique()}\")\n",
    "print(f\"Unique person names in valid: {small_valid_df['person_name'].nunique()}\")\n",
    "print(f\"Unique person names in test: {small_test_df['person_name'].nunique()}\")\n",
    "print(f\"Total images in train: {small_train_df.shape[0]}\")\n",
    "print(f\"Total images in valid: {small_valid_df.shape[0]}\")\n",
    "print(f\"Total images in test: {small_test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff19011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean channel values in train: (0.5904295, 0.44956186, 0.38371497)\n",
      "Std channel values in train: (0.2431854, 0.20771153, 0.19577332)\n"
     ]
    }
   ],
   "source": [
    "research_data = ResearchDataset(big_train_df, device)\n",
    "research_data_loader = DataLoader(research_data, batch_size=batch_size, shuffle=True)\n",
    "big_mean, big_std = get_mean_std(research_data_loader)\n",
    "\n",
    "print(f'Mean channel values in train: {big_mean}')\n",
    "print(f'Std channel values in train: {big_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e77a2202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean channel values in train: (0.633451, 0.47640446, 0.4034844)\n",
      "Std channel values in train: (0.21870759, 0.18738167, 0.17562027)\n"
     ]
    }
   ],
   "source": [
    "research_data = ResearchDataset(small_train_df, device)\n",
    "research_data_loader = DataLoader(research_data, batch_size=batch_size, shuffle=True)\n",
    "small_mean, small_std = get_mean_std(research_data_loader)\n",
    "\n",
    "print(f'Mean channel values in train: {small_mean}')\n",
    "print(f'Std channel values in train: {small_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc6597",
   "metadata": {},
   "source": [
    "Различия mean и std в двух наборах данных связаты с тем, что при извлечении лиц через mtcnn выделяются bbox'ы\n",
    "меньшей площади и содержат меньше информации о фоне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dc971b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate (limit=20, interpolation=1, border_mode=4, value=None, always_apply=True, p=1),\n",
    "    A.Normalize(mean=big_mean, std=big_std, max_pixel_value=1.0),\n",
    "    ToTensorV2(),])\n",
    "\n",
    "big_valid_test_transform = A.Compose([\n",
    "    A.Normalize(mean=big_mean, std=big_std, max_pixel_value=1.0),\n",
    "    ToTensorV2(),])\n",
    "\n",
    "small_train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate (limit=20, interpolation=1, border_mode=4, value=None, always_apply=True, p=1),\n",
    "    A.Normalize(mean=small_mean, std=small_std, max_pixel_value=1.0),\n",
    "    ToTensorV2(),])\n",
    "\n",
    "small_valid_test_transform = A.Compose([\n",
    "    A.Normalize(mean=small_mean, std=small_std, max_pixel_value=1.0),\n",
    "    ToTensorV2(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8a8affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train_data = CustomDataset(big_train_df, device, big_train_transform)\n",
    "big_valid_data = CustomDataset(big_valid_df, device, big_valid_test_transform)\n",
    "big_test_data = CustomDataset(big_test_df, device, big_valid_test_transform)\n",
    "\n",
    "big_train_data_loader = DataLoader(big_train_data, batch_size=batch_size, shuffle=True)\n",
    "big_valid_data_loader = DataLoader(big_valid_data, batch_size=batch_size, shuffle=True)\n",
    "big_test_data_loader = DataLoader(big_test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "small_train_data = CustomDataset(small_train_df, device, small_train_transform)\n",
    "small_valid_data = CustomDataset(small_valid_df, device, small_valid_test_transform)\n",
    "small_test_data = CustomDataset(small_test_df, device, small_valid_test_transform)\n",
    "\n",
    "small_train_data_loader = DataLoader(small_train_data, batch_size=batch_size, shuffle=True)\n",
    "small_valid_data_loader = DataLoader(small_valid_data, batch_size=batch_size, shuffle=True)\n",
    "small_test_data_loader = DataLoader(small_test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0161aaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MobileImagenet                                     --                        --\n",
       "├─MobileNetV3: 1-1                                 [32, 128]                 --\n",
       "│    └─Sequential: 2-1                             [32, 576, 5, 5]           --\n",
       "│    │    └─ConvBNActivation: 3-1                  [32, 16, 80, 80]          464\n",
       "│    │    └─InvertedResidual: 3-2                  [32, 16, 40, 40]          744\n",
       "│    │    └─InvertedResidual: 3-3                  [32, 24, 20, 20]          3,864\n",
       "│    │    └─InvertedResidual: 3-4                  [32, 24, 20, 20]          5,416\n",
       "│    │    └─InvertedResidual: 3-5                  [32, 40, 10, 10]          13,736\n",
       "│    │    └─InvertedResidual: 3-6                  [32, 40, 10, 10]          57,264\n",
       "│    │    └─InvertedResidual: 3-7                  [32, 40, 10, 10]          57,264\n",
       "│    │    └─InvertedResidual: 3-8                  [32, 48, 10, 10]          21,968\n",
       "│    │    └─InvertedResidual: 3-9                  [32, 48, 10, 10]          29,800\n",
       "│    │    └─InvertedResidual: 3-10                 [32, 96, 5, 5]            91,848\n",
       "│    │    └─InvertedResidual: 3-11                 [32, 96, 5, 5]            294,096\n",
       "│    │    └─InvertedResidual: 3-12                 [32, 96, 5, 5]            294,096\n",
       "│    │    └─ConvBNActivation: 3-13                 [32, 576, 5, 5]           56,448\n",
       "│    └─AdaptiveAvgPool2d: 2-2                      [32, 576, 1, 1]           --\n",
       "│    └─Sequential: 2-3                             [32, 128]                 --\n",
       "│    │    └─Linear: 3-14                           [32, 256]                 147,712\n",
       "│    │    └─Hardswish: 3-15                        [32, 256]                 --\n",
       "│    │    └─Dropout: 3-16                          [32, 256]                 --\n",
       "│    │    └─Linear: 3-17                           [32, 128]                 32,896\n",
       "====================================================================================================\n",
       "Total params: 1,107,616\n",
       "Trainable params: 1,107,616\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 909.72\n",
       "====================================================================================================\n",
       "Input size (MB): 9.83\n",
       "Forward/backward pass size (MB): 369.89\n",
       "Params size (MB): 4.43\n",
       "Estimated Total Size (MB): 384.15\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model from scratch\n",
    "#model = Model().to(device)\n",
    "\n",
    "# transfer learning with mobilenet v3 (imagenet)\n",
    "model = MobileImagenet().to(device)\n",
    "model_wrapper = ModelWrapper(model=model)\n",
    "summary(model_wrapper.model, input_size=(32, 3, 160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3a7dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = TripletLoss(margin=margin, device=device)\n",
    "optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=learning_rate)\n",
    "criterion = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "metric = RecognitionMetric()\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f682da19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.096635, Learning rate: 0.001\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 62.337253 sec\n",
      "Train Epoch: 1, Loss: 0.064593, Learning rate: 0.001\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.905559 sec\n",
      "Train Epoch: 1, Loss: 0.060135, Learning rate: 0.001\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.832221 sec\n",
      "Train Epoch: 1, Loss: 0.054912, Learning rate: 0.001\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.732972 sec\n",
      "Train Epoch: 1, Loss: 0.051116, Learning rate: 0.001\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.713237 sec\n",
      "Train Epoch: 1, Loss: 0.051301, Learning rate: 0.001\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.858773 sec\n",
      "Train Epoch: 1, Loss: 0.044811, Learning rate: 0.001\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.821240 sec\n",
      "================================================================================\n",
      "Epoch 1, train loss: 0.060029, valid loss: 0.046099, valid anc pos dist: 1.113252, valid anc neg dist: 2.014087\n",
      "================================================================================\n",
      "Train Epoch: 2, Loss: 0.045124, Learning rate: 0.0005\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.726657 sec\n",
      "Train Epoch: 2, Loss: 0.038274, Learning rate: 0.0005\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 62.309615 sec\n",
      "Train Epoch: 2, Loss: 0.035400, Learning rate: 0.0005\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.862812 sec\n",
      "Train Epoch: 2, Loss: 0.032742, Learning rate: 0.0005\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 62.013623 sec\n",
      "Train Epoch: 2, Loss: 0.031169, Learning rate: 0.0005\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.865941 sec\n",
      "Train Epoch: 2, Loss: 0.031282, Learning rate: 0.0005\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.916694 sec\n",
      "Train Epoch: 2, Loss: 0.032910, Learning rate: 0.0005\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.894213 sec\n",
      "================================================================================\n",
      "Epoch 2, train loss: 0.034982, valid loss: 0.030499, valid anc pos dist: 1.047888, valid anc neg dist: 2.078061\n",
      "================================================================================\n",
      "Train Epoch: 3, Loss: 0.030509, Learning rate: 0.00025\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.790844 sec\n",
      "Train Epoch: 3, Loss: 0.026720, Learning rate: 0.00025\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.846145 sec\n",
      "Train Epoch: 3, Loss: 0.026810, Learning rate: 0.00025\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.719331 sec\n",
      "Train Epoch: 3, Loss: 0.026128, Learning rate: 0.00025\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.922952 sec\n",
      "Train Epoch: 3, Loss: 0.024072, Learning rate: 0.00025\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.930039 sec\n",
      "Train Epoch: 3, Loss: 0.024798, Learning rate: 0.00025\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 62.076252 sec\n",
      "Train Epoch: 3, Loss: 0.025400, Learning rate: 0.00025\n",
      "Spended time for 50 batches (6400 triplets or 19200 images) : 61.865258 sec\n",
      "================================================================================\n",
      "Epoch 3, train loss: 0.026123, valid loss: 0.025793, valid anc pos dist: 1.002102, valid anc neg dist: 1.991822\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "result_train = model_wrapper.fit(criterion,\n",
    "             metric,\n",
    "             optimizer,\n",
    "             scheduler,\n",
    "             small_train_data_loader,\n",
    "             small_valid_data_loader,\n",
    "             epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f7756c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch_train_losses': [0.06002866575324139,\n",
       "  0.03498224838742589,\n",
       "  0.02612304136703962],\n",
       " 'epoch_valid_losses': [0.04609856699578098,\n",
       "  0.030498537471609525,\n",
       "  0.025793155838475853],\n",
       " 'epoch_valid_metrics': [array([1.11325152, 2.01408657]),\n",
       "  array([1.04788802, 2.07806055]),\n",
       "  array([1.00210186, 1.99182223])]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c99324b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.027044421481731973,\n",
       " 'valid_metric': array([1.00375848, 1.98308984])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_valid = model_wrapper.valid(criterion, metric, small_valid_data_loader)\n",
    "result_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "893bec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.02837662829361001,\n",
       " 'valid_metric': array([1.01532354, 1.96890503])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test = model_wrapper.valid(criterion, metric, small_test_data_loader)\n",
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "619a32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.save(path_to_save='./mobile_emb128_0.26test_margin2_122000own')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efc9c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = ModelWrapper(model=model)\n",
    "model_wrapper.load(path_to_model = './mobile_emb128_0.26test_margin2_122000own')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e80a6",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89111740",
   "metadata": {},
   "source": [
    "1. За 3 эпохи на VGGface2 (27000 лиц изначально больших 150х150) выходим на 0.25 на тесте. За 3 эпохи на VGGface2 (57000 лиц изначально больших 100х100) выходим на 0.25 на \n",
    "   тесте. Модель переобучается из-за малого количаства различных людей (500).\n",
    "\n",
    "2. За 3 эпохи на VGGface1 (15000) на valid и test быстро уходим в переобучение, так как данных мало.\n",
    "\n",
    "3. За 4 эпохи на VGGface1 (77000 лиц выделенных mtcnn, изначально больших 100х100) выходим на 0.17 на тесте.\n",
    "   Прогресс за счет увеличения данных (количества различных человек и общее количетсво изображений).\n",
    "\n",
    "4. Увеличение размерности эмбэддинга со 128 до 256 приводило к ухудшению качества.\n",
    "    \n",
    "5. Лучший результат при эмбэддинг = 128, VGGface1 (77000 лиц выделенных mtcnn изначально больших 100х100), \n",
    "   0.14 на test за 3 эпохи для margin = 1.\n",
    "\n",
    "6. Увеличение margin вдвое приводит к увеличению вдвое минимального лосса, но также вдвое увеличивает\n",
    "   расстояние между эмбэддингами для различных людей. Этот параметр лучше калибровать непосредственно в \n",
    "   приложении по распознаванию.\n",
    "\n",
    "7. На малом (77000) и большом (122000) VGGface1 минимальный loss на test равен примерно 0.14-0.15 для margin = 1\n",
    "   и 0.26-0.28 для margin = 2 на 4-5 эпохах. Дальше - переобучение. Для большего датасета качество немного лучше\n",
    "   (на 0.01-0.02). Большее влияние на точность оказывает увеличение датасета за счет добавления новых людей\n",
    "   (сравниваем с VGGface2), чем за счет добавления новых изображений уже существующих людей.\n",
    "    \n",
    "8. При обучении собственной модели ошибка уменьшается в несколько раз медленнее (по эпохам) при большем \n",
    "   количестве параметров, и минимальное значение метрики получается в несколько раз большим, чем при работе \n",
    "   с готовой архитектурой (непредобученной), поэтому целесообразно использовать готовые архитектуры, \n",
    "   так как они более оптимальны. \n",
    "    \n",
    "9. Готовая архитектура с предтренированными весами обучается быстрее, поэтому нужно меньше эпох для\n",
    "   достижения нужной метрики, следовательно, переобучение начнет проявляться при более низком значении \n",
    "   метрики.\n",
    "       \n",
    "10. Чем меньше замороженных весов в предобученной модели, тем быстрее модель обучается, поэтому оптимально\n",
    "    не замораживать никакие веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17dad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры кода заморозкой и разморозкой весов модели\n",
    "# Заморозить все веса модели\n",
    "for child in model_wrapper.model.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Разморозить веса блока модели\n",
    "for child in model_wrapper.model.backbone.features[12].children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Разморозить предпоследний полносвязный слой\n",
    "for param in model_wrapper.model.backbone.classifier[0].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Разморозить последний полносвязный слой\n",
    "for param in model_wrapper.model.backbone.classifier[3].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Разморозить все веса классификатора\n",
    "for child in model_wrapper.model.backbone.classifier.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
