{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b5928f",
   "metadata": {},
   "source": [
    "# Improvement Carvana_local_learning_prod_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4aaf",
   "metadata": {},
   "source": [
    "## Подключение библиотек и загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Улучшения: \n",
    "# Убрал .cuda() из класса модели и добавил в даталоадеры перенос на gpu\n",
    "# Добавил трэйсинг моделии и ее сохранение, загрузку модели\n",
    "# Заменил лишний ресайз масок на передачу оригинолов масок\n",
    "# Сделал предсказания с разным размером батча\n",
    "# Нейтрализовал проявления хардкода\n",
    "# Написал комментарии и пояснения\n",
    "# Сделал submission при обучении на всем трерировочном датасете\n",
    "# переписал pil на cv2 где это можно - скорость увеличилась\n",
    "# Реализовал ресайз через albumintation - скорость увеличилась\n",
    "# Сделал нормализацию через albumintation улучшений нет, только время увеличилось\n",
    "# Проверил, что при нормализации маска не изменяется\n",
    "# Изменил формат вывода при обучении и предсказании\n",
    "# сделал predict на torchscript - скорость не выросла\n",
    "# заменил при валидации и в predict интерполяцию масок с nearest на bilinear\n",
    "# с onnx получилось медленне, видимо нужно отдельно устанавливать cuda и cudnn\n",
    "# Если предсказания модели приводить к исходному разрешению (1918, 1280) через bilinear, метрика на батче\n",
    "# возрастает примерно на 0.001, что очень весомо (0.9962 против 09952) по сравнению с nearest. На изображениях \n",
    "# результат тоже заметен. Использование bicubic не дает стабильного прироста\n",
    "# наиболее предпочтительным методом интерполяции при аугментации является cv.INTER_AREA - из интернета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eadeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# дообучить torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)\n",
    "\n",
    "# Эксперименты с unet(mobilenet), unet(efficientnetb5), deeplabv3plus(mobilenet), deeplabv3plus(efficientnetb5)\n",
    "# Попробовать softdice loss + bce (как в dlcource.ai)\n",
    "# разобраться, заморожены ли веса энкодера у smp моделей\n",
    "# реализовать модель из https://github.com/lyakaap/Kaggle-Carvana-3rd-Place-Solution/blob/master/model_pytorch.py\n",
    "\n",
    "# разобраться с torchvision.models.segmentation.segmentation.deeplabv3_mobilenet_v3_large\n",
    "# Взять deeplab обученный из torchvision.models.segmentation.segmentation и протестировать вместо предыдущих моделей\n",
    "# с интерполяцией и на полном изображении - сравнить точность с 30 - эпоховой моделью. Быть может, уперлись в точность\n",
    "# именно из-за интерполяции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15971ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f662c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.onnx\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6386d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Выполнять, если датасет не загружен\n",
    "#!pip install -q kaggle\n",
    "#!mkdir ~/.kaggle\n",
    "#!cp ~/kaggle.json ~/.kaggle/\n",
    "#!chmod 600 ~/.kaggle/kaggle.json\n",
    "#!kaggle competitions download -c carvana-image-masking-challenge\n",
    "#!unzip ~/carvana-image-masking-challenge.zip ~/carvana_dataset/\n",
    "\n",
    "#!unzip ~/carvana_dataset/train.zip -d ~/carvana_dataset/train\n",
    "#!unzip ~/carvana_dataset/test.zip -d ~/carvana_dataset/test\n",
    "#!unzip ~/carvana_dataset/train_masks.zip -d ~/carvana_dataset/train_masks\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_hq.zip -d ~/carvana_dataset/train_hq\n",
    "#!unzip ~/carvana_dataset/test_hq.zip -d ~/carvana_dataset/test_hq\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_masks.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/sample_submission.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/metadata.csv.zip  ~/carvana_dataset/\n",
    "\n",
    "#!rm ~/carvana-image-masking-challenge.zip\n",
    "#!rm ~/carvana_dataset/test.zip\n",
    "#!rm ~/carvana_dataset/train_masks.zip\n",
    "#!rm ~/carvana_dataset/train.zip\n",
    "#!rm ~/carvana_dataset/test_hq.zip\n",
    "#!rm ~/carvana_dataset/train_hq.zip\n",
    "#!rm ~/carvana_dataset/train_masks.csv.zip\n",
    "#!rm ~/carvana_dataset/sample_submission.csv.zip\n",
    "#!rm ~/carvana_dataset/metadata.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8be7cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4005c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct  7 15:03:51 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   51C    P8    14W / 170W |    363MiB / 12045MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       958      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1653      G   /usr/lib/xorg/Xorg                142MiB |\r\n",
      "|    0   N/A  N/A      1783      G   /usr/bin/gnome-shell               44MiB |\r\n",
      "|    0   N/A  N/A      8102      G   /usr/bin/nvidia-settings            0MiB |\r\n",
      "|    0   N/A  N/A     15067      G   /usr/lib/firefox/firefox          124MiB |\r\n",
      "|    0   N/A  N/A     15188      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3608d7",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec78c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(imgs_path: str = None, masks_path: str = None) -> pd.DataFrame:\n",
    "    '''Функция получает на вход пути к директориям с изображениями и масками\n",
    "    и генерирует датафрейм, содержащий имя изображений, их адреса и адреса\n",
    "    соответствующих им масок\n",
    "  \n",
    "    Входные параметры:\n",
    "    imgs_path: str - путь к директории с изображениями,\n",
    "    masks_path: str - путь к директории с масками\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data - dataframe, содержащий адреса изображений и соответствующих им масок'''\n",
    "\n",
    "    assert (imgs_path != None) & (masks_path != None)\n",
    "    # imgs_path or masks_path is equal None\n",
    "\n",
    "    data_img = {}\n",
    "    data_mask = {}\n",
    "    data_img['imgs_path'] = []\n",
    "    data_mask['masks_path'] = []\n",
    "    data_img['imgs_path'] = list(glob.glob(imgs_path + \"/*\"))\n",
    "    data_mask['masks_path'] = list(glob.glob(masks_path + \"/*\"))\n",
    "\n",
    "    data_img = pd.DataFrame(data_img)\n",
    "    data_mask = pd.DataFrame(data_mask)\n",
    "\n",
    "    def file_name(x):\n",
    "        return x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    data_img[\"file_name\"] = data_img[\"imgs_path\"].apply(lambda x: file_name(x))\n",
    "    data_mask[\"file_name\"] = data_mask[\"masks_path\"].apply(lambda x: file_name(x)[:-5])\n",
    "\n",
    "    data = pd.merge(data_img, data_mask, on = \"file_name\", how = \"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5613d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(source_df: pd.DataFrame, separate_feature: str = None, test_size: float = 0.25) -> pd.DataFrame:\n",
    "    '''Функция разделяет source_df на две части с коэффициентом test_size\n",
    "    по уникальным значениям separate_feature так, чтобы в новых датафреймах\n",
    "    не было строк с одинаковыми значенияти из separate_feature\n",
    "\n",
    "    Входные параметры:\n",
    "    source_df: pd.DataFrame - датафрейм для разделения на train и test\n",
    "    separate_feature: str - поле, по которому датафрейм будет разделен\n",
    "    test_size: float - коэффициент разделения дтафрейма\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data_train - датафрейм для тренировки\n",
    "    pd.DataFrame: data_valid - датафрейм для валидации'''\n",
    "  \n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_cars, valid_cars = train_test_split(source_df[separate_feature].unique(), test_size=test_size, random_state=42)\n",
    "        data_valid = source_df[np.isin(source_df[separate_feature].values, valid_cars)]\n",
    "        data_train = source_df[np.isin(source_df[separate_feature].values, train_cars)]\n",
    "        assert source_df.shape[0] == (data_valid.shape[0] + data_train.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "    else:\n",
    "        data_train, data_valid = train_test_split(source_df, test_size=test_size)\n",
    "\n",
    "    return data_train, data_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712baa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(logits: torch.Tensor, targets: torch.Tensor, treashold: float) -> float:\n",
    "    '''Функция для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    Входные параметры:\n",
    "    logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "    targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "    treashold: float - порог для определения класса точки в предсказанной точке\n",
    "    Возвращаемые значения:\n",
    "    score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "    \n",
    "    smooth = 1\n",
    "    num = targets.size(0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    outputs = torch.where(probs > treashold, 1, 0)\n",
    "    m1 = outputs.view(num, -1)\n",
    "    m2 = targets.view(num, -1)\n",
    "    intersection = (m1 * m2)\n",
    "\n",
    "    score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "    score = score.sum() / num\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd108c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "    '''Функция принимает одну маску в тензорном формате, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    tensor: torch.Tensor - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строком виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    tensor = tensor.view(1, -1)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor[0] = 0\n",
    "    tensor[-1] = 0\n",
    "    rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    rle = rle.cpu().detach().numpy()\n",
    "    rle_str = rle_to_string(rle)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dba4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_rle(mask_image: np.ndarray) -> str:\n",
    "    '''Функция принимает одну маску в формате массива numpy, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    mask_image: numpy.ndarray - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    pixels = mask_image.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle_str = rle_to_string(runs)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf01f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_string(runs: torch.Tensor) -> str:\n",
    "    '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "    в строковое представление этой последовательности\n",
    "    Входные параметры:\n",
    "    runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - строковое представление последовательности чисел'''\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd732aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(mask_addr: str) -> str:\n",
    "    '''Функция преобразует маску, имеющую адрес mask_addr и сохраненную в\n",
    "    формате .gif, элементы которой имеют значения 0 и 1 в rle представление\n",
    "    в строковом виде\n",
    "    Входные параметры:\n",
    "    mask_addr: str - адрес маски\n",
    "    Возвращаемые значения:\n",
    "    mask_rle: str - rle представление маски в строком виде\n",
    "    '''\n",
    "    \n",
    "    mask = Image.open(mask_addr).convert('LA') # преобразование в серый\n",
    "    mask = np.asarray(mask).astype('float')[:,:,0]\n",
    "    mask = mask/255.0\n",
    "    mask_rle = numpy_to_rle(mask)\n",
    "    return mask_rle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dd14a",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b539ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceMetric(nn.Module):\n",
    "    '''Класс для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    с заданным порогом для определния класса каждой точки изображения'''\n",
    "    \n",
    "    def __init__(self, treashold: float=0.5):\n",
    "        '''treashold: float - порог для определения класса точки в предсказанной точке'''\n",
    "        super(DiceMetric, self).__init__()\n",
    "        self.treashold = treashold\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "        with torch.no_grad():\n",
    "            smooth = 1\n",
    "            num = targets.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            outputs = torch.where(probs > self.treashold, 1., 0.)\n",
    "            m1 = outputs.view(num, -1)\n",
    "            m2 = targets.view(num, -1)\n",
    "            intersection = (m1 * m2)\n",
    "\n",
    "            score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "            score = score.sum() / num\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e9aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    '''Класс для вычисления DICE loss для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE loss для набора предсказанных масок'''\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4045a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTrain(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str, transform: object, skip_mask: bool=False):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения и маски\n",
    "        skip_mask: bool - флаг, нужно ли генерировать исходную маску (без изменения размерности)\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTrain'''\n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Разделяем датафрейм на rgb картинки \n",
    "        self.image_arr = self.data_info.iloc[:,0]\n",
    "        # и на сегментированные картинки\n",
    "        self.mask_arr = self.data_info.iloc[:,2]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Нужно ли пробрасывать маску изображения на выход без изменений\n",
    "        self.skip_mask = skip_mask\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        mask: torch.Tensor - тензорное представление изображения с размерностью out_shape \n",
    "        (возвращается если значение skip_mask равно True)'''\n",
    "        image = cv2.imread(self.image_arr[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        # gif не открывается через open cv, поэтому используем для чтения PIL Image\n",
    "        mask = Image.open(self.mask_arr[index])\n",
    "        mask = np.asarray(mask)#.astype('float')\n",
    "        \n",
    "        transformed = self.transform(image=image, mask=mask)\n",
    "        tr_image = transformed['image']\n",
    "        tr_mask = transformed['mask']\n",
    "        \n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        tr_mask = tr_mask.to(self.device).float().unsqueeze(0)\n",
    "\n",
    "        \n",
    "        # Если необходима исходная маска, то дополнительно возвращаем ее\n",
    "        if self.skip_mask == True:\n",
    "            mask = (torch.as_tensor(mask)).to(self.device).float().unsqueeze(0)\n",
    "            return (tr_image, tr_mask, mask)\n",
    "        else:\n",
    "            return (tr_image, tr_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15588a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTest(Dataset):\n",
    "    '''Класс для создания тестовых датасетов'''\n",
    "    def __init__(self, data_info, device: str, transform: object):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами и именами изображений\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTest'''\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Получаем адреса RGB изображений \n",
    "        self.image_addresses = self.data_info.iloc[:,0]\n",
    "        # Получаем имена RGB изображений \n",
    "        self.image_names = self.data_info.iloc[:,1]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        image_name: str - имя изображения'''\n",
    "        image = cv2.imread(self.image_addresses[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        transformed = self.transform(image=image)\n",
    "        tr_image = transformed['image']\n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        image_name = self.image_names[index]\n",
    "    \n",
    "        return (index, tr_image, image_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a9bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''Класс для создания работы с нейронной сетью для семантической сегментации Carvana'''\n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: nn.Module - последовательность слоев или модель, через которую будут проходить данные\n",
    "        Возвращаемые значения: \n",
    "        объект класса NeuralNetwork'''\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        '''Функция прямого прохода через объкт класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения\n",
    "        Возвращаемые значения: \n",
    "        input_data: torch.Tensor - тензорное представление маски изображения'''\n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "        '''Статический метод принимает одну маску в тензорном формате, элементы которой\n",
    "        имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "        Входные параметры:\n",
    "        tensor: torch.Tensor - маска в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "        # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "        # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "        # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "        # не сильно упадет)\n",
    "        with torch.no_grad():\n",
    "            tensor = tensor.view(1, -1)\n",
    "            tensor = tensor.squeeze(0)\n",
    "            tensor[0] = 0\n",
    "            tensor[-1] = 0\n",
    "            rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "            rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "            rle = rle.cpu().detach().numpy()\n",
    "            rle_str = NeuralNetwork.rle_to_string(rle)\n",
    "            return rle_str\n",
    "    \n",
    "    @staticmethod\n",
    "    def rle_to_string(runs: torch.Tensor) -> str:\n",
    "        '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "        в строковое представление этой последовательности\n",
    "        Входные параметры:\n",
    "        runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - строковое представление последовательности чисел'''\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "    \n",
    "    \n",
    "    def fit(self, criterion: object, metric: object, optimizer: object, \n",
    "                  train_data_loader: DataLoader, valid_data_loader: DataLoader=None, epochs: int=1):\n",
    "        '''Метод для обучения объекта класса\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        optimizer: object - оптимизатор\n",
    "        train_data_loader: DataLoader - загрузчик данных для обучения\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        epochs: int - количество эпох обучения\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        result = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss =0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, (data, labels) in enumerate(train_data_loader):\n",
    "                data, labels = Variable(data), Variable(labels)        \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % 300 == 0:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {(running_loss/300):.6f}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spend time for {300*data.shape[0]} images: {(time2-time1):.6f} sec')\n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)        \n",
    "            \n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                self.model.eval()\n",
    "                valid_metrics = []\n",
    "                valid_losses = []\n",
    "                for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "                    data, labels, labels_small = Variable(data), Variable(labels), Variable(labels_small)\n",
    "                    outputs = self.model(data)\n",
    "                    # loss вычисляется для сжатых масок для правильной валидации (обучались на сжатых)\n",
    "                    # чтобы вовремя определить переобучение\n",
    "                    loss = criterion(outputs, labels_small)\n",
    "                    valid_losses.append(loss.item())\n",
    "                    #Преобразуем выход модели к размеру соответствующей маски\n",
    "                    outputs = F.interpolate(input=outputs, size=(labels.shape[2], \n",
    "                                                                 labels.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "                    # метрика считается для исходных размеров потому что именно так итоговое качество\n",
    "                    # определяется алгоритмом kaggle \n",
    "                    metric_value = metric(outputs, labels)\n",
    "                    valid_metrics.append(metric_value.item())\n",
    "                    \n",
    "                valid_loss    = np.mean(valid_losses)\n",
    "                valid_metric  = np.mean(valid_metrics)\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}, valid_loss: {(valid_loss):.6f}, valid_metric: {(valid_metric):.6f}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}')\n",
    "                valid_loss = None\n",
    "                valid_metric = None\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        result['epoch_train_losses'] = epoch_train_losses\n",
    "        result['epoch_valid_losses'] = epoch_valid_losses\n",
    "        result['epoch_valid_metrics'] = epoch_valid_metrics\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def valid(self, criterion: object, metric: object, valid_data_loader: DataLoader):\n",
    "        '''Метод для валидации модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        self.model.eval()\n",
    "        valid_metrics = []\n",
    "        valid_losses = []\n",
    "        result = {}\n",
    "        for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "            data, labels, labels_small = Variable(data), Variable(labels), Variable(labels_small)\n",
    "            outputs = self.model(data)\n",
    "            # loss вычисляется для сжатых масок для правильной валидации (обучались на сжатых)\n",
    "            # чтобы вовремя определить переобучение\n",
    "            loss = criterion(outputs, labels_small)\n",
    "            valid_losses.append(loss.item())\n",
    "            #Преобразуем выход модели к размеру соответствующей маски\n",
    "            outputs = F.interpolate(input=outputs, size=(labels.shape[2], \n",
    "                                                         labels.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # метрика считается для исходных размеров потому что именно так итоговое качество\n",
    "            # определяется алгоритмом kaggle \n",
    "            metric_value = metric(outputs, labels)\n",
    "            valid_metrics.append(metric_value.item())\n",
    "                    \n",
    "        valid_loss    = np.mean(valid_losses)\n",
    "        valid_metric  = np.mean(valid_metrics)\n",
    "        result['valid_loss'] = valid_loss\n",
    "        result['valid_metric'] = valid_metric\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def predict(self, test_data_loader: DataLoader, predict_directory: str=None, output_size: tuple=(1280, 1918), \n",
    "                mask_treashold: float=0.5, generate_rle_dataframe: bool=True) -> pd.DataFrame:\n",
    "        '''Метод для предсказания масок для набора изображения\n",
    "        Входные параметры:\n",
    "        test_data_loader: DataLoader - загрузчик данных для предсказания\n",
    "        predict_directory: str - директория, в которую будут сохраняться сгенерированные маски (если None,\n",
    "        то маски сохраняться не будут)\n",
    "        output_size: tuple - пространственная размерность выходных масок\n",
    "        mask_treashold: float - порог, по которому будет определяться класс каждой точки для масок\n",
    "        generate_rle_dataframe: bool - флаг, нужна ли генерация rle представлений масок\n",
    "        Возвращаемые значения:\n",
    "        rle_dataframe: pd.DataFrame - датафрейм с rle представлениями для масок (если \n",
    "        generate_rle_dataframe==True)\n",
    "        Маски в формате .gif для изображений с соответствующими именами, находятся в директории predict_directory'''\n",
    "        self.model.eval()\n",
    "        img_names = []\n",
    "        img_rles = []\n",
    "        time1 = time.time()\n",
    "        time2 = time.time()\n",
    "        for batch_idx, (index, img, img_name)  in enumerate(test_data_loader):\n",
    "\n",
    "            img = Variable(img)        \n",
    "            pred_mask_logit = self.model(img)\n",
    "            pred_mask_logit = F.interpolate(input=pred_mask_logit, size=output_size, mode='bilinear', align_corners=False)\n",
    "            pred_mask_logit_prob = torch.sigmoid(pred_mask_logit)\n",
    "            pred_mask = torch.where(pred_mask_logit_prob > mask_treashold, 1, 0)\n",
    "            \n",
    "            # Каждое изображение в тензоре преобразуем в картинку и сохраняем\n",
    "            for i in range(pred_mask.shape[0]):\n",
    "                if predict_directory != None:\n",
    "                    mask = (pred_mask[i].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "                    PIL_image = Image.fromarray(mask.astype('uint8'), 'L')\n",
    "                    PIL_image.save((predict_directory+img_name[i]).split('.')[0]+'.gif')\n",
    "                \n",
    "                # Если требуется, получаем значения rle для каждой картинки\n",
    "                if generate_rle_dataframe == True:\n",
    "                    img_names.append(img_name[i])\n",
    "                    img_rles.append(NeuralNetwork.tensor_to_rle(pred_mask[i]))\n",
    "            \n",
    "            if (batch_idx+1) % 300 == 0:\n",
    "                    print('-'*50)\n",
    "                    print(f'Processed images: {(batch_idx+1)*img.shape[0]}')\n",
    "                    time3 = time.time()\n",
    "                    print(f'Total time: {(time3-time1):.2f} sec')\n",
    "                    print(f'Time to process {300*img.shape[0]} images: {(time3-time2):.2f} sec')\n",
    "                    time2 = time.time()\n",
    "                \n",
    "        if generate_rle_dataframe == True:\n",
    "            rle_dataframe = pd.DataFrame(list(zip(img_names, img_rles)), columns =['img_name', 'img_rle'])\n",
    "            return rle_dataframe\n",
    "    \n",
    "    def save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения весов модели\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения состояния модели'''\n",
    "        torch.save(self.model.state_dict(), path_to_save)\n",
    "    \n",
    "    def trace_save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения модели через torchscript\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.rand(1, 3, 512, 512).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "            \n",
    "        traced_model = torch.jit.trace((self.model).eval(), example_forward_input)\n",
    "        torch.jit.save(traced_model, path_to_save)\n",
    "    \n",
    "    def onnx_save(self, path_to_save: str='./carvana_model.onnx'):\n",
    "        '''Метод сохранения модели в формате ONNX\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.randn(1, 3, 1024, 1024, requires_grad=True).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "\n",
    "        torch.onnx.export(self.model,\n",
    "                          example_forward_input,\n",
    "                          path_to_save,\n",
    "                          export_params=True,\n",
    "                          opset_version=10,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'],\n",
    "                          dynamic_axes={'input' : {0 : 'batch_size'},    # Модель будет работать с произвольным\n",
    "                                        'output' : {0 : 'batch_size'}})  # размером батча\n",
    "    \n",
    "    def load(self, path_to_model: str='./model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e07ce",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21937687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/dima/carvana_dataset'\n",
    "imgs_path  = dataset_path + '/train/train'\n",
    "masks_path = dataset_path + '/train_masks/train_masks'\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "mask_treashold = 0.5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d58730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function preprocess_input at 0x7ff4bcfe2790>, input_space='RGB', input_range=[0, 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "preprocess_input = get_preprocessing_fn('mobilenet_v2', pretrained='imagenet')\n",
    "preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "888ec35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(1024, 2048, cv2.INTER_AREA), # для масок автоматически будет применяться своя интерполяция, \n",
    "                                          # поэтому на выходе значения маски останутся 0 и 1\n",
    "    #A.HorizontalFlip(p=0.5)\n",
    "    #С нормализацией хуже почему то\n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(1024, 2048, cv2.INTER_AREA), # INTER_AREA как правило лучше осуществляет переход к меньшему разрешению\n",
    "    #С нормализацией хуже почему то\n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28b75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e54a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение с валидацией\n",
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_data = CustomDatasetForTrain(train_df, device, train_transform, skip_mask=False)\n",
    "valid_data = CustomDatasetForTrain(valid_df, device, valid_transform, skip_mask=True)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение без валидации\n",
    "train_data = CustomDatasetForTrain(data, device, train_transform, skip_mask=False)\n",
    "train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca4b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель на основе предложенной архитектуры\n",
    "#model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "#                 encoder_weights='imagenet', decoder_channels = [256, 128, 64, 32, 16]).to(device)\n",
    "\n",
    "#my_model = NeuralNetwork(model=model)\n",
    "#summary(model, input_size=(2, 3, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ccff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#efficientnet-b5 mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3339c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лучший результат был получен здесь\n",
    "\n",
    "#model = smp.DeepLabV3Plus(encoder_name='mobilenet_v2', encoder_depth=5, encoder_weights='imagenet', \n",
    "#                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "#                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4abf005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эту надо обучить\n",
    "model = smp.DeepLabV3Plus(encoder_name='timm-mobilenetv3_small_100', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e40c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNetwork(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961486dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "DeepLabV3Plus                                           --                        --\n",
       "├─MobileNetV3Encoder: 1-1                               [2, 3, 1024, 2048]        --\n",
       "│    └─MobileNetV3Features: 2                           --                        --\n",
       "│    │    └─Conv2dSame: 3-1                             [2, 16, 512, 1024]        432\n",
       "│    │    └─BatchNorm2d: 3-2                            [2, 16, 512, 1024]        32\n",
       "│    │    └─Hardswish: 3-3                              [2, 16, 512, 1024]        --\n",
       "├─DeepLabV3PlusDecoder: 1-2                             [2, 256, 256, 512]        --\n",
       "│    └─Sequential: 2-1                                  [2, 256, 64, 128]         --\n",
       "│    │    └─ASPP: 3-4                                   [2, 256, 64, 128]         1,083,584\n",
       "│    │    └─SeparableConv2d: 3-5                        [2, 256, 64, 128]         67,840\n",
       "│    │    └─BatchNorm2d: 3-6                            [2, 256, 64, 128]         512\n",
       "│    │    └─ReLU: 3-7                                   [2, 256, 64, 128]         --\n",
       "│    └─UpsamplingBilinear2d: 2-2                        [2, 256, 256, 512]        --\n",
       "│    └─Sequential: 2-3                                  [2, 48, 256, 512]         --\n",
       "│    │    └─Conv2d: 3-8                                 [2, 48, 256, 512]         768\n",
       "│    │    └─BatchNorm2d: 3-9                            [2, 48, 256, 512]         96\n",
       "│    │    └─ReLU: 3-10                                  [2, 48, 256, 512]         --\n",
       "│    └─Sequential: 2-4                                  [2, 256, 256, 512]        --\n",
       "│    │    └─SeparableConv2d: 3-11                       [2, 256, 256, 512]        80,560\n",
       "│    │    └─BatchNorm2d: 3-12                           [2, 256, 256, 512]        512\n",
       "│    │    └─ReLU: 3-13                                  [2, 256, 256, 512]        --\n",
       "├─SegmentationHead: 1-3                                 [2, 1, 1024, 2048]        --\n",
       "│    └─Conv2d: 2-5                                      [2, 1, 256, 512]          257\n",
       "│    └─UpsamplingBilinear2d: 2-6                        [2, 1, 1024, 2048]        --\n",
       "│    └─Activation: 2-7                                  [2, 1, 1024, 2048]        --\n",
       "│    │    └─Identity: 3-14                              [2, 1, 1024, 2048]        --\n",
       "=========================================================================================================\n",
       "Total params: 2,161,137\n",
       "Trainable params: 2,161,137\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 46.52\n",
       "=========================================================================================================\n",
       "Input size (MB): 50.33\n",
       "Forward/backward pass size (MB): 5146.47\n",
       "Params size (MB): 8.64\n",
       "Estimated Total Size (MB): 5205.44\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(2, 3, 1024, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SoftDiceLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "metric = DiceMetric(treashold=mask_treashold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca111f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.043971\n",
      "Spend time for 600 images: 105.431221 sec\n",
      "Train Epoch: 1, Loss: 0.011134\n",
      "Spend time for 600 images: 103.223126 sec\n",
      "Train Epoch: 1, Loss: 0.008151\n",
      "Spend time for 600 images: 103.301069 sec\n",
      "Train Epoch: 1, Loss: 0.007140\n",
      "Spend time for 600 images: 103.301821 sec\n",
      "Train Epoch: 1, Loss: 0.006508\n",
      "Spend time for 600 images: 103.255912 sec\n",
      "Train Epoch: 1, Loss: 0.006372\n",
      "Spend time for 600 images: 103.139380 sec\n",
      "Epoch 1, train loss: 0.013624, valid_loss: 0.006969, valid_metric: 0.993068\n",
      "Train Epoch: 2, Loss: 0.006256\n",
      "Spend time for 600 images: 102.844965 sec\n",
      "Train Epoch: 2, Loss: 0.005749\n",
      "Spend time for 600 images: 103.156085 sec\n",
      "Train Epoch: 2, Loss: 0.005509\n",
      "Spend time for 600 images: 103.097690 sec\n",
      "Train Epoch: 2, Loss: 0.005412\n",
      "Spend time for 600 images: 102.896319 sec\n",
      "Train Epoch: 2, Loss: 0.005304\n",
      "Spend time for 600 images: 102.943192 sec\n",
      "Train Epoch: 2, Loss: 0.005153\n",
      "Spend time for 600 images: 102.926994 sec\n",
      "Epoch 2, train loss: 0.005532, valid_loss: 0.005115, valid_metric: 0.995143\n",
      "Train Epoch: 3, Loss: 0.004957\n",
      "Spend time for 600 images: 102.776745 sec\n",
      "Train Epoch: 3, Loss: 0.005013\n",
      "Spend time for 600 images: 102.930162 sec\n",
      "Train Epoch: 3, Loss: 0.005130\n",
      "Spend time for 600 images: 102.830230 sec\n",
      "Train Epoch: 3, Loss: 0.005059\n",
      "Spend time for 600 images: 102.811001 sec\n",
      "Train Epoch: 3, Loss: 0.006613\n",
      "Spend time for 600 images: 102.835300 sec\n",
      "Train Epoch: 3, Loss: 0.006098\n",
      "Spend time for 600 images: 102.899232 sec\n",
      "Epoch 3, train loss: 0.005461, valid_loss: 0.005247, valid_metric: 0.994771\n",
      "Train Epoch: 4, Loss: 0.005114\n",
      "Spend time for 600 images: 102.954283 sec\n",
      "Train Epoch: 4, Loss: 0.004691\n",
      "Spend time for 600 images: 102.846481 sec\n",
      "Train Epoch: 4, Loss: 0.004649\n",
      "Spend time for 600 images: 103.005490 sec\n",
      "Train Epoch: 4, Loss: 0.004710\n",
      "Spend time for 600 images: 102.872537 sec\n",
      "Train Epoch: 4, Loss: 0.004563\n",
      "Spend time for 600 images: 103.087955 sec\n",
      "Train Epoch: 4, Loss: 0.004957\n",
      "Spend time for 600 images: 102.817398 sec\n",
      "Epoch 4, train loss: 0.004775, valid_loss: 0.004814, valid_metric: 0.994920\n",
      "Train Epoch: 5, Loss: 0.004613\n",
      "Spend time for 600 images: 102.853538 sec\n",
      "Train Epoch: 5, Loss: 0.004580\n",
      "Spend time for 600 images: 102.881161 sec\n",
      "Train Epoch: 5, Loss: 0.004533\n",
      "Spend time for 600 images: 102.868692 sec\n",
      "Train Epoch: 5, Loss: 0.004338\n",
      "Spend time for 600 images: 102.840978 sec\n",
      "Train Epoch: 5, Loss: 0.004330\n",
      "Spend time for 600 images: 102.838751 sec\n",
      "Train Epoch: 5, Loss: 0.004519\n",
      "Spend time for 600 images: 102.930194 sec\n",
      "Epoch 5, train loss: 0.004471, valid_loss: 0.004381, valid_metric: 0.995461\n",
      "Train Epoch: 6, Loss: 0.004276\n",
      "Spend time for 600 images: 102.756113 sec\n",
      "Train Epoch: 6, Loss: 0.004282\n",
      "Spend time for 600 images: 102.975317 sec\n",
      "Train Epoch: 6, Loss: 0.004270\n",
      "Spend time for 600 images: 102.718695 sec\n",
      "Train Epoch: 6, Loss: 0.005512\n",
      "Spend time for 600 images: 102.938550 sec\n",
      "Train Epoch: 6, Loss: 0.004499\n",
      "Spend time for 600 images: 102.877802 sec\n",
      "Train Epoch: 6, Loss: 0.004188\n",
      "Spend time for 600 images: 102.924029 sec\n",
      "Epoch 6, train loss: 0.004489, valid_loss: 0.004323, valid_metric: 0.995675\n",
      "Train Epoch: 7, Loss: 0.004138\n",
      "Spend time for 600 images: 102.864204 sec\n",
      "Train Epoch: 7, Loss: 0.004235\n",
      "Spend time for 600 images: 102.776883 sec\n",
      "Train Epoch: 7, Loss: 0.004105\n",
      "Spend time for 600 images: 102.882785 sec\n",
      "Train Epoch: 7, Loss: 0.004028\n",
      "Spend time for 600 images: 102.817689 sec\n",
      "Train Epoch: 7, Loss: 0.004202\n",
      "Spend time for 600 images: 102.964544 sec\n",
      "Train Epoch: 7, Loss: 0.004249\n",
      "Spend time for 600 images: 102.820121 sec\n",
      "Epoch 7, train loss: 0.004144, valid_loss: 0.004378, valid_metric: 0.995575\n",
      "Train Epoch: 8, Loss: 0.003986\n",
      "Spend time for 600 images: 102.852087 sec\n",
      "Train Epoch: 8, Loss: 0.004031\n",
      "Spend time for 600 images: 102.857625 sec\n",
      "Train Epoch: 8, Loss: 0.004092\n",
      "Spend time for 600 images: 102.890321 sec\n",
      "Train Epoch: 8, Loss: 0.004028\n",
      "Spend time for 600 images: 102.868195 sec\n",
      "Train Epoch: 8, Loss: 0.004144\n",
      "Spend time for 600 images: 102.841115 sec\n",
      "Train Epoch: 8, Loss: 0.004134\n",
      "Spend time for 600 images: 102.905348 sec\n",
      "Epoch 8, train loss: 0.004062, valid_loss: 0.004274, valid_metric: 0.995707\n",
      "Train Epoch: 9, Loss: 0.003810\n",
      "Spend time for 600 images: 102.771377 sec\n",
      "Train Epoch: 9, Loss: 0.003914\n",
      "Spend time for 600 images: 102.906848 sec\n",
      "Train Epoch: 9, Loss: 0.003936\n",
      "Spend time for 600 images: 102.820907 sec\n",
      "Train Epoch: 9, Loss: 0.004629\n",
      "Spend time for 600 images: 103.028251 sec\n",
      "Train Epoch: 9, Loss: 0.004002\n",
      "Spend time for 600 images: 102.938287 sec\n",
      "Train Epoch: 9, Loss: 0.003909\n",
      "Spend time for 600 images: 102.994847 sec\n",
      "Epoch 9, train loss: 0.004036, valid_loss: 0.004425, valid_metric: 0.995071\n",
      "Train Epoch: 10, Loss: 0.003842\n",
      "Spend time for 600 images: 102.820013 sec\n",
      "Train Epoch: 10, Loss: 0.003804\n",
      "Spend time for 600 images: 102.988816 sec\n",
      "Train Epoch: 10, Loss: 0.003904\n",
      "Spend time for 600 images: 102.770612 sec\n",
      "Train Epoch: 10, Loss: 0.003786\n",
      "Spend time for 600 images: 102.960863 sec\n",
      "Train Epoch: 10, Loss: 0.003769\n",
      "Spend time for 600 images: 102.874323 sec\n",
      "Train Epoch: 10, Loss: 0.003753\n",
      "Spend time for 600 images: 102.904952 sec\n",
      "Epoch 10, train loss: 0.003812, valid_loss: 0.004138, valid_metric: 0.995625\n",
      "Train Epoch: 11, Loss: 0.003669\n",
      "Spend time for 600 images: 102.894645 sec\n",
      "Train Epoch: 11, Loss: 0.003729\n",
      "Spend time for 600 images: 102.835581 sec\n",
      "Train Epoch: 11, Loss: 0.003812\n",
      "Spend time for 600 images: 102.902517 sec\n",
      "Train Epoch: 11, Loss: 0.003859\n",
      "Spend time for 600 images: 102.757926 sec\n",
      "Train Epoch: 11, Loss: 0.003730\n",
      "Spend time for 600 images: 102.956207 sec\n",
      "Train Epoch: 11, Loss: 0.003663\n",
      "Spend time for 600 images: 102.796537 sec\n",
      "Epoch 11, train loss: 0.003744, valid_loss: 0.004029, valid_metric: 0.995716\n",
      "Train Epoch: 12, Loss: 0.003558\n",
      "Spend time for 600 images: 102.903740 sec\n",
      "Train Epoch: 12, Loss: 0.003694\n",
      "Spend time for 600 images: 102.979065 sec\n",
      "Train Epoch: 12, Loss: 0.003656\n",
      "Spend time for 600 images: 102.975674 sec\n",
      "Train Epoch: 12, Loss: 0.003769\n",
      "Spend time for 600 images: 102.929806 sec\n",
      "Train Epoch: 12, Loss: 0.003709\n",
      "Spend time for 600 images: 102.861810 sec\n",
      "Train Epoch: 12, Loss: 0.003631\n",
      "Spend time for 600 images: 102.885229 sec\n",
      "Epoch 12, train loss: 0.003670, valid_loss: 0.003918, valid_metric: 0.995903\n",
      "Train Epoch: 13, Loss: 0.003614\n",
      "Spend time for 600 images: 102.777699 sec\n",
      "Train Epoch: 13, Loss: 0.003658\n",
      "Spend time for 600 images: 102.972456 sec\n",
      "Train Epoch: 13, Loss: 0.003582\n",
      "Spend time for 600 images: 102.803288 sec\n",
      "Train Epoch: 13, Loss: 0.003564\n",
      "Spend time for 600 images: 102.959316 sec\n",
      "Train Epoch: 13, Loss: 0.003576\n",
      "Spend time for 600 images: 102.908377 sec\n",
      "Train Epoch: 13, Loss: 0.003512\n",
      "Spend time for 600 images: 102.893164 sec\n",
      "Epoch 13, train loss: 0.003590, valid_loss: 0.003903, valid_metric: 0.995809\n",
      "Train Epoch: 14, Loss: 0.003494\n",
      "Spend time for 600 images: 102.935993 sec\n",
      "Train Epoch: 14, Loss: 0.003441\n",
      "Spend time for 600 images: 102.843394 sec\n",
      "Train Epoch: 14, Loss: 0.003474\n",
      "Spend time for 600 images: 102.974905 sec\n",
      "Train Epoch: 14, Loss: 0.003577\n",
      "Spend time for 600 images: 102.861075 sec\n",
      "Train Epoch: 14, Loss: 0.003583\n",
      "Spend time for 600 images: 103.034270 sec\n",
      "Train Epoch: 14, Loss: 0.003668\n",
      "Spend time for 600 images: 102.791337 sec\n",
      "Epoch 14, train loss: 0.003543, valid_loss: 0.003884, valid_metric: 0.995827\n",
      "Train Epoch: 15, Loss: 0.003476\n",
      "Spend time for 600 images: 102.851225 sec\n",
      "Train Epoch: 15, Loss: 0.003455\n",
      "Spend time for 600 images: 102.911299 sec\n",
      "Train Epoch: 15, Loss: 0.003475\n",
      "Spend time for 600 images: 102.887598 sec\n",
      "Train Epoch: 15, Loss: 0.003329\n",
      "Spend time for 600 images: 102.952394 sec\n",
      "Train Epoch: 15, Loss: 0.003536\n",
      "Spend time for 600 images: 102.940801 sec\n",
      "Train Epoch: 15, Loss: 0.003623\n",
      "Spend time for 600 images: 102.949599 sec\n",
      "Epoch 15, train loss: 0.003480, valid_loss: 0.003821, valid_metric: 0.995836\n",
      "Train Epoch: 16, Loss: 0.003388\n",
      "Spend time for 600 images: 102.842921 sec\n",
      "Train Epoch: 16, Loss: 0.003284\n",
      "Spend time for 600 images: 103.012790 sec\n",
      "Train Epoch: 16, Loss: 0.003484\n",
      "Spend time for 600 images: 102.742512 sec\n",
      "Train Epoch: 16, Loss: 0.003460\n",
      "Spend time for 600 images: 102.856931 sec\n",
      "Train Epoch: 16, Loss: 0.003490\n",
      "Spend time for 600 images: 102.818672 sec\n",
      "Train Epoch: 16, Loss: 0.003442\n",
      "Spend time for 600 images: 102.884129 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, train loss: 0.003421, valid_loss: 0.003827, valid_metric: 0.995883\n",
      "Train Epoch: 17, Loss: 0.003328\n",
      "Spend time for 600 images: 102.955935 sec\n",
      "Train Epoch: 17, Loss: 0.003587\n",
      "Spend time for 600 images: 102.872514 sec\n",
      "Train Epoch: 17, Loss: 0.003730\n",
      "Spend time for 600 images: 102.956046 sec\n",
      "Train Epoch: 17, Loss: 0.003462\n",
      "Spend time for 600 images: 102.795228 sec\n",
      "Train Epoch: 17, Loss: 0.003418\n",
      "Spend time for 600 images: 103.010399 sec\n",
      "Train Epoch: 17, Loss: 0.003394\n",
      "Spend time for 600 images: 102.774577 sec\n",
      "Epoch 17, train loss: 0.003485, valid_loss: 0.003874, valid_metric: 0.995726\n",
      "Train Epoch: 18, Loss: 0.003352\n",
      "Spend time for 600 images: 102.780909 sec\n",
      "Train Epoch: 18, Loss: 0.003281\n",
      "Spend time for 600 images: 102.904968 sec\n",
      "Train Epoch: 18, Loss: 0.003245\n",
      "Spend time for 600 images: 102.918228 sec\n",
      "Train Epoch: 18, Loss: 0.003350\n",
      "Spend time for 600 images: 102.927761 sec\n",
      "Train Epoch: 18, Loss: 0.003326\n",
      "Spend time for 600 images: 102.808670 sec\n",
      "Train Epoch: 18, Loss: 0.003275\n",
      "Spend time for 600 images: 102.908674 sec\n",
      "Epoch 18, train loss: 0.003308, valid_loss: 0.003816, valid_metric: 0.995974\n",
      "Train Epoch: 19, Loss: 0.003260\n",
      "Spend time for 600 images: 102.770791 sec\n",
      "Train Epoch: 19, Loss: 0.003268\n",
      "Spend time for 600 images: 102.908556 sec\n",
      "Train Epoch: 19, Loss: 0.003254\n",
      "Spend time for 600 images: 102.841929 sec\n",
      "Train Epoch: 19, Loss: 0.003264\n",
      "Spend time for 600 images: 102.949958 sec\n",
      "Train Epoch: 19, Loss: 0.003290\n",
      "Spend time for 600 images: 102.786449 sec\n",
      "Train Epoch: 19, Loss: 0.003320\n",
      "Spend time for 600 images: 102.827878 sec\n",
      "Epoch 19, train loss: 0.003278, valid_loss: 0.003749, valid_metric: 0.995993\n",
      "Train Epoch: 20, Loss: 0.003138\n",
      "Spend time for 600 images: 102.842089 sec\n",
      "Train Epoch: 20, Loss: 0.003210\n",
      "Spend time for 600 images: 102.716645 sec\n",
      "Train Epoch: 20, Loss: 0.003351\n",
      "Spend time for 600 images: 102.848479 sec\n",
      "Train Epoch: 20, Loss: 0.003360\n",
      "Spend time for 600 images: 102.755095 sec\n",
      "Train Epoch: 20, Loss: 0.003368\n",
      "Spend time for 600 images: 102.894840 sec\n",
      "Train Epoch: 20, Loss: 0.003229\n",
      "Spend time for 600 images: 102.664695 sec\n",
      "Epoch 20, train loss: 0.003276, valid_loss: 0.003769, valid_metric: 0.995965\n",
      "Train Epoch: 21, Loss: 0.003214\n",
      "Spend time for 600 images: 102.810054 sec\n",
      "Train Epoch: 21, Loss: 0.003202\n",
      "Spend time for 600 images: 102.924674 sec\n",
      "Train Epoch: 21, Loss: 0.003212\n",
      "Spend time for 600 images: 102.925119 sec\n",
      "Train Epoch: 21, Loss: 0.003200\n",
      "Spend time for 600 images: 102.867805 sec\n",
      "Train Epoch: 21, Loss: 0.003152\n",
      "Spend time for 600 images: 102.865183 sec\n",
      "Train Epoch: 21, Loss: 0.003194\n",
      "Spend time for 600 images: 102.878592 sec\n",
      "Epoch 21, train loss: 0.003193, valid_loss: 0.003745, valid_metric: 0.995889\n",
      "Train Epoch: 22, Loss: 0.003056\n",
      "Spend time for 600 images: 102.731285 sec\n",
      "Train Epoch: 22, Loss: 0.003219\n",
      "Spend time for 600 images: 102.931281 sec\n",
      "Train Epoch: 22, Loss: 0.003150\n",
      "Spend time for 600 images: 102.762988 sec\n",
      "Train Epoch: 22, Loss: 0.003485\n",
      "Spend time for 600 images: 102.875326 sec\n",
      "Train Epoch: 22, Loss: 0.003308\n",
      "Spend time for 600 images: 102.828077 sec\n",
      "Train Epoch: 22, Loss: 0.003234\n",
      "Spend time for 600 images: 102.884405 sec\n",
      "Epoch 22, train loss: 0.003236, valid_loss: 0.003707, valid_metric: 0.995951\n",
      "Train Epoch: 23, Loss: 0.003083\n",
      "Spend time for 600 images: 102.922218 sec\n",
      "Train Epoch: 23, Loss: 0.003078\n",
      "Spend time for 600 images: 102.817622 sec\n",
      "Train Epoch: 23, Loss: 0.003137\n",
      "Spend time for 600 images: 102.892871 sec\n",
      "Train Epoch: 23, Loss: 0.003144\n",
      "Spend time for 600 images: 102.820684 sec\n",
      "Train Epoch: 23, Loss: 0.003111\n",
      "Spend time for 600 images: 103.021866 sec\n",
      "Train Epoch: 23, Loss: 0.003222\n",
      "Spend time for 600 images: 102.807016 sec\n",
      "Epoch 23, train loss: 0.003126, valid_loss: 0.003775, valid_metric: 0.995802\n",
      "Train Epoch: 24, Loss: 0.003123\n",
      "Spend time for 600 images: 102.811329 sec\n",
      "Train Epoch: 24, Loss: 0.003073\n",
      "Spend time for 600 images: 102.880402 sec\n",
      "Train Epoch: 24, Loss: 0.003131\n",
      "Spend time for 600 images: 102.813519 sec\n",
      "Train Epoch: 24, Loss: 0.003081\n",
      "Spend time for 600 images: 102.866380 sec\n",
      "Train Epoch: 24, Loss: 0.003147\n",
      "Spend time for 600 images: 102.871608 sec\n",
      "Train Epoch: 24, Loss: 0.003085\n",
      "Spend time for 600 images: 102.972448 sec\n",
      "Epoch 24, train loss: 0.003111, valid_loss: 0.003742, valid_metric: 0.996041\n",
      "Train Epoch: 25, Loss: 0.003049\n",
      "Spend time for 600 images: 102.771864 sec\n",
      "Train Epoch: 25, Loss: 0.003058\n",
      "Spend time for 600 images: 102.956098 sec\n",
      "Train Epoch: 25, Loss: 0.003089\n",
      "Spend time for 600 images: 102.720660 sec\n",
      "Train Epoch: 25, Loss: 0.003046\n",
      "Spend time for 600 images: 102.950434 sec\n",
      "Train Epoch: 25, Loss: 0.003054\n",
      "Spend time for 600 images: 102.787610 sec\n",
      "Train Epoch: 25, Loss: 0.003061\n",
      "Spend time for 600 images: 102.921512 sec\n",
      "Epoch 25, train loss: 0.003059, valid_loss: 0.003754, valid_metric: 0.995950\n",
      "Train Epoch: 26, Loss: 0.003080\n",
      "Spend time for 600 images: 102.865230 sec\n",
      "Train Epoch: 26, Loss: 0.002981\n",
      "Spend time for 600 images: 102.814227 sec\n",
      "Train Epoch: 26, Loss: 0.003046\n",
      "Spend time for 600 images: 102.904316 sec\n",
      "Train Epoch: 26, Loss: 0.003020\n",
      "Spend time for 600 images: 102.776022 sec\n",
      "Train Epoch: 26, Loss: 0.002980\n",
      "Spend time for 600 images: 102.967988 sec\n",
      "Train Epoch: 26, Loss: 0.003058\n",
      "Spend time for 600 images: 102.804325 sec\n",
      "Epoch 26, train loss: 0.003037, valid_loss: 0.003798, valid_metric: 0.995780\n",
      "Train Epoch: 27, Loss: 0.002953\n",
      "Spend time for 600 images: 102.825697 sec\n",
      "Train Epoch: 27, Loss: 0.003030\n",
      "Spend time for 600 images: 102.792932 sec\n",
      "Train Epoch: 27, Loss: 0.003026\n",
      "Spend time for 600 images: 102.821481 sec\n",
      "Train Epoch: 27, Loss: 0.003051\n",
      "Spend time for 600 images: 102.824321 sec\n",
      "Train Epoch: 27, Loss: 0.003011\n",
      "Spend time for 600 images: 102.817797 sec\n",
      "Train Epoch: 27, Loss: 0.002999\n",
      "Spend time for 600 images: 102.834388 sec\n",
      "Epoch 27, train loss: 0.003011, valid_loss: 0.003852, valid_metric: 0.995611\n",
      "Train Epoch: 28, Loss: 0.002883\n",
      "Spend time for 600 images: 102.789587 sec\n",
      "Train Epoch: 28, Loss: 0.002952\n",
      "Spend time for 600 images: 102.941939 sec\n",
      "Train Epoch: 28, Loss: 0.002974\n",
      "Spend time for 600 images: 102.878685 sec\n",
      "Train Epoch: 28, Loss: 0.002986\n",
      "Spend time for 600 images: 102.924363 sec\n",
      "Train Epoch: 28, Loss: 0.003048\n",
      "Spend time for 600 images: 102.739422 sec\n",
      "Train Epoch: 28, Loss: 0.003074\n",
      "Spend time for 600 images: 102.831355 sec\n",
      "Epoch 28, train loss: 0.002981, valid_loss: 0.003681, valid_metric: 0.995943\n",
      "Train Epoch: 29, Loss: 0.002962\n",
      "Spend time for 600 images: 102.926021 sec\n",
      "Train Epoch: 29, Loss: 0.002939\n",
      "Spend time for 600 images: 102.998565 sec\n",
      "Train Epoch: 29, Loss: 0.002899\n",
      "Spend time for 600 images: 102.807817 sec\n",
      "Train Epoch: 29, Loss: 0.002978\n",
      "Spend time for 600 images: 102.938589 sec\n",
      "Train Epoch: 29, Loss: 0.002969\n",
      "Spend time for 600 images: 102.806652 sec\n",
      "Train Epoch: 29, Loss: 0.002916\n",
      "Spend time for 600 images: 102.900103 sec\n",
      "Epoch 29, train loss: 0.002942, valid_loss: 0.003679, valid_metric: 0.996037\n",
      "Train Epoch: 30, Loss: 0.002907\n",
      "Spend time for 600 images: 102.909055 sec\n",
      "Train Epoch: 30, Loss: 0.002897\n",
      "Spend time for 600 images: 102.814234 sec\n",
      "Train Epoch: 30, Loss: 0.002924\n",
      "Spend time for 600 images: 102.932314 sec\n",
      "Train Epoch: 30, Loss: 0.002909\n",
      "Spend time for 600 images: 102.718750 sec\n",
      "Train Epoch: 30, Loss: 0.002932\n",
      "Spend time for 600 images: 102.920134 sec\n",
      "Train Epoch: 30, Loss: 0.002885\n",
      "Spend time for 600 images: 102.751179 sec\n",
      "Epoch 30, train loss: 0.002916, valid_loss: 0.003709, valid_metric: 0.996013\n"
     ]
    }
   ],
   "source": [
    "result = my_model.fit(criterion,\n",
    "             metric,\n",
    "             optimizer,\n",
    "             train_data_loader,\n",
    "             valid_data_loader,\n",
    "             epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d15ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 30, train loss: 0.002916, valid_loss: 0.003709, valid_metric: 0.996013 - model_lab_v2.pth - no aug, \n",
    "# no nomalize\n",
    "# Сохраняем веса обученной модели\n",
    "my_model.save(path_to_save = './model_lab_v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e78f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:19: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:19: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:31: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:32: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)\n"
     ]
    }
   ],
   "source": [
    "# Сохраняем оттрассированную модель\n",
    "my_model.trace_save(path_to_save = './model_lab_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9bc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/lib/python3.8/site-packages/torch/onnx/utils.py:88: UserWarning: `enable_onnx_checker' is deprecated and ignored. It will be removed inthe next PyTorch release. To proceed despite ONNX checker failures, youcan catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"`enable_onnx_checker' is deprecated and ignored. It will be removed in\"\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:381: UserWarning: You are trying to export the model with onnx:Resize for ONNX opset version 10. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator.\n",
      "  warnings.warn(\"You are trying to export the model with \" + onnx_op + \" for ONNX opset version \"\n"
     ]
    }
   ],
   "source": [
    "# Экспорт модели в onnx\n",
    "my_model.onnx_save(path_to_save = './carvana_model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700bfa",
   "metadata": {},
   "source": [
    "## Загрузка сохраненной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6a384ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводим модель по известной архитектуре и сохраненным весам\n",
    "model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "                 encoder_weights='imagenet', decoder_channels = [256, 128, 64, 32, 16]).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8d909e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводим модель по известной архитектуре и сохраненным весам\n",
    "model = smp.DeepLabV3Plus(encoder_name='mobilenet_v2', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model_deeplab_30epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb8d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем оттрассированную модель\n",
    "my_model = torch.jit.load('./model.pt')\n",
    "my_model = NeuralNetwork(model=my_model)\n",
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988cd6",
   "metadata": {},
   "source": [
    "## Предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a31d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "test_dataset = '/home/dima/carvana_dataset/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6cc16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}\n",
    "test_dataframe['img_addr'] = list(glob.glob(test_dataset + \"/*\"))\n",
    "test_dataframe = pd.DataFrame(test_dataframe)\n",
    "test_dataframe['img_name'] = test_dataframe['img_addr'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b8aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDatasetForTest(test_dataframe, device, valid_transform)\n",
    "test_data_loader = DataLoader(test_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51507db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:3679: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4852/2166412596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# С сохранением сгенерированных масок в predict_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                  mask_treashold=mask_treashold, generate_rle_dataframe=True)\n",
      "\u001b[0;32m/tmp/ipykernel_4852/1186113508.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_data_loader, predict_directory, output_size, mask_treashold, generate_rle_dataframe)\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# [0] - избавляемся от батч размерности\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0mPIL_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                     \u001b[0mPIL_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_directory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.gif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;31m# Если требуется, получаем значения rle для каждой картинки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2235\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2236\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, save_all)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_all\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0m_write_single_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# end of file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_write_single_frame\u001b[0;34m(im, fp, palette)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mim_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mim_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_global_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_normalize_palette\u001b[0;34m(im, palette, info)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_palette\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mused_palette_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mused_palette_colors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremap_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_palette_colors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_palette\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_get_optimize\u001b[0;34m(im, info)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0;31m# check which colors are used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0mused_palette_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     \u001b[0mused_palette_colors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(self, mask, extrema)\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0mextrema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetextrema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextrema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextrema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# С сохранением сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acf87e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 600\n",
      "Total time: 40.21 sec\n",
      "Time to process 600 images: 40.21 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 1200\n",
      "Total time: 80.15 sec\n",
      "Time to process 600 images: 39.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 1800\n",
      "Total time: 120.06 sec\n",
      "Time to process 600 images: 39.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 2400\n",
      "Total time: 160.14 sec\n",
      "Time to process 600 images: 40.08 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 3000\n",
      "Total time: 200.75 sec\n",
      "Time to process 600 images: 40.61 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 3600\n",
      "Total time: 241.01 sec\n",
      "Time to process 600 images: 40.26 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 4200\n",
      "Total time: 281.32 sec\n",
      "Time to process 600 images: 40.31 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 4800\n",
      "Total time: 321.69 sec\n",
      "Time to process 600 images: 40.37 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 5400\n",
      "Total time: 361.88 sec\n",
      "Time to process 600 images: 40.18 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 6000\n",
      "Total time: 401.70 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 6600\n",
      "Total time: 441.45 sec\n",
      "Time to process 600 images: 39.76 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 7200\n",
      "Total time: 481.12 sec\n",
      "Time to process 600 images: 39.67 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 7800\n",
      "Total time: 521.02 sec\n",
      "Time to process 600 images: 39.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 8400\n",
      "Total time: 560.84 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 9000\n",
      "Total time: 600.72 sec\n",
      "Time to process 600 images: 39.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 9600\n",
      "Total time: 640.94 sec\n",
      "Time to process 600 images: 40.22 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 10200\n",
      "Total time: 680.88 sec\n",
      "Time to process 600 images: 39.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 10800\n",
      "Total time: 720.57 sec\n",
      "Time to process 600 images: 39.69 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 11400\n",
      "Total time: 760.29 sec\n",
      "Time to process 600 images: 39.72 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 12000\n",
      "Total time: 800.00 sec\n",
      "Time to process 600 images: 39.72 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 12600\n",
      "Total time: 839.85 sec\n",
      "Time to process 600 images: 39.84 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 13200\n",
      "Total time: 879.69 sec\n",
      "Time to process 600 images: 39.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 13800\n",
      "Total time: 919.55 sec\n",
      "Time to process 600 images: 39.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 14400\n",
      "Total time: 959.34 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 15000\n",
      "Total time: 999.09 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 15600\n",
      "Total time: 1038.89 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 16200\n",
      "Total time: 1078.61 sec\n",
      "Time to process 600 images: 39.73 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 16800\n",
      "Total time: 1118.32 sec\n",
      "Time to process 600 images: 39.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 17400\n",
      "Total time: 1158.14 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 18000\n",
      "Total time: 1197.76 sec\n",
      "Time to process 600 images: 39.62 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 18600\n",
      "Total time: 1237.41 sec\n",
      "Time to process 600 images: 39.65 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 19200\n",
      "Total time: 1277.07 sec\n",
      "Time to process 600 images: 39.66 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 19800\n",
      "Total time: 1316.69 sec\n",
      "Time to process 600 images: 39.62 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 20400\n",
      "Total time: 1356.34 sec\n",
      "Time to process 600 images: 39.65 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 21000\n",
      "Total time: 1395.97 sec\n",
      "Time to process 600 images: 39.62 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 21600\n",
      "Total time: 1435.48 sec\n",
      "Time to process 600 images: 39.51 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 22200\n",
      "Total time: 1474.99 sec\n",
      "Time to process 600 images: 39.52 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 22800\n",
      "Total time: 1514.52 sec\n",
      "Time to process 600 images: 39.53 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 23400\n",
      "Total time: 1554.07 sec\n",
      "Time to process 600 images: 39.54 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 24000\n",
      "Total time: 1593.70 sec\n",
      "Time to process 600 images: 39.64 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 24600\n",
      "Total time: 1633.34 sec\n",
      "Time to process 600 images: 39.63 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 25200\n",
      "Total time: 1672.89 sec\n",
      "Time to process 600 images: 39.55 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 25800\n",
      "Total time: 1712.47 sec\n",
      "Time to process 600 images: 39.58 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 26400\n",
      "Total time: 1752.30 sec\n",
      "Time to process 600 images: 39.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 27000\n",
      "Total time: 1792.42 sec\n",
      "Time to process 600 images: 40.12 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 27600\n",
      "Total time: 1832.31 sec\n",
      "Time to process 600 images: 39.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 28200\n",
      "Total time: 1872.14 sec\n",
      "Time to process 600 images: 39.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 28800\n",
      "Total time: 1912.02 sec\n",
      "Time to process 600 images: 39.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 29400\n",
      "Total time: 1951.71 sec\n",
      "Time to process 600 images: 39.69 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 30000\n",
      "Total time: 1991.45 sec\n",
      "Time to process 600 images: 39.74 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 30600\n",
      "Total time: 2031.91 sec\n",
      "Time to process 600 images: 40.46 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 31200\n",
      "Total time: 2073.36 sec\n",
      "Time to process 600 images: 41.44 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 31800\n",
      "Total time: 2113.20 sec\n",
      "Time to process 600 images: 39.84 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 32400\n",
      "Total time: 2153.03 sec\n",
      "Time to process 600 images: 39.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 33000\n",
      "Total time: 2192.94 sec\n",
      "Time to process 600 images: 39.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 33600\n",
      "Total time: 2232.82 sec\n",
      "Time to process 600 images: 39.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 34200\n",
      "Total time: 2273.09 sec\n",
      "Time to process 600 images: 40.27 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 34800\n",
      "Total time: 2313.66 sec\n",
      "Time to process 600 images: 40.57 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 35400\n",
      "Total time: 2353.51 sec\n",
      "Time to process 600 images: 39.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 36000\n",
      "Total time: 2393.31 sec\n",
      "Time to process 600 images: 39.80 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 36600\n",
      "Total time: 2433.05 sec\n",
      "Time to process 600 images: 39.74 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 37200\n",
      "Total time: 2472.91 sec\n",
      "Time to process 600 images: 39.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 37800\n",
      "Total time: 2512.74 sec\n",
      "Time to process 600 images: 39.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 38400\n",
      "Total time: 2552.55 sec\n",
      "Time to process 600 images: 39.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 39000\n",
      "Total time: 2592.35 sec\n",
      "Time to process 600 images: 39.80 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 39600\n",
      "Total time: 2632.10 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 40200\n",
      "Total time: 2671.97 sec\n",
      "Time to process 600 images: 39.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 40800\n",
      "Total time: 2711.72 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 41400\n",
      "Total time: 2751.45 sec\n",
      "Time to process 600 images: 39.73 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 42000\n",
      "Total time: 2791.24 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 42600\n",
      "Total time: 2831.03 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 43200\n",
      "Total time: 2870.80 sec\n",
      "Time to process 600 images: 39.77 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 43800\n",
      "Total time: 2910.54 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 44400\n",
      "Total time: 2950.46 sec\n",
      "Time to process 600 images: 39.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 45000\n",
      "Total time: 2990.30 sec\n",
      "Time to process 600 images: 39.84 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 45600\n",
      "Total time: 3030.09 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 46200\n",
      "Total time: 3069.91 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 46800\n",
      "Total time: 3109.73 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 47400\n",
      "Total time: 3149.48 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 48000\n",
      "Total time: 3189.37 sec\n",
      "Time to process 600 images: 39.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 48600\n",
      "Total time: 3229.14 sec\n",
      "Time to process 600 images: 39.77 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 49200\n",
      "Total time: 3268.97 sec\n",
      "Time to process 600 images: 39.84 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 49800\n",
      "Total time: 3308.86 sec\n",
      "Time to process 600 images: 39.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 50400\n",
      "Total time: 3348.65 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 51000\n",
      "Total time: 3388.39 sec\n",
      "Time to process 600 images: 39.74 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 51600\n",
      "Total time: 3428.14 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 52200\n",
      "Total time: 3467.85 sec\n",
      "Time to process 600 images: 39.72 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 52800\n",
      "Total time: 3507.58 sec\n",
      "Time to process 600 images: 39.72 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 53400\n",
      "Total time: 3547.36 sec\n",
      "Time to process 600 images: 39.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 54000\n",
      "Total time: 3587.12 sec\n",
      "Time to process 600 images: 39.76 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 54600\n",
      "Total time: 3626.89 sec\n",
      "Time to process 600 images: 39.77 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 55200\n",
      "Total time: 3666.66 sec\n",
      "Time to process 600 images: 39.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 55800\n",
      "Total time: 3706.33 sec\n",
      "Time to process 600 images: 39.66 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 56400\n",
      "Total time: 3746.03 sec\n",
      "Time to process 600 images: 39.70 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 57000\n",
      "Total time: 3785.77 sec\n",
      "Time to process 600 images: 39.74 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 57600\n",
      "Total time: 3825.44 sec\n",
      "Time to process 600 images: 39.68 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 58200\n",
      "Total time: 3865.15 sec\n",
      "Time to process 600 images: 39.70 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 58800\n",
      "Total time: 3904.96 sec\n",
      "Time to process 600 images: 39.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 59400\n",
      "Total time: 3944.70 sec\n",
      "Time to process 600 images: 39.74 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 60000\n",
      "Total time: 3984.42 sec\n",
      "Time to process 600 images: 39.72 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 60600\n",
      "Total time: 4024.12 sec\n",
      "Time to process 600 images: 39.70 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 61200\n",
      "Total time: 4063.81 sec\n",
      "Time to process 600 images: 39.69 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 61800\n",
      "Total time: 4103.53 sec\n",
      "Time to process 600 images: 39.72 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 62400\n",
      "Total time: 4143.24 sec\n",
      "Time to process 600 images: 39.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 63000\n",
      "Total time: 4182.89 sec\n",
      "Time to process 600 images: 39.64 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 63600\n",
      "Total time: 4222.57 sec\n",
      "Time to process 600 images: 39.68 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 64200\n",
      "Total time: 4262.28 sec\n",
      "Time to process 600 images: 39.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 64800\n",
      "Total time: 4301.97 sec\n",
      "Time to process 600 images: 39.69 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 65400\n",
      "Total time: 4341.65 sec\n",
      "Time to process 600 images: 39.67 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 66000\n",
      "Total time: 4381.38 sec\n",
      "Time to process 600 images: 39.74 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 66600\n",
      "Total time: 4421.11 sec\n",
      "Time to process 600 images: 39.73 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 67200\n",
      "Total time: 4460.86 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 67800\n",
      "Total time: 4500.65 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 68400\n",
      "Total time: 4540.35 sec\n",
      "Time to process 600 images: 39.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 69000\n",
      "Total time: 4580.10 sec\n",
      "Time to process 600 images: 39.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 69600\n",
      "Total time: 4619.86 sec\n",
      "Time to process 600 images: 39.76 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 70200\n",
      "Total time: 4659.62 sec\n",
      "Time to process 600 images: 39.76 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 70800\n",
      "Total time: 4699.29 sec\n",
      "Time to process 600 images: 39.67 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 71400\n",
      "Total time: 4738.98 sec\n",
      "Time to process 600 images: 39.69 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 72000\n",
      "Total time: 4778.69 sec\n",
      "Time to process 600 images: 39.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 72600\n",
      "Total time: 4818.42 sec\n",
      "Time to process 600 images: 39.73 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 73200\n",
      "Total time: 4858.34 sec\n",
      "Time to process 600 images: 39.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 73800\n",
      "Total time: 4898.19 sec\n",
      "Time to process 600 images: 39.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 74400\n",
      "Total time: 4938.00 sec\n",
      "Time to process 600 images: 39.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 75000\n",
      "Total time: 4977.87 sec\n",
      "Time to process 600 images: 39.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 75600\n",
      "Total time: 5017.66 sec\n",
      "Time to process 600 images: 39.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 76200\n",
      "Total time: 5057.44 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 76800\n",
      "Total time: 5097.29 sec\n",
      "Time to process 600 images: 39.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 77400\n",
      "Total time: 5137.11 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 78000\n",
      "Total time: 5176.90 sec\n",
      "Time to process 600 images: 39.80 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 78600\n",
      "Total time: 5216.68 sec\n",
      "Time to process 600 images: 39.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 79200\n",
      "Total time: 5256.42 sec\n",
      "Time to process 600 images: 39.74 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 79800\n",
      "Total time: 5296.23 sec\n",
      "Time to process 600 images: 39.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 80400\n",
      "Total time: 5336.06 sec\n",
      "Time to process 600 images: 39.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 81000\n",
      "Total time: 5375.89 sec\n",
      "Time to process 600 images: 39.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 81600\n",
      "Total time: 5415.67 sec\n",
      "Time to process 600 images: 39.77 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 82200\n",
      "Total time: 5455.45 sec\n",
      "Time to process 600 images: 39.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 82800\n",
      "Total time: 5495.30 sec\n",
      "Time to process 600 images: 39.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 83400\n",
      "Total time: 5535.64 sec\n",
      "Time to process 600 images: 40.34 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 84000\n",
      "Total time: 5576.10 sec\n",
      "Time to process 600 images: 40.46 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 84600\n",
      "Total time: 5616.65 sec\n",
      "Time to process 600 images: 40.55 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 85200\n",
      "Total time: 5656.66 sec\n",
      "Time to process 600 images: 40.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 85800\n",
      "Total time: 5696.71 sec\n",
      "Time to process 600 images: 40.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 86400\n",
      "Total time: 5736.50 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 87000\n",
      "Total time: 5776.29 sec\n",
      "Time to process 600 images: 39.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 87600\n",
      "Total time: 5816.13 sec\n",
      "Time to process 600 images: 39.84 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 88200\n",
      "Total time: 5856.04 sec\n",
      "Time to process 600 images: 39.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 88800\n",
      "Total time: 5895.91 sec\n",
      "Time to process 600 images: 39.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 89400\n",
      "Total time: 5935.76 sec\n",
      "Time to process 600 images: 39.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 90000\n",
      "Total time: 5975.71 sec\n",
      "Time to process 600 images: 39.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 90600\n",
      "Total time: 6015.70 sec\n",
      "Time to process 600 images: 39.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 91200\n",
      "Total time: 6055.52 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 91800\n",
      "Total time: 6095.38 sec\n",
      "Time to process 600 images: 39.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 92400\n",
      "Total time: 6135.16 sec\n",
      "Time to process 600 images: 39.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 93000\n",
      "Total time: 6175.04 sec\n",
      "Time to process 600 images: 39.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 93600\n",
      "Total time: 6214.88 sec\n",
      "Time to process 600 images: 39.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 94200\n",
      "Total time: 6254.70 sec\n",
      "Time to process 600 images: 39.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 94800\n",
      "Total time: 6294.66 sec\n",
      "Time to process 600 images: 39.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 95400\n",
      "Total time: 6334.57 sec\n",
      "Time to process 600 images: 39.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 96000\n",
      "Total time: 6374.38 sec\n",
      "Time to process 600 images: 39.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 96600\n",
      "Total time: 6414.25 sec\n",
      "Time to process 600 images: 39.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 97200\n",
      "Total time: 6454.06 sec\n",
      "Time to process 600 images: 39.80 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 97800\n",
      "Total time: 6493.87 sec\n",
      "Time to process 600 images: 39.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 98400\n",
      "Total time: 6534.04 sec\n",
      "Time to process 600 images: 40.17 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 99000\n",
      "Total time: 6574.52 sec\n",
      "Time to process 600 images: 40.48 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 99600\n",
      "Total time: 6614.77 sec\n",
      "Time to process 600 images: 40.25 sec\n"
     ]
    }
   ],
   "source": [
    "# Без сохранения сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ad9e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем датафрейм с результатом для заливки на kaggle\n",
    "rle_dataframe.to_csv('rle_dataframe.csv', index=True)\n",
    "sample_submission = pd.read_csv('/home/dima/carvana_dataset/sample_submission.csv')\n",
    "sample_submission = sample_submission.merge(rle_dataframe, how='left', left_on='img', right_on='img_name')\n",
    "sample_submission.drop(columns=['rle_mask', 'img_name'], inplace=True)\n",
    "sample_submission.rename(columns={'img_rle': 'rle_mask'}, inplace=True)\n",
    "sample_submission.to_csv('submission_10_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81effd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6cde352f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100064, 2)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43041e5d",
   "metadata": {},
   "source": [
    "## Сравнение интерполяций исходных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edb4e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33b9e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводим модель по известной архитектуре и сохраненным весам\n",
    "model = smp.DeepLabV3Plus(encoder_name='mobilenet_v2', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model_deeplab_30epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ccf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81425d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform_area = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_AREA),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform_linear = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_LINEAR),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5073c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c50436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "valid_data_area = CustomDatasetForTrain(valid_df, device, valid_transform_area, skip_mask=True)\n",
    "valid_data_loader_area = DataLoader(valid_data_area, batch_size=2, shuffle=False)\n",
    "\n",
    "valid_data_linear = CustomDatasetForTrain(valid_df, device, valid_transform_linear, skip_mask=True)\n",
    "valid_data_loader_linear = DataLoader(valid_data_linear, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09853f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_area = []\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader_area):\n",
    "\n",
    "    out_area = my_model(data)\n",
    "    out_area = F.interpolate(input=out_area, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    dice_area = dice(out_area, labels)\n",
    "    dices_area.append(dice_area.item())\n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71da6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_area = np.mean(dices_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3219f5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958455577492714"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "167a98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_linear = []\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader_linear):\n",
    "\n",
    "    out_linear = my_model(data)\n",
    "    out_linear = F.interpolate(input=out_linear, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    dice_linear = dice(out_linear, labels)\n",
    "    dices_linear.append(dice_linear.item())\n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b90446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_linear = np.mean(dices_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72f3b6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958532294258475"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad227b0",
   "metadata": {},
   "source": [
    "### Вывод: INTER_AREA дает примерно такой же результат как и INTER_LINEAR, но работает быстрее, поэтому используем INTER_AREA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d32e87",
   "metadata": {},
   "source": [
    "## Сравнение интерполяций результата"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c9422",
   "metadata": {},
   "source": [
    "### Для 1000 батчей с усреднением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f6118d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5efc0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_nearest = []\n",
    "dices_bilinear_align = []\n",
    "dices_bicubic_align = []\n",
    "dices_bilinear = []\n",
    "dices_bicubic = []\n",
    "\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "\n",
    "    out = my_model(data)\n",
    "    \n",
    "    output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "    output_bilinear_align = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=True)\n",
    "    output_bicubic_align = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=True)\n",
    "    output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=False)\n",
    "\n",
    "    dice_nearest = dice(output_nearest, labels)\n",
    "    dice_bilinear_align = dice(output_bilinear_align, labels)\n",
    "    dice_bicubic_align = dice(output_bicubic_align, labels)\n",
    "    dice_bilinear = dice(output_bilinear, labels)\n",
    "    dice_bicubic = dice(output_bicubic, labels)\n",
    "    \n",
    "    \n",
    "    dices_nearest.append(dice_nearest.item())\n",
    "    dices_bilinear_align.append(dice_bilinear_align.item())\n",
    "    dices_bicubic_align.append(dice_bicubic_align.item())\n",
    "    dices_bilinear.append(dice_bilinear.item())\n",
    "    dices_bicubic.append(dice_bicubic.item())\n",
    "    \n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f80c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dices_nearest: 0.9950660129077733\n",
      "dices_bilinear_align: 0.9958146193996071\n",
      "dices_bicubic_align: 0.9958100168034434\n",
      "dices_bilinear: 0.9958423018455506\n",
      "dices_bicubic: 0.9958409286104143\n"
     ]
    }
   ],
   "source": [
    "print(f'dices_nearest: {np.mean(dices_nearest)}')\n",
    "print(f'dices_bilinear_align: {np.mean(dices_bilinear_align)}')\n",
    "print(f'dices_bicubic_align: {np.mean(dices_bicubic_align)}')\n",
    "print(f'dices_bilinear: {np.mean(dices_bilinear)}')\n",
    "print(f'dices_bicubic: {np.mean(dices_bicubic)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b4d08",
   "metadata": {},
   "source": [
    "### Для одного батча с сохранением картинок для просмотра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ecbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "iterator = iter(valid_data_loader)\n",
    "input_tensor = iterator.next()\n",
    "out = my_model.model(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "311eb4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice_nearest: 0.9952350854873657\n",
      "dice_bilinear: 0.9960499405860901\n",
      "dice_bicubic: 0.9960504770278931\n"
     ]
    }
   ],
   "source": [
    "output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=True)\n",
    "output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=True)\n",
    "\n",
    "\n",
    "dice_nearest = dice(output_nearest, input_tensor[2])\n",
    "dice_bilinear = dice(output_bilinear, input_tensor[2])\n",
    "dice_bicubic = dice(output_bicubic, input_tensor[2])\n",
    "\n",
    "print(f'dice_nearest: {dice_nearest}')\n",
    "print(f'dice_bilinear: {dice_bilinear}')\n",
    "print(f'dice_bicubic: {dice_bicubic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56be51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice_nearest: 0.9952350854873657\n",
      "dice_bilinear: 0.9961531162261963\n",
      "dice_bicubic: 0.9961585998535156\n"
     ]
    }
   ],
   "source": [
    "output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=False)\n",
    "\n",
    "dice = DiceMetric()\n",
    "dice_nearest = dice(output_nearest, input_tensor[2])\n",
    "dice_bilinear = dice(output_bilinear, input_tensor[2])\n",
    "dice_bicubic = dice(output_bicubic, input_tensor[2])\n",
    "\n",
    "print(f'dice_nearest: {dice_nearest}')\n",
    "print(f'dice_bilinear: {dice_bilinear}')\n",
    "print(f'dice_bicubic: {dice_bicubic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cdad3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.sigmoid(output_nearest)\n",
    "output_bilinear = torch.sigmoid(output_bilinear)\n",
    "output_bicubic = torch.sigmoid(output_bicubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff4426c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.where(output_nearest > 0.5, 1, 0)\n",
    "output_bilinear = torch.where(output_bilinear > 0.5, 1, 0)\n",
    "output_bicubic = torch.where(output_bicubic > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74349240",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = (output_nearest[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_nearest = Image.fromarray(output_nearest.astype('uint8'), 'L')\n",
    "output_nearest.save((predict_directory+'111').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e05a6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bilinear = (output_bilinear[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_bilinear = Image.fromarray(output_bilinear.astype('uint8'), 'L')\n",
    "output_bilinear.save((predict_directory+'222').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33ccdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bicubic = (output_bicubic[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_bicubic = Image.fromarray(output_bicubic.astype('uint8'), 'L')\n",
    "output_bicubic.save((predict_directory+'333').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a30d2a",
   "metadata": {},
   "source": [
    "### Вывод: лучше использовать bilinear с align_corners = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
