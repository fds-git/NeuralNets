{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b5928f",
   "metadata": {},
   "source": [
    "# Production ready for Carvana_learning_local_research "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4aaf",
   "metadata": {},
   "source": [
    "## Подключение библиотек и загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f662c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models_pytorch in ./anaconda3/lib/python3.8/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: efficientnet-pytorch==0.6.3 in ./anaconda3/lib/python3.8/site-packages (from segmentation_models_pytorch) (0.6.3)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in ./anaconda3/lib/python3.8/site-packages (from segmentation_models_pytorch) (0.11.0.dev20210924)\r\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in ./anaconda3/lib/python3.8/site-packages (from segmentation_models_pytorch) (0.7.4)\r\n",
      "Requirement already satisfied: timm==0.4.12 in ./anaconda3/lib/python3.8/site-packages (from segmentation_models_pytorch) (0.4.12)\r\n",
      "Requirement already satisfied: torch in ./anaconda3/lib/python3.8/site-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.11.0.dev20210924)\r\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.62.1)\r\n",
      "Requirement already satisfied: munch in ./anaconda3/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\r\n",
      "Requirement already satisfied: typing_extensions in ./anaconda3/lib/python3.8/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (3.10.0.0)\r\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.8/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.20.3)\r\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in ./anaconda3/lib/python3.8/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (8.3.1)\r\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.8/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import glob\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6386d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполнять, если датасет не загружен\n",
    "!pip install -q kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!cp ~/kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c carvana-image-masking-challenge\n",
    "!unzip ~/carvana-image-masking-challenge.zip ~/carvana_dataset/\n",
    "\n",
    "!unzip ~/carvana_dataset/train.zip -d ~/carvana_dataset/train\n",
    "!unzip ~/carvana_dataset/test.zip -d ~/carvana_dataset/test\n",
    "!unzip ~/carvana_dataset/train_masks.zip -d ~/carvana_dataset/train_masks\n",
    "\n",
    "!unzip ~/carvana_dataset/train_hq.zip -d ~/carvana_dataset/train_hq\n",
    "!unzip ~/carvana_dataset/test_hq.zip -d ~/carvana_dataset/test_hq\n",
    "\n",
    "!unzip ~/carvana_dataset/train_masks.csv.zip  ~/carvana_dataset/\n",
    "!unzip ~/carvana_dataset/sample_submission.csv.zip  ~/carvana_dataset/\n",
    "!unzip ~/carvana_dataset/metadata.csv.zip  ~/carvana_dataset/\n",
    "\n",
    "!rm ~/carvana-image-masking-challenge.zip\n",
    "!rm ~/carvana_dataset/test.zip\n",
    "!rm ~/carvana_dataset/train_masks.zip\n",
    "!rm ~/carvana_dataset/train.zip\n",
    "!rm ~/carvana_dataset/test_hq.zip\n",
    "!rm ~/carvana_dataset/train_hq.zip\n",
    "!rm ~/carvana_dataset/train_masks.csv.zip\n",
    "!rm ~/carvana_dataset/sample_submission.csv.zip\n",
    "!rm ~/carvana_dataset/metadata.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3608d7",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec78c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(imgs_path: str = None, masks_path: str = None) -> pd.DataFrame:\n",
    "    '''Funtion gets images from imgs_path and masks from masks_path\n",
    "    and generates pd.DataFrame, contains links to images and masks related \n",
    "    to a certain photo of car\n",
    "  \n",
    "    input parameters:\n",
    "    string: imgs_path - path to folder with images,\n",
    "    masks_path - path to folder with masks\n",
    "  \n",
    "    output parameters:\n",
    "    pd.DataFrame: data - dataframe, contains links to images and masks'''\n",
    "\n",
    "    assert (imgs_path != None) & (masks_path != None)\n",
    "    # imgs_path or masks_path is equal None\n",
    "\n",
    "    data_img = {}\n",
    "    data_mask = {}\n",
    "    data_img['imgs_path'] = []\n",
    "    data_mask['masks_path'] = []\n",
    "    data_img['imgs_path'] = list(glob.glob(imgs_path + \"/*\"))\n",
    "    data_mask['masks_path'] = list(glob.glob(masks_path + \"/*\"))\n",
    "\n",
    "    data_img = pd.DataFrame(data_img)\n",
    "    data_mask = pd.DataFrame(data_mask)\n",
    "\n",
    "    def file_name(x):\n",
    "        return x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    data_img[\"file_name\"] = data_img[\"imgs_path\"].apply(lambda x: file_name(x))\n",
    "    data_mask[\"file_name\"] = data_mask[\"masks_path\"].apply(lambda x: file_name(x)[:-5])\n",
    "\n",
    "    data = pd.merge(data_img, data_mask, on = \"file_name\", how = \"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5613d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(source_df: pd.DataFrame, separate_feature: str = None, test_size: int = 0.25) -> pd.DataFrame:\n",
    "    '''Function get source_df and split it on train and valid pd.DataFrame \n",
    "    with test_size coefficient. If separate_feature not None, splitting will \n",
    "    be on unique values of that feature\n",
    "\n",
    "    input parameters:\n",
    "    source_df: pd.DataFrame - datafraim that will be splitted\n",
    "\n",
    "    separate_feature: str - datafraim will \n",
    "    be splitted on unique values of that feature\n",
    "\n",
    "    test_size: int - splitting coefficient\n",
    "  \n",
    "    output parameters:\n",
    "    pd.DataFrame: data - dataframe, contains links to images and masks'''\n",
    "  \n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_cars, valid_cars = train_test_split(data[separate_feature].unique(), test_size=test_size, random_state=42)\n",
    "        data_valid = data[np.isin(data[separate_feature].values, valid_cars)]\n",
    "        data_train = data[np.isin(data[separate_feature].values, train_cars)]\n",
    "        assert data.shape[0] == (data_valid.shape[0] + data_train.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "    else:\n",
    "        data_train, data_valid = train_test_split(data, test_size=test_size)\n",
    "\n",
    "    return data_train, data_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712baa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(logits, targets):\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        outputs = torch.where(probs > 0.5, 1, 0)\n",
    "        m1 = outputs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd108c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменить np на torch\n",
    "def tensor_to_rle(tensor):\n",
    "    # We avoid issues with '1' at the start or end (at the corners of \n",
    "    # the original image) by setting those pixels to '0' explicitly.\n",
    "    # We do not expect these to be non-zero for an accurate mask, \n",
    "    # so this should not harm the score.\n",
    "    tensor = tensor.view(1, -1)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor[0] = 0\n",
    "    tensor[-1] = 0\n",
    "    rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    rle = rle.cpu().detach().numpy()\n",
    "    rle_str = rle_to_string(rle)\n",
    "    #rle_str = np.array_str(rle)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dba4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_rle(mask_image):\n",
    "    pixels = mask_image.flatten()\n",
    "    # We avoid issues with '1' at the start or end (at the corners of \n",
    "    # the original image) by setting those pixels to '0' explicitly.\n",
    "    # We do not expect these to be non-zero for an accurate mask, \n",
    "    # so this should not harm the score.\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle_str = rle_to_string(runs)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf01f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd732aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(mask_addr):\n",
    "    mask = Image.open(mask_addr).convert('LA') # преобразование в серый\n",
    "    mask = np.asarray(mask).astype('float')[:,:,0]\n",
    "    mask = mask/255.0\n",
    "    mask_rle = numpy_to_rle(mask)\n",
    "    return mask_rle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dd14a",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b539ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceMetric(nn.Module):\n",
    "    def __init__(self, treashold=0.5):\n",
    "        super(DiceMetric, self).__init__()\n",
    "        self.treashold = treashold\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        with torch.no_grad():\n",
    "            smooth = 1\n",
    "            num = targets.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            outputs = torch.where(probs > self.treashold, 1, 0)\n",
    "            m1 = outputs.view(num, -1)\n",
    "            m2 = targets.view(num, -1)\n",
    "            intersection = (m1 * m2)\n",
    "\n",
    "            score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "            score = score.sum() / num\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e9aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробовать softdice loss + bce (как в dlcource.ai)\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0768ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTrain(Dataset):\n",
    "    def __init__(self, data_info):\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        \n",
    "        # Разделяем датафрейм на rgb картинки \n",
    "        self.image_arr = self.data_info.iloc[:,0]\n",
    "        # и на сегментированные картинки\n",
    "        self.label_arr = self.data_info.iloc[:,2]\n",
    "        \n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Читаем картинку и сразу же представляем ее в виде numpy-массива \n",
    "        img = np.asarray(Image.open(self.image_arr[index])).astype('float')\n",
    "        # Нормализуем изображение в значениях [0,1]\n",
    "        img = torch.as_tensor(img)/255    \n",
    "        # 1) unsqueeze - меняет размерность img c (H, W, 3) -> (1, H, W, 3),\n",
    "        # т.е. оборачивает картинку в батч размером в одну картинку\n",
    "        # 2) permute - меняет местами измерения , т.е. (1, H, W, 3) -> (1, 3, H, W)\n",
    "        img = img.unsqueeze(0).permute(0,3,1,2)\n",
    "        \n",
    "        # Мы используем функцию интерполяции для того,\n",
    "        # чтобы поменять рамерность картинки с HхW на 256х256\n",
    "        # clamp не позволяет выйти за границы\n",
    "        img = F.interpolate(input=img, size=(512, 512), align_corners=False, mode='bicubic').clamp(min=0, max=1)\n",
    "        img = img.squeeze(0)\n",
    "        # Читаем сегментированную картинку и сразу же представляем ее в виде numpy-массива \n",
    "        mask = Image.open(self.label_arr[index]).convert('LA') # преобразование в серый\n",
    "        mask = np.asarray(mask).astype('float')[:,:,0]\n",
    "        mask = torch.as_tensor(np.where(mask > 100, 1.0, 0)).unsqueeze(0) # введение порога и нормализация\n",
    "        mask = mask.unsqueeze(0)\n",
    "        mask = mask.float()\n",
    "        # делаем ресайз картинки на 256х256\n",
    "        mask = F.interpolate(input=mask, size=512, mode='nearest')\n",
    "        mask = mask.squeeze(0)\n",
    "        \n",
    "        \n",
    "        return (img.float(), mask.float())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15588a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTest(Dataset):\n",
    "    def __init__(self, data_info):\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        \n",
    "        # Получаем адреса RGB изображений \n",
    "        self.image_names = self.data_info.iloc[:,0]\n",
    "        \n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Читаем картинку и сразу же представляем ее в виде numpy-массива \n",
    "        img = np.asarray(Image.open(source + self.image_names[index])).astype('float')\n",
    "        # Нормализуем изображение в значениях [0,1]\n",
    "        img = torch.as_tensor(img)/255    \n",
    "        # 1) unsqueeze - меняет размерность img c (H, W, 3) -> (1, H, W, 3),\n",
    "        # т.е. оборачивает картинку в батч размером в одну картинку\n",
    "        # 2) permute - меняет местами измерения , т.е. (1, H, W, 3) -> (1, 3, H, W)\n",
    "        img = img.unsqueeze(0).permute(0,3,1,2)\n",
    "        \n",
    "        # Мы используем функцию интерполяции для того,\n",
    "        # чтобы поменять рамерность картинки с HхW на 256х256\n",
    "        # clamp не позволяет выйти за границы\n",
    "        img = F.interpolate(input=img, size=(512, 512), align_corners=False, mode='bicubic').clamp(min=0, max=1)\n",
    "        img = img.squeeze(0)\n",
    "        \n",
    "        image_name = self.image_names[index]\n",
    "    \n",
    "        return (index, img.float(), image_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c97ac76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрать .cuda() из класса добавить в даталоадеры и сравнить скорость обучения\n",
    "# Попробовать разные типы интерполяции\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_rle(tensor):\n",
    "        # We avoid issues with '1' at the start or end (at the corners of \n",
    "        # the original image) by setting those pixels to '0' explicitly.\n",
    "        # We do not expect these to be non-zero for an accurate mask, \n",
    "        # so this should not harm the score.\n",
    "        with torch.no_grad():\n",
    "            tensor = tensor.view(1, -1)\n",
    "            tensor = tensor.squeeze(0)\n",
    "            tensor[0] = 0\n",
    "            tensor[-1] = 0\n",
    "            rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "            rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "            rle = rle.cpu().detach().numpy()\n",
    "            rle_str = rle_to_string(rle)\n",
    "            #rle_str = np.array_str(rle)\n",
    "            return rle_str\n",
    "    \n",
    "    @staticmethod\n",
    "    def rle_to_string(runs):\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, criterion, metric, optimizer, train_data_loader, valid_data_loader=None, epochs=1):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        # запускаем главный тренировочный цикл\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss =0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, (data, labels) in enumerate(train_data_loader):\n",
    "                data, labels = Variable(data), Variable(labels)        \n",
    "                data = data.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % 300 == 299:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {running_loss/300}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spend time for 300 images: {time2-time1} sec')\n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)        \n",
    "            \n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                self.model.eval()\n",
    "                valid_metrics = []\n",
    "                valid_losses = []\n",
    "                for batch_idx, (data, labels) in enumerate(valid_data_loader):\n",
    "                    data, labels = Variable(data), Variable(labels)        \n",
    "                    data = data.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    outputs = self.model(data)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    valid_losses.append(loss.item())\n",
    "                    outputs = F.interpolate(input=outputs, size=(1280, 1918), mode='nearest')\n",
    "                    # Нужно проверить не изменилась ли маска после интерполяции\n",
    "                    # Лучше избавиться от лишней интерполяции\n",
    "                    labels = F.interpolate(input=labels, size=(1280, 1918), mode='nearest')\n",
    "\n",
    "                    metric_value = metric(outputs, labels).item()\n",
    "                    #metric_value = metric_value.cpu()\n",
    "                    valid_metrics.append(metric_value)\n",
    "                    \n",
    "                valid_loss    = np.mean(valid_losses)\n",
    "                valid_metric  = np.mean(valid_metrics)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}, train loss: {train_loss}, valid_loss: {valid_loss}, valid_metric: {valid_metric}')\n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        return epoch_train_losses, epoch_valid_losses, epoch_valid_metrics\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, test_data_loader, predict_directory, mask_treashold=0.5, generate_rle_dataframe=True):\n",
    "        self.model.eval()\n",
    "        img_names = []\n",
    "        img_rles = []\n",
    "        \n",
    "        for batch_idx, (index, img, img_name)  in enumerate(test_data_loader):\n",
    "\n",
    "            img = Variable(img)        \n",
    "            img = img.cuda()\n",
    "            pred_mask_logit = self.model(img)\n",
    "            pred_mask_logit = F.interpolate(input=pred_mask_logit, size=(1280, 1918), mode='nearest')\n",
    "            pred_mask_logit_prob = torch.sigmoid(pred_mask_logit)\n",
    "            pred_mask = torch.where(pred_mask_logit_prob > mask_treashold, 1, 0)\n",
    "            pred_mask = pred_mask.squeeze(0)\n",
    "            pred_mask_cpu = pred_mask.cpu()\n",
    "            pred_mask_cpu = pred_mask_cpu.numpy()\n",
    "            pred_mask_cpu = pred_mask_cpu * 255.0\n",
    "            PIL_image = Image.fromarray(pred_mask_cpu[0].astype('uint8'), 'L')\n",
    "            PIL_image.save((predict_directory+img_name[0]).split('.')[0]+'.gif')\n",
    "            if generate_rle_dataframe == True:\n",
    "                img_names.append(img_name[0])\n",
    "                img_rles.append(tensor_to_rle(pred_mask))\n",
    "                \n",
    "        if generate_rle_dataframe == True:\n",
    "            rle_dataframe = pd.DataFrame(list(zip(img_names, img_rles)), columns =['img_name', 'img_rle'])\n",
    "            return rle_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e07ce",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2458acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем библиотеку time для расчета, сколько времени у нас уходит на одну эпоху\n",
    "import time\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21937687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/dima/carvana_dataset'\n",
    "imgs_path  = dataset_path + '/train/train'\n",
    "masks_path = dataset_path + '/train_masks/train_masks'\n",
    "    \n",
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_data = CustomDatasetForTrain(train_df)\n",
    "valid_data = CustomDatasetForTrain(valid_df)\n",
    "\n",
    "train_data_loader = DataLoader(train_data,batch_size=1,shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = smp.Unet('mobilenet_v2', classes=1, encoder_weights='imagenet').to(device)\n",
    "my_model = NeuralNetwork(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fb86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "criterion = SoftDiceLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "metric = DiceMetric(treashold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca111f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.12431617200374603\n",
      "Spend time for 300 images: 36.063334465026855 sec\n",
      "Train Epoch: 1, Loss: 0.018702537814776102\n",
      "Spend time for 300 images: 34.58461332321167 sec\n",
      "Train Epoch: 1, Loss: 0.023418235778808593\n",
      "Spend time for 300 images: 34.36200189590454 sec\n",
      "Train Epoch: 1, Loss: 0.014062984784444173\n",
      "Spend time for 300 images: 34.34321665763855 sec\n",
      "Train Epoch: 1, Loss: 0.010355209310849508\n",
      "Spend time for 300 images: 34.243250131607056 sec\n",
      "Train Epoch: 1, Loss: 0.00928508977095286\n",
      "Spend time for 300 images: 35.17607402801514 sec\n",
      "Train Epoch: 1, Loss: 0.013880410989125569\n",
      "Spend time for 300 images: 34.73040556907654 sec\n",
      "Train Epoch: 1, Loss: 0.008560932079950968\n",
      "Spend time for 300 images: 33.805450201034546 sec\n",
      "Train Epoch: 1, Loss: 0.007805132269859314\n",
      "Spend time for 300 images: 34.26298260688782 sec\n",
      "Train Epoch: 1, Loss: 0.00740453581015269\n",
      "Spend time for 300 images: 34.14823007583618 sec\n",
      "Train Epoch: 1, Loss: 0.007062990069389343\n",
      "Spend time for 300 images: 33.94478750228882 sec\n",
      "Train Epoch: 1, Loss: 0.00667354683081309\n",
      "Spend time for 300 images: 33.894328355789185 sec\n",
      "Epoch 1, train loss: 0.0201815306639471, valid_loss: 0.006782039115205407, valid_metric: 0.9932809326332063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0201815306639471], [0.006782039115205407], [0.9932809326332063])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit(criterion,\n",
    "             metric, \n",
    "             optimizer,\n",
    "             train_data_loader, \n",
    "             valid_data_loader, \n",
    "             epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988cd6",
   "metadata": {},
   "source": [
    "## Предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a31d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "test_dataset = '/home/dima/carvana_dataset/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6cc16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}\n",
    "test_dataframe['img_addr'] = list(glob.glob(test_dataset + \"/*\"))\n",
    "test_dataframe = pd.DataFrame(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74ff76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_treashold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f64ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTest(Dataset):\n",
    "    def __init__(self, data_info):\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        \n",
    "        # Получаем адреса RGB изображений \n",
    "        self.image_addresses = self.data_info.iloc[:,0]\n",
    "        \n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Читаем картинку и сразу же представляем ее в виде numpy-массива \n",
    "        img = np.asarray(Image.open(self.image_addresses[index])).astype('float')\n",
    "        # Нормализуем изображение в значениях [0,1]\n",
    "        img = torch.as_tensor(img)/255    \n",
    "        # 1) unsqueeze - меняет размерность img c (H, W, 3) -> (1, H, W, 3),\n",
    "        # т.е. оборачивает картинку в батч размером в одну картинку\n",
    "        # 2) permute - меняет местами измерения , т.е. (1, H, W, 3) -> (1, 3, H, W)\n",
    "        img = img.unsqueeze(0).permute(0,3,1,2)\n",
    "        \n",
    "        # Мы используем функцию интерполяции для того,\n",
    "        # чтобы поменять рамерность картинки с HхW на 256х256\n",
    "        # clamp не позволяет выйти за границы\n",
    "        img = F.interpolate(input=img, size=(512, 512), align_corners=False, mode='bicubic').clamp(min=0, max=1)\n",
    "        img = img.squeeze(0)\n",
    "        \n",
    "        image_address = self.image_addresses[index]\n",
    "        image_name = image_address.split('/')[-1]\n",
    "    \n",
    "        return (index, img.float(), image_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b8aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDatasetForTest(test_dataframe)\n",
    "test_data_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "#loader = iter(test_data_loader)\n",
    "#index, img, img_name = loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51507db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ad9e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_dataframe.to_csv('rle_dataframe.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc155fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>img_rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13857e9947b2_11.jpg</td>\n",
       "      <td>988558 71 990476 71 992394 71 994297 139 99445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3ffa310d71ce_12.jpg</td>\n",
       "      <td>863881 116 865799 116 867717 116 869593 214 87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>da5e65183070_02.jpg</td>\n",
       "      <td>729759 56 731677 56 733595 56 735468 191 73738...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bc202073bf8c_14.jpg</td>\n",
       "      <td>518450 179 520368 179 522286 179 524151 11 524...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10e03166b5dc_13.jpg</td>\n",
       "      <td>757960 22 759878 22 761796 22 763699 41 765617...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              img_name                                            img_rle\n",
       "0  13857e9947b2_11.jpg  988558 71 990476 71 992394 71 994297 139 99445...\n",
       "1  3ffa310d71ce_12.jpg  863881 116 865799 116 867717 116 869593 214 87...\n",
       "2  da5e65183070_02.jpg  729759 56 731677 56 733595 56 735468 191 73738...\n",
       "3  bc202073bf8c_14.jpg  518450 179 520368 179 522286 179 524151 11 524...\n",
       "4  10e03166b5dc_13.jpg  757960 22 759878 22 761796 22 763699 41 765617..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adf59ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('/home/dima/carvana_dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89d44bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>rle_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004d4463b50_01.jpg</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0004d4463b50_02.jpg</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004d4463b50_03.jpg</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004d4463b50_04.jpg</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004d4463b50_05.jpg</td>\n",
       "      <td>1 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   img rle_mask\n",
       "0  0004d4463b50_01.jpg      1 1\n",
       "1  0004d4463b50_02.jpg      1 1\n",
       "2  0004d4463b50_03.jpg      1 1\n",
       "3  0004d4463b50_04.jpg      1 1\n",
       "4  0004d4463b50_05.jpg      1 1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9aa0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = sample_submission.merge(rle_dataframe, how='left', left_on='img', right_on='img_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c8b2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.drop(columns=['rle_mask', 'img_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75f4be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.rename(columns={'img_rle': 'rle_mask'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50aea5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>rle_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004d4463b50_01.jpg</td>\n",
       "      <td>610847 11 612765 11 614601 164 616519 164 6184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0004d4463b50_02.jpg</td>\n",
       "      <td>610851 7 612769 7 614597 172 616515 172 618433...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004d4463b50_03.jpg</td>\n",
       "      <td>614657 116 614829 8 616575 116 616747 8 618493...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004d4463b50_04.jpg</td>\n",
       "      <td>610847 15 612765 15 614668 101 614904 11 61658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004d4463b50_05.jpg</td>\n",
       "      <td>614679 94 614799 19 616597 94 616717 19 618515...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   img                                           rle_mask\n",
       "0  0004d4463b50_01.jpg  610847 11 612765 11 614601 164 616519 164 6184...\n",
       "1  0004d4463b50_02.jpg  610851 7 612769 7 614597 172 616515 172 618433...\n",
       "2  0004d4463b50_03.jpg  614657 116 614829 8 616575 116 616747 8 618493...\n",
       "3  0004d4463b50_04.jpg  610847 15 612765 15 614668 101 614904 11 61658...\n",
       "4  0004d4463b50_05.jpg  614679 94 614799 19 616597 94 616717 19 618515..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d6ed9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission_01_10.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
