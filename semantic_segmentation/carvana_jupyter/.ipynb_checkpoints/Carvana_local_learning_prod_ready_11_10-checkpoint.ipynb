{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b5928f",
   "metadata": {},
   "source": [
    "# Improvement Carvana_local_learning_prod_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4aaf",
   "metadata": {},
   "source": [
    "## Подключение библиотек и загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Улучшения: \n",
    "# Убрал .cuda() из класса модели и добавил в даталоадеры перенос на gpu\n",
    "# Добавил трэйсинг моделии и ее сохранение, загрузку модели\n",
    "# Заменил лишний ресайз масок на передачу оригинолов масок\n",
    "# Сделал предсказания с разным размером батча\n",
    "# Нейтрализовал проявления хардкода\n",
    "# Написал комментарии и пояснения\n",
    "# Сделал submission при обучении на всем трерировочном датасете\n",
    "# переписал pil на cv2 где это можно - скорость увеличилась\n",
    "# Реализовал ресайз через albumintation - скорость увеличилась\n",
    "# Сделал нормализацию через albumintation\n",
    "# Проверил, что при нормализации маска не изменяется\n",
    "# Изменил формат вывода при обучении и предсказании\n",
    "# сделал predict на torchscript - скорость не выросла\n",
    "# заменил при валидации и в predict интерполяцию масок с nearest на bilinear\n",
    "# с onnx получилось медленне, видимо нужно отдельно устанавливать cuda и cudnn\n",
    "# Если предсказания модели приводить к исходному разрешению (1918, 1280) через bilinear, метрика на батче\n",
    "# возрастает примерно на 0.001, что очень весомо (0.9962 против 09952) по сравнению с nearest. На изображениях \n",
    "# результат тоже заметен. Использование bicubic не дает стабильного прироста\n",
    "# наиболее предпочтительным методом интерполяции при аугментации является cv.INTER_AREA - из интернета\n",
    "# Попробовал softdice loss + bce (как в dlcource.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eadeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переделать множественное наследование в bcediceloss\n",
    "# реализовать модель из https://github.com/lyakaap/Kaggle-Carvana-3rd-Place-Solution/blob/master/model_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15971ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f662c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.onnx\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6386d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Выполнять, если датасет не загружен\n",
    "#!pip install -q kaggle\n",
    "#!mkdir ~/.kaggle\n",
    "#!cp ~/kaggle.json ~/.kaggle/\n",
    "#!chmod 600 ~/.kaggle/kaggle.json\n",
    "#!kaggle competitions download -c carvana-image-masking-challenge\n",
    "#!unzip ~/carvana-image-masking-challenge.zip ~/carvana_dataset/\n",
    "\n",
    "#!unzip ~/carvana_dataset/train.zip -d ~/carvana_dataset/train\n",
    "#!unzip ~/carvana_dataset/test.zip -d ~/carvana_dataset/test\n",
    "#!unzip ~/carvana_dataset/train_masks.zip -d ~/carvana_dataset/train_masks\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_hq.zip -d ~/carvana_dataset/train_hq\n",
    "#!unzip ~/carvana_dataset/test_hq.zip -d ~/carvana_dataset/test_hq\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_masks.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/sample_submission.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/metadata.csv.zip  ~/carvana_dataset/\n",
    "\n",
    "#!rm ~/carvana-image-masking-challenge.zip\n",
    "#!rm ~/carvana_dataset/test.zip\n",
    "#!rm ~/carvana_dataset/train_masks.zip\n",
    "#!rm ~/carvana_dataset/train.zip\n",
    "#!rm ~/carvana_dataset/test_hq.zip\n",
    "#!rm ~/carvana_dataset/train_hq.zip\n",
    "#!rm ~/carvana_dataset/train_masks.csv.zip\n",
    "#!rm ~/carvana_dataset/sample_submission.csv.zip\n",
    "#!rm ~/carvana_dataset/metadata.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4005c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3608d7",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec78c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(imgs_path: str = None, masks_path: str = None) -> pd.DataFrame:\n",
    "    '''Функция получает на вход пути к директориям с изображениями и масками\n",
    "    и генерирует датафрейм, содержащий имя изображений, их адреса и адреса\n",
    "    соответствующих им масок\n",
    "  \n",
    "    Входные параметры:\n",
    "    imgs_path: str - путь к директории с изображениями,\n",
    "    masks_path: str - путь к директории с масками\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data - dataframe, содержащий адреса изображений и соответствующих им масок'''\n",
    "\n",
    "    assert (imgs_path != None) & (masks_path != None)\n",
    "    # imgs_path or masks_path is equal None\n",
    "\n",
    "    data_img = {}\n",
    "    data_mask = {}\n",
    "    data_img['imgs_path'] = []\n",
    "    data_mask['masks_path'] = []\n",
    "    data_img['imgs_path'] = list(glob.glob(imgs_path + \"/*\"))\n",
    "    data_mask['masks_path'] = list(glob.glob(masks_path + \"/*\"))\n",
    "\n",
    "    data_img = pd.DataFrame(data_img)\n",
    "    data_mask = pd.DataFrame(data_mask)\n",
    "\n",
    "    def file_name(x):\n",
    "        return x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    data_img[\"file_name\"] = data_img[\"imgs_path\"].apply(lambda x: file_name(x))\n",
    "    data_mask[\"file_name\"] = data_mask[\"masks_path\"].apply(lambda x: file_name(x)[:-5])\n",
    "\n",
    "    data = pd.merge(data_img, data_mask, on = \"file_name\", how = \"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5613d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(source_df: pd.DataFrame, separate_feature: str = None, test_size: float = 0.25) -> pd.DataFrame:\n",
    "    '''Функция разделяет source_df на две части с коэффициентом test_size\n",
    "    по уникальным значениям separate_feature так, чтобы в новых датафреймах\n",
    "    не было строк с одинаковыми значенияти из separate_feature\n",
    "\n",
    "    Входные параметры:\n",
    "    source_df: pd.DataFrame - датафрейм для разделения на train и test\n",
    "    separate_feature: str - поле, по которому датафрейм будет разделен\n",
    "    test_size: float - коэффициент разделения дтафрейма\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data_train - датафрейм для тренировки\n",
    "    pd.DataFrame: data_valid - датафрейм для валидации'''\n",
    "  \n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_cars, valid_cars = train_test_split(source_df[separate_feature].unique(), test_size=test_size, random_state=42)\n",
    "        data_valid = source_df[np.isin(source_df[separate_feature].values, valid_cars)]\n",
    "        data_train = source_df[np.isin(source_df[separate_feature].values, train_cars)]\n",
    "        assert source_df.shape[0] == (data_valid.shape[0] + data_train.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "    else:\n",
    "        data_train, data_valid = train_test_split(source_df, test_size=test_size)\n",
    "\n",
    "    return data_train, data_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712baa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(logits: torch.Tensor, targets: torch.Tensor, treashold: float) -> float:\n",
    "    '''Функция для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    Входные параметры:\n",
    "    logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "    targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "    treashold: float - порог для определения класса точки в предсказанной точке\n",
    "    Возвращаемые значения:\n",
    "    score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "    \n",
    "    smooth = 1\n",
    "    num = targets.size(0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    outputs = torch.where(probs > treashold, 1, 0)\n",
    "    m1 = outputs.view(num, -1)\n",
    "    m2 = targets.view(num, -1)\n",
    "    intersection = (m1 * m2)\n",
    "\n",
    "    score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "    score = score.sum() / num\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd108c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "    '''Функция принимает одну маску в тензорном формате, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    tensor: torch.Tensor - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строком виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    tensor = tensor.view(1, -1)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor[0] = 0\n",
    "    tensor[-1] = 0\n",
    "    rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    rle = rle.cpu().detach().numpy()\n",
    "    rle_str = rle_to_string(rle)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dba4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_rle(mask_image: np.ndarray) -> str:\n",
    "    '''Функция принимает одну маску в формате массива numpy, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    mask_image: numpy.ndarray - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    pixels = mask_image.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle_str = rle_to_string(runs)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf01f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_string(runs: torch.Tensor) -> str:\n",
    "    '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "    в строковое представление этой последовательности\n",
    "    Входные параметры:\n",
    "    runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - строковое представление последовательности чисел'''\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd732aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(mask_addr: str) -> str:\n",
    "    '''Функция преобразует маску, имеющую адрес mask_addr и сохраненную в\n",
    "    формате .gif, элементы которой имеют значения 0 и 1 в rle представление\n",
    "    в строковом виде\n",
    "    Входные параметры:\n",
    "    mask_addr: str - адрес маски\n",
    "    Возвращаемые значения:\n",
    "    mask_rle: str - rle представление маски в строком виде\n",
    "    '''\n",
    "    \n",
    "    mask = Image.open(mask_addr).convert('LA') # преобразование в серый\n",
    "    mask = np.asarray(mask).astype('float')[:,:,0]\n",
    "    mask = mask/255.0\n",
    "    mask_rle = numpy_to_rle(mask)\n",
    "    return mask_rle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dd14a",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b539ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceMetric(nn.Module):\n",
    "    '''Класс для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    с заданным порогом для определния класса каждой точки изображения'''\n",
    "    \n",
    "    def __init__(self, treashold: float=0.5):\n",
    "        '''treashold: float - порог для определения класса точки в предсказанной точке'''\n",
    "        super(DiceMetric, self).__init__()\n",
    "        self.treashold = treashold\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "        with torch.no_grad():\n",
    "            smooth = 1\n",
    "            num = targets.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            outputs = torch.where(probs > self.treashold, 1., 0.)\n",
    "            m1 = outputs.view(num, -1)\n",
    "            m2 = targets.view(num, -1)\n",
    "            intersection = (m1 * m2)\n",
    "\n",
    "            score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "            score = score.sum() / num\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e9aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    '''Класс для вычисления DICE loss для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE loss для набора предсказанных масок'''\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e380bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCESoftDiceLoss(nn.Module):\n",
    "    '''Класс для вычисления DICE loss для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self):\n",
    "        super(BCESoftDiceLoss, self).__init__()\n",
    "        #super(torch.nn.BCEWithLogitsLoss, self).__init__()\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss()\n",
    "        self.soft_dice = SoftDiceLoss()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE loss для набора предсказанных масок'''\n",
    "        bce_dice = self.bce(logits, targets) + self.soft_dice(logits, targets)\n",
    "        return bce_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4045a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTrain(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str, transform: object, skip_mask: bool=False):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения и маски\n",
    "        skip_mask: bool - флаг, нужно ли генерировать исходную маску (без изменения размерности)\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTrain'''\n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Разделяем датафрейм на rgb картинки \n",
    "        self.image_arr = self.data_info.iloc[:,0]\n",
    "        # и на сегментированные картинки\n",
    "        self.mask_arr = self.data_info.iloc[:,2]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Нужно ли пробрасывать маску изображения на выход без изменений\n",
    "        self.skip_mask = skip_mask\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        mask: torch.Tensor - тензорное представление изображения с размерностью out_shape \n",
    "        (возвращается если значение skip_mask равно True)'''\n",
    "        image = cv2.imread(self.image_arr[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        # gif не открывается через open cv, поэтому используем для чтения PIL Image\n",
    "        mask = Image.open(self.mask_arr[index])\n",
    "        mask = np.asarray(mask)#.astype('float')\n",
    "        \n",
    "        transformed = self.transform(image=image, mask=mask)\n",
    "        tr_image = transformed['image']\n",
    "        tr_mask = transformed['mask']\n",
    "        \n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        tr_mask = tr_mask.to(self.device).float().unsqueeze(0)\n",
    "\n",
    "        \n",
    "        # Если необходима исходная маска, то дополнительно возвращаем ее\n",
    "        if self.skip_mask == True:\n",
    "            mask = (torch.as_tensor(mask)).to(self.device).float().unsqueeze(0)\n",
    "            return (tr_image, tr_mask, mask)\n",
    "        else:\n",
    "            return (tr_image, tr_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15588a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTest(Dataset):\n",
    "    '''Класс для создания тестовых датасетов'''\n",
    "    def __init__(self, data_info, device: str, transform: object):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами и именами изображений\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTest'''\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Получаем адреса RGB изображений \n",
    "        self.image_addresses = self.data_info.iloc[:,0]\n",
    "        # Получаем имена RGB изображений \n",
    "        self.image_names = self.data_info.iloc[:,1]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        image_name: str - имя изображения'''\n",
    "        image = cv2.imread(self.image_addresses[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        transformed = self.transform(image=image)\n",
    "        tr_image = transformed['image']\n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        image_name = self.image_names[index]\n",
    "    \n",
    "        return (index, tr_image, image_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0a9bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''Класс для создания работы с нейронной сетью для семантической сегментации Carvana'''\n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: nn.Module - последовательность слоев или модель, через которую будут проходить данные\n",
    "        Возвращаемые значения: \n",
    "        объект класса NeuralNetwork'''\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        '''Функция прямого прохода через объкт класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения\n",
    "        Возвращаемые значения: \n",
    "        input_data: torch.Tensor - тензорное представление маски изображения'''\n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "        '''Статический метод принимает одну маску в тензорном формате, элементы которой\n",
    "        имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "        Входные параметры:\n",
    "        tensor: torch.Tensor - маска в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "        # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "        # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "        # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "        # не сильно упадет)\n",
    "        with torch.no_grad():\n",
    "            tensor = tensor.view(1, -1)\n",
    "            tensor = tensor.squeeze(0)\n",
    "            tensor[0] = 0\n",
    "            tensor[-1] = 0\n",
    "            rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "            rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "            rle = rle.cpu().detach().numpy()\n",
    "            rle_str = NeuralNetwork.rle_to_string(rle)\n",
    "            return rle_str\n",
    "    \n",
    "    @staticmethod\n",
    "    def rle_to_string(runs: torch.Tensor) -> str:\n",
    "        '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "        в строковое представление этой последовательности\n",
    "        Входные параметры:\n",
    "        runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - строковое представление последовательности чисел'''\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "    \n",
    "    \n",
    "    def fit(self, criterion: object, metric: object, optimizer: object, \n",
    "                  train_data_loader: DataLoader, valid_data_loader: DataLoader=None, epochs: int=1):\n",
    "        '''Метод для обучения объекта класса\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        optimizer: object - оптимизатор\n",
    "        train_data_loader: DataLoader - загрузчик данных для обучения\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        epochs: int - количество эпох обучения\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        result = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss =0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, (data, labels) in enumerate(train_data_loader):\n",
    "                data, labels = Variable(data), Variable(labels)        \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % 300 == 0:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {(running_loss/300):.6f}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spend time for {300*data.shape[0]} images: {(time2-time1):.6f} sec')\n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)        \n",
    "            \n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                self.model.eval()\n",
    "                valid_metrics = []\n",
    "                valid_losses = []\n",
    "                for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "                    data, labels, labels_small = Variable(data), Variable(labels), Variable(labels_small)\n",
    "                    outputs = self.model(data)\n",
    "                    # loss вычисляется для сжатых масок для правильной валидации (обучались на сжатых)\n",
    "                    # чтобы вовремя определить переобучение\n",
    "                    loss = criterion(outputs, labels_small)\n",
    "                    valid_losses.append(loss.item())\n",
    "                    #Преобразуем выход модели к размеру соответствующей маски\n",
    "                    outputs = F.interpolate(input=outputs, size=(labels.shape[2], \n",
    "                                                                 labels.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "                    # метрика считается для исходных размеров потому что именно так итоговое качество\n",
    "                    # определяется алгоритмом kaggle \n",
    "                    metric_value = metric(outputs, labels)\n",
    "                    valid_metrics.append(metric_value.item())\n",
    "                    \n",
    "                valid_loss    = np.mean(valid_losses)\n",
    "                valid_metric  = np.mean(valid_metrics)\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}, valid_loss: {(valid_loss):.6f}, valid_metric: {(valid_metric):.6f}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}')\n",
    "                valid_loss = None\n",
    "                valid_metric = None\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        result['epoch_train_losses'] = epoch_train_losses\n",
    "        result['epoch_valid_losses'] = epoch_valid_losses\n",
    "        result['epoch_valid_metrics'] = epoch_valid_metrics\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def valid(self, criterion: object, metric: object, valid_data_loader: DataLoader):\n",
    "        '''Метод для валидации модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        self.model.eval()\n",
    "        valid_metrics = []\n",
    "        valid_losses = []\n",
    "        result = {}\n",
    "        for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "            data, labels, labels_small = Variable(data), Variable(labels), Variable(labels_small)\n",
    "            outputs = self.model(data)\n",
    "            # loss вычисляется для сжатых масок для правильной валидации (обучались на сжатых)\n",
    "            # чтобы вовремя определить переобучение\n",
    "            loss = criterion(outputs, labels_small)\n",
    "            valid_losses.append(loss.item())\n",
    "            #Преобразуем выход модели к размеру соответствующей маски\n",
    "            outputs = F.interpolate(input=outputs, size=(labels.shape[2], \n",
    "                                                         labels.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # метрика считается для исходных размеров потому что именно так итоговое качество\n",
    "            # определяется алгоритмом kaggle \n",
    "            metric_value = metric(outputs, labels)\n",
    "            valid_metrics.append(metric_value.item())\n",
    "                    \n",
    "        valid_loss    = np.mean(valid_losses)\n",
    "        valid_metric  = np.mean(valid_metrics)\n",
    "        result['valid_loss'] = valid_loss\n",
    "        result['valid_metric'] = valid_metric\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def predict(self, test_data_loader: DataLoader, predict_directory: str=None, output_size: tuple=(1280, 1918), \n",
    "                mask_treashold: float=0.5, generate_rle_dataframe: bool=True) -> pd.DataFrame:\n",
    "        '''Метод для предсказания масок для набора изображения\n",
    "        Входные параметры:\n",
    "        test_data_loader: DataLoader - загрузчик данных для предсказания\n",
    "        predict_directory: str - директория, в которую будут сохраняться сгенерированные маски (если None,\n",
    "        то маски сохраняться не будут)\n",
    "        output_size: tuple - пространственная размерность выходных масок\n",
    "        mask_treashold: float - порог, по которому будет определяться класс каждой точки для масок\n",
    "        generate_rle_dataframe: bool - флаг, нужна ли генерация rle представлений масок\n",
    "        Возвращаемые значения:\n",
    "        rle_dataframe: pd.DataFrame - датафрейм с rle представлениями для масок (если \n",
    "        generate_rle_dataframe==True)\n",
    "        Маски в формате .gif для изображений с соответствующими именами, находятся в директории predict_directory'''\n",
    "        self.model.eval()\n",
    "        img_names = []\n",
    "        img_rles = []\n",
    "        time1 = time.time()\n",
    "        time2 = time.time()\n",
    "        for batch_idx, (index, img, img_name)  in enumerate(test_data_loader):\n",
    "\n",
    "            img = Variable(img)        \n",
    "            pred_mask_logit = self.model(img)\n",
    "            pred_mask_logit = F.interpolate(input=pred_mask_logit, size=output_size, mode='bilinear', align_corners=False)\n",
    "            pred_mask_logit_prob = torch.sigmoid(pred_mask_logit)\n",
    "            pred_mask = torch.where(pred_mask_logit_prob > mask_treashold, 1, 0)\n",
    "            \n",
    "            # Каждое изображение в тензоре преобразуем в картинку и сохраняем\n",
    "            for i in range(pred_mask.shape[0]):\n",
    "                if predict_directory != None:\n",
    "                    mask = (pred_mask[i].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "                    PIL_image = Image.fromarray(mask.astype('uint8'), 'L')\n",
    "                    PIL_image.save((predict_directory+img_name[i]).split('.')[0]+'.gif')\n",
    "                \n",
    "                # Если требуется, получаем значения rle для каждой картинки\n",
    "                if generate_rle_dataframe == True:\n",
    "                    img_names.append(img_name[i])\n",
    "                    img_rles.append(NeuralNetwork.tensor_to_rle(pred_mask[i]))\n",
    "            \n",
    "            if (batch_idx+1) % 300 == 0:\n",
    "                    print('-'*50)\n",
    "                    print(f'Processed images: {(batch_idx+1)*img.shape[0]}')\n",
    "                    time3 = time.time()\n",
    "                    print(f'Total time: {(time3-time1):.2f} sec')\n",
    "                    print(f'Time to process {300*img.shape[0]} images: {(time3-time2):.2f} sec')\n",
    "                    time2 = time.time()\n",
    "                \n",
    "        if generate_rle_dataframe == True:\n",
    "            rle_dataframe = pd.DataFrame(list(zip(img_names, img_rles)), columns =['img_name', 'img_rle'])\n",
    "            return rle_dataframe\n",
    "    \n",
    "    def save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения весов модели\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения состояния модели'''\n",
    "        torch.save(self.model.state_dict(), path_to_save)\n",
    "    \n",
    "    def trace_save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения модели через torchscript\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.rand(1, 3, 512, 512).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "            \n",
    "        traced_model = torch.jit.trace((self.model).eval(), example_forward_input)\n",
    "        torch.jit.save(traced_model, path_to_save)\n",
    "    \n",
    "    def onnx_save(self, path_to_save: str='./carvana_model.onnx'):\n",
    "        '''Метод сохранения модели в формате ONNX\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.randn(1, 3, 1024, 1024, requires_grad=True).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "\n",
    "        torch.onnx.export(self.model,\n",
    "                          example_forward_input,\n",
    "                          path_to_save,\n",
    "                          export_params=True,\n",
    "                          opset_version=10,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'],\n",
    "                          dynamic_axes={'input' : {0 : 'batch_size'},    # Модель будет работать с произвольным\n",
    "                                        'output' : {0 : 'batch_size'}})  # размером батча\n",
    "    \n",
    "    def load(self, path_to_model: str='./model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e07ce",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21937687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/dima/carvana_dataset'\n",
    "imgs_path  = dataset_path + '/train/train'\n",
    "masks_path = dataset_path + '/train_masks/train_masks'\n",
    "\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 30\n",
    "mask_treashold = 0.5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1d58730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function preprocess_input at 0x7fe351ac2790>, input_space='RGB', input_range=[0, 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "preprocess_input = get_preprocessing_fn('timm-mobilenetv3_small_100', pretrained='imagenet')\n",
    "preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "888ec35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(1024, 2048, cv2.INTER_AREA), # для масок автоматически будет применяться своя интерполяция, \n",
    "                                          # поэтому на выходе значения маски останутся 0 и 1\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    #С нормализацией хуже почему то\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(1024, 2048, cv2.INTER_AREA), # INTER_AREA как правило лучше осуществляет переход к меньшему разрешению\n",
    "    #С нормализацией хуже почему то\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d28b75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e54a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение с валидацией\n",
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_data = CustomDatasetForTrain(train_df, device, train_transform, skip_mask=False)\n",
    "valid_data = CustomDatasetForTrain(valid_df, device, valid_transform, skip_mask=True)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение без валидации\n",
    "train_data = CustomDatasetForTrain(data, device, train_transform, skip_mask=False)\n",
    "train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4abf005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эту надо обучить\n",
    "model = smp.DeepLabV3Plus(encoder_name='timm-mobilenetv3_small_100', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84e40c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNetwork(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961486dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "DeepLabV3Plus                                           --                        --\n",
       "├─MobileNetV3Encoder: 1-1                               [2, 3, 1024, 2048]        --\n",
       "│    └─MobileNetV3Features: 2                           --                        --\n",
       "│    │    └─Conv2dSame: 3-1                             [2, 16, 512, 1024]        432\n",
       "│    │    └─BatchNorm2d: 3-2                            [2, 16, 512, 1024]        32\n",
       "│    │    └─Hardswish: 3-3                              [2, 16, 512, 1024]        --\n",
       "├─DeepLabV3PlusDecoder: 1-2                             [2, 256, 256, 512]        --\n",
       "│    └─Sequential: 2-1                                  [2, 256, 64, 128]         --\n",
       "│    │    └─ASPP: 3-4                                   [2, 256, 64, 128]         1,083,584\n",
       "│    │    └─SeparableConv2d: 3-5                        [2, 256, 64, 128]         67,840\n",
       "│    │    └─BatchNorm2d: 3-6                            [2, 256, 64, 128]         512\n",
       "│    │    └─ReLU: 3-7                                   [2, 256, 64, 128]         --\n",
       "│    └─UpsamplingBilinear2d: 2-2                        [2, 256, 256, 512]        --\n",
       "│    └─Sequential: 2-3                                  [2, 48, 256, 512]         --\n",
       "│    │    └─Conv2d: 3-8                                 [2, 48, 256, 512]         768\n",
       "│    │    └─BatchNorm2d: 3-9                            [2, 48, 256, 512]         96\n",
       "│    │    └─ReLU: 3-10                                  [2, 48, 256, 512]         --\n",
       "│    └─Sequential: 2-4                                  [2, 256, 256, 512]        --\n",
       "│    │    └─SeparableConv2d: 3-11                       [2, 256, 256, 512]        80,560\n",
       "│    │    └─BatchNorm2d: 3-12                           [2, 256, 256, 512]        512\n",
       "│    │    └─ReLU: 3-13                                  [2, 256, 256, 512]        --\n",
       "├─SegmentationHead: 1-3                                 [2, 1, 1024, 2048]        --\n",
       "│    └─Conv2d: 2-5                                      [2, 1, 256, 512]          257\n",
       "│    └─UpsamplingBilinear2d: 2-6                        [2, 1, 1024, 2048]        --\n",
       "│    └─Activation: 2-7                                  [2, 1, 1024, 2048]        --\n",
       "│    │    └─Identity: 3-14                              [2, 1, 1024, 2048]        --\n",
       "=========================================================================================================\n",
       "Total params: 2,161,137\n",
       "Trainable params: 2,161,137\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 46.52\n",
       "=========================================================================================================\n",
       "Input size (MB): 50.33\n",
       "Forward/backward pass size (MB): 5146.47\n",
       "Params size (MB): 8.64\n",
       "Estimated Total Size (MB): 5205.44\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(2, 3, 1024, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fb86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = SoftDiceLoss()\n",
    "criterion = BCESoftDiceLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "metric = DiceMetric(treashold=mask_treashold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca111f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.079611\n",
      "Spend time for 600 images: 122.799630 sec\n",
      "Train Epoch: 1, Loss: 0.024662\n",
      "Spend time for 600 images: 122.006894 sec\n",
      "Train Epoch: 1, Loss: 0.018667\n",
      "Spend time for 600 images: 122.644700 sec\n",
      "Train Epoch: 1, Loss: 0.016423\n",
      "Spend time for 600 images: 122.228669 sec\n",
      "Train Epoch: 1, Loss: 0.015116\n",
      "Spend time for 600 images: 121.885241 sec\n",
      "Train Epoch: 1, Loss: 0.013964\n",
      "Spend time for 600 images: 122.120481 sec\n",
      "Epoch 1, train loss: 0.027239, valid_loss: 0.012670, valid_metric: 0.994601\n",
      "Train Epoch: 2, Loss: 0.013919\n",
      "Spend time for 600 images: 122.825898 sec\n",
      "Train Epoch: 2, Loss: 0.013131\n",
      "Spend time for 600 images: 121.441173 sec\n",
      "Train Epoch: 2, Loss: 0.012352\n",
      "Spend time for 600 images: 121.710842 sec\n",
      "Train Epoch: 2, Loss: 0.012172\n",
      "Spend time for 600 images: 121.613091 sec\n",
      "Train Epoch: 2, Loss: 0.028806\n",
      "Spend time for 600 images: 121.393983 sec\n",
      "Train Epoch: 2, Loss: 0.014638\n",
      "Spend time for 600 images: 121.040921 sec\n",
      "Epoch 2, train loss: 0.015711, valid_loss: 0.012739, valid_metric: 0.994794\n",
      "Train Epoch: 3, Loss: 0.012732\n",
      "Spend time for 600 images: 121.358884 sec\n",
      "Train Epoch: 3, Loss: 0.012673\n",
      "Spend time for 600 images: 121.676938 sec\n",
      "Train Epoch: 3, Loss: 0.011882\n",
      "Spend time for 600 images: 122.233485 sec\n",
      "Train Epoch: 3, Loss: 0.011957\n",
      "Spend time for 600 images: 122.794549 sec\n",
      "Train Epoch: 3, Loss: 0.011503\n",
      "Spend time for 600 images: 125.088027 sec\n",
      "Train Epoch: 3, Loss: 0.011207\n",
      "Spend time for 600 images: 123.178797 sec\n",
      "Epoch 3, train loss: 0.011948, valid_loss: 0.011551, valid_metric: 0.995013\n",
      "Train Epoch: 4, Loss: 0.011239\n",
      "Spend time for 600 images: 122.090238 sec\n",
      "Train Epoch: 4, Loss: 0.011224\n",
      "Spend time for 600 images: 121.846678 sec\n",
      "Train Epoch: 4, Loss: 0.011351\n",
      "Spend time for 600 images: 122.079307 sec\n",
      "Train Epoch: 4, Loss: 0.010329\n",
      "Spend time for 600 images: 122.165180 sec\n",
      "Train Epoch: 4, Loss: 0.010657\n",
      "Spend time for 600 images: 122.991250 sec\n",
      "Train Epoch: 4, Loss: 0.010768\n",
      "Spend time for 600 images: 123.236888 sec\n",
      "Epoch 4, train loss: 0.010882, valid_loss: 0.010600, valid_metric: 0.995509\n",
      "Train Epoch: 5, Loss: 0.010006\n",
      "Spend time for 600 images: 121.990455 sec\n",
      "Train Epoch: 5, Loss: 0.010318\n",
      "Spend time for 600 images: 122.228672 sec\n",
      "Train Epoch: 5, Loss: 0.010543\n",
      "Spend time for 600 images: 121.963784 sec\n",
      "Train Epoch: 5, Loss: 0.014612\n",
      "Spend time for 600 images: 121.639733 sec\n",
      "Train Epoch: 5, Loss: 0.010867\n",
      "Spend time for 600 images: 122.105953 sec\n",
      "Train Epoch: 5, Loss: 0.010358\n",
      "Spend time for 600 images: 122.005045 sec\n",
      "Epoch 5, train loss: 0.011073, valid_loss: 0.010518, valid_metric: 0.995372\n",
      "Train Epoch: 6, Loss: 0.009802\n",
      "Spend time for 600 images: 121.924155 sec\n",
      "Train Epoch: 6, Loss: 0.010162\n",
      "Spend time for 600 images: 121.995262 sec\n",
      "Train Epoch: 6, Loss: 0.010079\n",
      "Spend time for 600 images: 122.155198 sec\n",
      "Train Epoch: 6, Loss: 0.009703\n",
      "Spend time for 600 images: 122.056171 sec\n",
      "Train Epoch: 6, Loss: 0.009709\n",
      "Spend time for 600 images: 121.844100 sec\n",
      "Train Epoch: 6, Loss: 0.009677\n",
      "Spend time for 600 images: 121.783663 sec\n",
      "Epoch 6, train loss: 0.009837, valid_loss: 0.009841, valid_metric: 0.995535\n",
      "Train Epoch: 7, Loss: 0.009603\n",
      "Spend time for 600 images: 121.365548 sec\n",
      "Train Epoch: 7, Loss: 0.010820\n",
      "Spend time for 600 images: 121.189971 sec\n",
      "Train Epoch: 7, Loss: 0.009545\n",
      "Spend time for 600 images: 121.177706 sec\n",
      "Train Epoch: 7, Loss: 0.009716\n",
      "Spend time for 600 images: 121.246345 sec\n",
      "Train Epoch: 7, Loss: 0.009321\n",
      "Spend time for 600 images: 121.318973 sec\n",
      "Train Epoch: 7, Loss: 0.009514\n",
      "Spend time for 600 images: 121.409866 sec\n",
      "Epoch 7, train loss: 0.009730, valid_loss: 0.009496, valid_metric: 0.995731\n",
      "Train Epoch: 8, Loss: 0.010156\n",
      "Spend time for 600 images: 121.425309 sec\n",
      "Train Epoch: 8, Loss: 0.009373\n",
      "Spend time for 600 images: 121.464062 sec\n",
      "Train Epoch: 8, Loss: 0.009210\n",
      "Spend time for 600 images: 121.443943 sec\n",
      "Train Epoch: 8, Loss: 0.009069\n",
      "Spend time for 600 images: 121.728320 sec\n",
      "Train Epoch: 8, Loss: 0.009179\n",
      "Spend time for 600 images: 121.190589 sec\n",
      "Train Epoch: 8, Loss: 0.009341\n",
      "Spend time for 600 images: 121.485386 sec\n",
      "Epoch 8, train loss: 0.009406, valid_loss: 0.018771, valid_metric: 0.992693\n",
      "Train Epoch: 9, Loss: 0.008906\n",
      "Spend time for 600 images: 121.930522 sec\n",
      "Train Epoch: 9, Loss: 0.008699\n",
      "Spend time for 600 images: 124.173861 sec\n",
      "Train Epoch: 9, Loss: 0.009139\n",
      "Spend time for 600 images: 122.648767 sec\n",
      "Train Epoch: 9, Loss: 0.009034\n",
      "Spend time for 600 images: 123.081033 sec\n",
      "Train Epoch: 9, Loss: 0.008785\n",
      "Spend time for 600 images: 122.932511 sec\n",
      "Train Epoch: 9, Loss: 0.009783\n",
      "Spend time for 600 images: 121.549446 sec\n",
      "Epoch 9, train loss: 0.009081, valid_loss: 0.009381, valid_metric: 0.995857\n",
      "Train Epoch: 10, Loss: 0.008853\n",
      "Spend time for 600 images: 122.027124 sec\n",
      "Train Epoch: 10, Loss: 0.010579\n",
      "Spend time for 600 images: 121.620774 sec\n",
      "Train Epoch: 10, Loss: 0.009195\n",
      "Spend time for 600 images: 121.345186 sec\n",
      "Train Epoch: 10, Loss: 0.008805\n",
      "Spend time for 600 images: 121.377775 sec\n",
      "Train Epoch: 10, Loss: 0.008934\n",
      "Spend time for 600 images: 121.731781 sec\n",
      "Train Epoch: 10, Loss: 0.008621\n",
      "Spend time for 600 images: 121.580920 sec\n",
      "Epoch 10, train loss: 0.009129, valid_loss: 0.009119, valid_metric: 0.995985\n",
      "Train Epoch: 11, Loss: 0.008539\n",
      "Spend time for 600 images: 121.429632 sec\n",
      "Train Epoch: 11, Loss: 0.008281\n",
      "Spend time for 600 images: 121.479916 sec\n",
      "Train Epoch: 11, Loss: 0.008462\n",
      "Spend time for 600 images: 121.284980 sec\n",
      "Train Epoch: 11, Loss: 0.008643\n",
      "Spend time for 600 images: 121.630421 sec\n",
      "Train Epoch: 11, Loss: 0.008715\n",
      "Spend time for 600 images: 121.797524 sec\n",
      "Train Epoch: 11, Loss: 0.008574\n",
      "Spend time for 600 images: 121.269103 sec\n",
      "Epoch 11, train loss: 0.008535, valid_loss: 0.009308, valid_metric: 0.995931\n",
      "Train Epoch: 12, Loss: 0.008372\n",
      "Spend time for 600 images: 121.706811 sec\n",
      "Train Epoch: 12, Loss: 0.008225\n",
      "Spend time for 600 images: 121.539109 sec\n",
      "Train Epoch: 12, Loss: 0.008720\n",
      "Spend time for 600 images: 121.651938 sec\n",
      "Train Epoch: 12, Loss: 0.008758\n",
      "Spend time for 600 images: 121.217520 sec\n",
      "Train Epoch: 12, Loss: 0.010056\n",
      "Spend time for 600 images: 121.406128 sec\n",
      "Train Epoch: 12, Loss: 0.008440\n",
      "Spend time for 600 images: 121.540286 sec\n",
      "Epoch 12, train loss: 0.008737, valid_loss: 0.008999, valid_metric: 0.995946\n",
      "Train Epoch: 13, Loss: 0.008261\n",
      "Spend time for 600 images: 121.344023 sec\n",
      "Train Epoch: 13, Loss: 0.008126\n",
      "Spend time for 600 images: 121.226356 sec\n",
      "Train Epoch: 13, Loss: 0.008102\n",
      "Spend time for 600 images: 121.489854 sec\n",
      "Train Epoch: 13, Loss: 0.008212\n",
      "Spend time for 600 images: 121.567401 sec\n",
      "Train Epoch: 13, Loss: 0.008059\n",
      "Spend time for 600 images: 121.484529 sec\n",
      "Train Epoch: 13, Loss: 0.008389\n",
      "Spend time for 600 images: 121.509153 sec\n",
      "Epoch 13, train loss: 0.008208, valid_loss: 0.009152, valid_metric: 0.995965\n",
      "Train Epoch: 14, Loss: 0.009782\n",
      "Spend time for 600 images: 121.706972 sec\n",
      "Train Epoch: 14, Loss: 0.008361\n",
      "Spend time for 600 images: 121.714417 sec\n",
      "Train Epoch: 14, Loss: 0.008300\n",
      "Spend time for 600 images: 121.366556 sec\n",
      "Train Epoch: 14, Loss: 0.008228\n",
      "Spend time for 600 images: 121.572311 sec\n",
      "Train Epoch: 14, Loss: 0.008265\n",
      "Spend time for 600 images: 121.523678 sec\n",
      "Train Epoch: 14, Loss: 0.007993\n",
      "Spend time for 600 images: 121.283635 sec\n",
      "Epoch 14, train loss: 0.008469, valid_loss: 0.008742, valid_metric: 0.996230\n",
      "Train Epoch: 15, Loss: 0.007682\n",
      "Spend time for 600 images: 121.047132 sec\n",
      "Train Epoch: 15, Loss: 0.008149\n",
      "Spend time for 600 images: 121.314206 sec\n",
      "Train Epoch: 15, Loss: 0.008648\n",
      "Spend time for 600 images: 121.352583 sec\n",
      "Train Epoch: 15, Loss: 0.008017\n",
      "Spend time for 600 images: 121.064884 sec\n",
      "Train Epoch: 15, Loss: 0.008021\n",
      "Spend time for 600 images: 121.502851 sec\n",
      "Train Epoch: 15, Loss: 0.008010\n",
      "Spend time for 600 images: 120.922620 sec\n",
      "Epoch 15, train loss: 0.008060, valid_loss: 0.008791, valid_metric: 0.995998\n",
      "Train Epoch: 16, Loss: 0.007970\n",
      "Spend time for 600 images: 120.924768 sec\n",
      "Train Epoch: 16, Loss: 0.007783\n",
      "Spend time for 600 images: 121.433706 sec\n",
      "Train Epoch: 16, Loss: 0.007920\n",
      "Spend time for 600 images: 121.602871 sec\n",
      "Train Epoch: 16, Loss: 0.007917\n",
      "Spend time for 600 images: 121.474688 sec\n",
      "Train Epoch: 16, Loss: 0.007918\n",
      "Spend time for 600 images: 121.578228 sec\n",
      "Train Epoch: 16, Loss: 0.007907\n",
      "Spend time for 600 images: 121.421719 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, train loss: 0.007901, valid_loss: 0.009226, valid_metric: 0.995931\n",
      "Train Epoch: 17, Loss: 0.007690\n",
      "Spend time for 600 images: 121.442571 sec\n",
      "Train Epoch: 17, Loss: 0.007622\n",
      "Spend time for 600 images: 121.451187 sec\n",
      "Train Epoch: 17, Loss: 0.007780\n",
      "Spend time for 600 images: 121.506177 sec\n",
      "Train Epoch: 17, Loss: 0.007782\n",
      "Spend time for 600 images: 121.266150 sec\n",
      "Train Epoch: 17, Loss: 0.007957\n",
      "Spend time for 600 images: 121.511184 sec\n",
      "Train Epoch: 17, Loss: 0.007926\n",
      "Spend time for 600 images: 121.180971 sec\n",
      "Epoch 17, train loss: 0.007808, valid_loss: 0.008688, valid_metric: 0.996009\n",
      "Train Epoch: 18, Loss: 0.007535\n",
      "Spend time for 600 images: 121.066902 sec\n",
      "Train Epoch: 18, Loss: 0.007684\n",
      "Spend time for 600 images: 121.183441 sec\n",
      "Train Epoch: 18, Loss: 0.007809\n",
      "Spend time for 600 images: 121.500575 sec\n",
      "Train Epoch: 18, Loss: 0.007647\n",
      "Spend time for 600 images: 121.332627 sec\n",
      "Train Epoch: 18, Loss: 0.007748\n",
      "Spend time for 600 images: 121.346223 sec\n",
      "Train Epoch: 18, Loss: 0.008087\n",
      "Spend time for 600 images: 121.676479 sec\n",
      "Epoch 18, train loss: 0.007779, valid_loss: 0.009083, valid_metric: 0.996027\n",
      "Train Epoch: 19, Loss: 0.007456\n",
      "Spend time for 600 images: 121.174781 sec\n",
      "Train Epoch: 19, Loss: 0.007370\n",
      "Spend time for 600 images: 121.446196 sec\n",
      "Train Epoch: 19, Loss: 0.007496\n",
      "Spend time for 600 images: 121.705741 sec\n",
      "Train Epoch: 19, Loss: 0.007644\n",
      "Spend time for 600 images: 121.592324 sec\n",
      "Train Epoch: 19, Loss: 0.007520\n",
      "Spend time for 600 images: 121.590010 sec\n",
      "Train Epoch: 19, Loss: 0.007618\n",
      "Spend time for 600 images: 120.999664 sec\n",
      "Epoch 19, train loss: 0.007573, valid_loss: 0.010122, valid_metric: 0.995300\n",
      "Train Epoch: 20, Loss: 0.007679\n",
      "Spend time for 600 images: 121.434297 sec\n",
      "Train Epoch: 20, Loss: 0.007432\n",
      "Spend time for 600 images: 121.567299 sec\n",
      "Train Epoch: 20, Loss: 0.007477\n",
      "Spend time for 600 images: 121.202621 sec\n",
      "Train Epoch: 20, Loss: 0.007501\n",
      "Spend time for 600 images: 121.463923 sec\n",
      "Train Epoch: 20, Loss: 0.007657\n",
      "Spend time for 600 images: 121.207335 sec\n",
      "Train Epoch: 20, Loss: 0.007218\n",
      "Spend time for 600 images: 121.694354 sec\n",
      "Epoch 20, train loss: 0.007506, valid_loss: 0.008600, valid_metric: 0.996284\n",
      "Train Epoch: 21, Loss: 0.007285\n",
      "Spend time for 600 images: 121.203024 sec\n",
      "Train Epoch: 21, Loss: 0.007183\n",
      "Spend time for 600 images: 121.620265 sec\n",
      "Train Epoch: 21, Loss: 0.007318\n",
      "Spend time for 600 images: 121.185999 sec\n",
      "Train Epoch: 21, Loss: 0.007481\n",
      "Spend time for 600 images: 121.661612 sec\n",
      "Train Epoch: 21, Loss: 0.007493\n",
      "Spend time for 600 images: 121.561614 sec\n",
      "Train Epoch: 21, Loss: 0.008430\n",
      "Spend time for 600 images: 121.177120 sec\n",
      "Epoch 21, train loss: 0.007538, valid_loss: 0.008645, valid_metric: 0.996114\n",
      "Train Epoch: 22, Loss: 0.007275\n",
      "Spend time for 600 images: 121.017463 sec\n",
      "Train Epoch: 22, Loss: 0.007108\n",
      "Spend time for 600 images: 121.378359 sec\n",
      "Train Epoch: 22, Loss: 0.007344\n",
      "Spend time for 600 images: 121.409082 sec\n",
      "Train Epoch: 22, Loss: 0.007151\n",
      "Spend time for 600 images: 121.493689 sec\n",
      "Train Epoch: 22, Loss: 0.007364\n",
      "Spend time for 600 images: 121.365192 sec\n",
      "Train Epoch: 22, Loss: 0.007454\n",
      "Spend time for 600 images: 121.423536 sec\n",
      "Epoch 22, train loss: 0.007277, valid_loss: 0.008641, valid_metric: 0.996071\n",
      "Train Epoch: 23, Loss: 0.007192\n",
      "Spend time for 600 images: 121.165483 sec\n",
      "Train Epoch: 23, Loss: 0.007067\n",
      "Spend time for 600 images: 121.402185 sec\n",
      "Train Epoch: 23, Loss: 0.007240\n",
      "Spend time for 600 images: 121.224491 sec\n",
      "Train Epoch: 23, Loss: 0.007259\n",
      "Spend time for 600 images: 121.386170 sec\n",
      "Train Epoch: 23, Loss: 0.007244\n",
      "Spend time for 600 images: 121.262026 sec\n",
      "Train Epoch: 23, Loss: 0.007108\n",
      "Spend time for 600 images: 121.378934 sec\n",
      "Epoch 23, train loss: 0.007178, valid_loss: 0.008587, valid_metric: 0.996159\n",
      "Train Epoch: 24, Loss: 0.007231\n",
      "Spend time for 600 images: 120.807986 sec\n",
      "Train Epoch: 24, Loss: 0.007052\n",
      "Spend time for 600 images: 121.673147 sec\n",
      "Train Epoch: 24, Loss: 0.007369\n",
      "Spend time for 600 images: 121.651191 sec\n",
      "Train Epoch: 24, Loss: 0.007247\n",
      "Spend time for 600 images: 121.145683 sec\n",
      "Train Epoch: 24, Loss: 0.007165\n",
      "Spend time for 600 images: 121.603821 sec\n",
      "Train Epoch: 24, Loss: 0.007088\n",
      "Spend time for 600 images: 121.409539 sec\n",
      "Epoch 24, train loss: 0.007188, valid_loss: 0.008415, valid_metric: 0.996127\n",
      "Train Epoch: 25, Loss: 0.007013\n",
      "Spend time for 600 images: 120.957958 sec\n",
      "Train Epoch: 25, Loss: 0.007168\n",
      "Spend time for 600 images: 121.889898 sec\n",
      "Train Epoch: 25, Loss: 0.006883\n",
      "Spend time for 600 images: 121.139388 sec\n",
      "Train Epoch: 25, Loss: 0.007020\n",
      "Spend time for 600 images: 121.616946 sec\n",
      "Train Epoch: 25, Loss: 0.007058\n",
      "Spend time for 600 images: 121.451625 sec\n",
      "Train Epoch: 25, Loss: 0.007136\n",
      "Spend time for 600 images: 121.446613 sec\n",
      "Epoch 25, train loss: 0.007044, valid_loss: 0.008741, valid_metric: 0.996077\n",
      "Train Epoch: 26, Loss: 0.006919\n",
      "Spend time for 600 images: 121.607593 sec\n",
      "Train Epoch: 26, Loss: 0.006913\n",
      "Spend time for 600 images: 121.686272 sec\n",
      "Train Epoch: 26, Loss: 0.006954\n",
      "Spend time for 600 images: 121.404271 sec\n",
      "Train Epoch: 26, Loss: 0.006926\n",
      "Spend time for 600 images: 121.415909 sec\n",
      "Train Epoch: 26, Loss: 0.007017\n",
      "Spend time for 600 images: 121.248333 sec\n",
      "Train Epoch: 26, Loss: 0.006874\n",
      "Spend time for 600 images: 121.476737 sec\n",
      "Epoch 26, train loss: 0.006944, valid_loss: 0.008476, valid_metric: 0.996158\n",
      "Train Epoch: 27, Loss: 0.006614\n",
      "Spend time for 600 images: 120.856557 sec\n",
      "Train Epoch: 27, Loss: 0.006945\n",
      "Spend time for 600 images: 121.309330 sec\n",
      "Train Epoch: 27, Loss: 0.007020\n",
      "Spend time for 600 images: 121.173047 sec\n",
      "Train Epoch: 27, Loss: 0.006854\n",
      "Spend time for 600 images: 121.524251 sec\n",
      "Train Epoch: 27, Loss: 0.007068\n",
      "Spend time for 600 images: 121.385268 sec\n",
      "Train Epoch: 27, Loss: 0.006907\n",
      "Spend time for 600 images: 121.454243 sec\n",
      "Epoch 27, train loss: 0.006907, valid_loss: 0.008542, valid_metric: 0.996317\n",
      "Train Epoch: 28, Loss: 0.006675\n",
      "Spend time for 600 images: 121.351950 sec\n",
      "Train Epoch: 28, Loss: 0.006712\n",
      "Spend time for 600 images: 121.707615 sec\n",
      "Train Epoch: 28, Loss: 0.006810\n",
      "Spend time for 600 images: 121.227332 sec\n",
      "Train Epoch: 28, Loss: 0.006693\n",
      "Spend time for 600 images: 121.325942 sec\n",
      "Train Epoch: 28, Loss: 0.006916\n",
      "Spend time for 600 images: 121.714900 sec\n",
      "Train Epoch: 28, Loss: 0.006908\n",
      "Spend time for 600 images: 121.157939 sec\n",
      "Epoch 28, train loss: 0.006795, valid_loss: 0.008621, valid_metric: 0.995997\n",
      "Train Epoch: 29, Loss: 0.006604\n",
      "Spend time for 600 images: 121.305079 sec\n",
      "Train Epoch: 29, Loss: 0.006704\n",
      "Spend time for 600 images: 121.539389 sec\n",
      "Train Epoch: 29, Loss: 0.006824\n",
      "Spend time for 600 images: 121.375904 sec\n",
      "Train Epoch: 29, Loss: 0.006834\n",
      "Spend time for 600 images: 121.455635 sec\n",
      "Train Epoch: 29, Loss: 0.006786\n",
      "Spend time for 600 images: 121.036493 sec\n",
      "Train Epoch: 29, Loss: 0.006715\n",
      "Spend time for 600 images: 121.023001 sec\n",
      "Epoch 29, train loss: 0.006743, valid_loss: 0.008940, valid_metric: 0.995894\n",
      "Train Epoch: 30, Loss: 0.006695\n",
      "Spend time for 600 images: 121.039974 sec\n",
      "Train Epoch: 30, Loss: 0.006565\n",
      "Spend time for 600 images: 121.398475 sec\n",
      "Train Epoch: 30, Loss: 0.006500\n",
      "Spend time for 600 images: 121.666037 sec\n",
      "Train Epoch: 30, Loss: 0.006626\n",
      "Spend time for 600 images: 121.293608 sec\n",
      "Train Epoch: 30, Loss: 0.006720\n",
      "Spend time for 600 images: 121.413881 sec\n",
      "Train Epoch: 30, Loss: 0.007072\n",
      "Spend time for 600 images: 121.513124 sec\n",
      "Epoch 30, train loss: 0.006697, valid_loss: 0.008555, valid_metric: 0.996337\n"
     ]
    }
   ],
   "source": [
    "result = my_model.fit(criterion,\n",
    "             metric,\n",
    "             optimizer,\n",
    "             train_data_loader,\n",
    "             valid_data_loader,\n",
    "             epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2d15ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 30, train loss: 0.002916, valid_loss: 0.003709, valid_metric: 0.996013 - model_lab_v2.pth - no aug, \n",
    "# no nomalize\n",
    "# Сохраняем веса обученной модели\n",
    "my_model.save(path_to_save = './model_lab_v3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e78f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:19: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:19: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:31: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/timm/models/layers/padding.py:32: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)\n"
     ]
    }
   ],
   "source": [
    "# Сохраняем оттрассированную модель\n",
    "my_model.trace_save(path_to_save = './model_lab_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9bc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/lib/python3.8/site-packages/torch/onnx/utils.py:88: UserWarning: `enable_onnx_checker' is deprecated and ignored. It will be removed inthe next PyTorch release. To proceed despite ONNX checker failures, youcan catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"`enable_onnx_checker' is deprecated and ignored. It will be removed in\"\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:381: UserWarning: You are trying to export the model with onnx:Resize for ONNX opset version 10. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator.\n",
      "  warnings.warn(\"You are trying to export the model with \" + onnx_op + \" for ONNX opset version \"\n"
     ]
    }
   ],
   "source": [
    "# Экспорт модели в onnx\n",
    "my_model.onnx_save(path_to_save = './model_lab_v3.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700bfa",
   "metadata": {},
   "source": [
    "## Загрузка сохраненной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94d8df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(encoder_name='timm-mobilenetv3_small_100', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model_lab_v3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb8d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем оттрассированную модель\n",
    "my_model = torch.jit.load('./model_lab_v3.pt')\n",
    "my_model = NeuralNetwork(model=my_model)\n",
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988cd6",
   "metadata": {},
   "source": [
    "## Предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a31d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "test_dataset = '/home/dima/carvana_dataset/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6cc16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}\n",
    "test_dataframe['img_addr'] = list(glob.glob(test_dataset + \"/*\"))\n",
    "test_dataframe = pd.DataFrame(test_dataframe)\n",
    "test_dataframe['img_name'] = test_dataframe['img_addr'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b8aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDatasetForTest(test_dataframe, device, valid_transform)\n",
    "test_data_loader = DataLoader(test_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51507db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4164/2166412596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# С сохранением сгенерированных масок в predict_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                  mask_treashold=mask_treashold, generate_rle_dataframe=True)\n",
      "\u001b[0;32m/tmp/ipykernel_4164/4293119295.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_data_loader, predict_directory, output_size, mask_treashold, generate_rle_dataframe)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpredict_directory\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# [0] - избавляемся от батч размерности\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                     \u001b[0mPIL_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                     \u001b[0mPIL_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_directory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.gif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# С сохранением сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acf87e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 600\n",
      "Total time: 59.67 sec\n",
      "Time to process 600 images: 59.67 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 1200\n",
      "Total time: 118.11 sec\n",
      "Time to process 600 images: 58.44 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 1800\n",
      "Total time: 176.53 sec\n",
      "Time to process 600 images: 58.42 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 2400\n",
      "Total time: 235.01 sec\n",
      "Time to process 600 images: 58.48 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 3000\n",
      "Total time: 293.73 sec\n",
      "Time to process 600 images: 58.72 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 3600\n",
      "Total time: 352.64 sec\n",
      "Time to process 600 images: 58.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 4200\n",
      "Total time: 411.35 sec\n",
      "Time to process 600 images: 58.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 4800\n",
      "Total time: 470.24 sec\n",
      "Time to process 600 images: 58.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 5400\n",
      "Total time: 528.59 sec\n",
      "Time to process 600 images: 58.35 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 6000\n",
      "Total time: 587.38 sec\n",
      "Time to process 600 images: 58.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 6600\n",
      "Total time: 646.53 sec\n",
      "Time to process 600 images: 59.15 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 7200\n",
      "Total time: 704.86 sec\n",
      "Time to process 600 images: 58.33 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 7800\n",
      "Total time: 763.25 sec\n",
      "Time to process 600 images: 58.38 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 8400\n",
      "Total time: 822.28 sec\n",
      "Time to process 600 images: 59.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 9000\n",
      "Total time: 881.52 sec\n",
      "Time to process 600 images: 59.24 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 9600\n",
      "Total time: 939.90 sec\n",
      "Time to process 600 images: 58.38 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 10200\n",
      "Total time: 999.36 sec\n",
      "Time to process 600 images: 59.46 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 10800\n",
      "Total time: 1058.39 sec\n",
      "Time to process 600 images: 59.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 11400\n",
      "Total time: 1118.33 sec\n",
      "Time to process 600 images: 59.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 12000\n",
      "Total time: 1180.92 sec\n",
      "Time to process 600 images: 62.59 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 12600\n",
      "Total time: 1239.60 sec\n",
      "Time to process 600 images: 58.68 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 13200\n",
      "Total time: 1298.64 sec\n",
      "Time to process 600 images: 59.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 13800\n",
      "Total time: 1358.04 sec\n",
      "Time to process 600 images: 59.40 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 14400\n",
      "Total time: 1416.30 sec\n",
      "Time to process 600 images: 58.25 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 15000\n",
      "Total time: 1474.75 sec\n",
      "Time to process 600 images: 58.45 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 15600\n",
      "Total time: 1533.18 sec\n",
      "Time to process 600 images: 58.43 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 16200\n",
      "Total time: 1592.04 sec\n",
      "Time to process 600 images: 58.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 16800\n",
      "Total time: 1650.46 sec\n",
      "Time to process 600 images: 58.41 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 17400\n",
      "Total time: 1708.78 sec\n",
      "Time to process 600 images: 58.32 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 18000\n",
      "Total time: 1767.14 sec\n",
      "Time to process 600 images: 58.36 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 18600\n",
      "Total time: 1825.66 sec\n",
      "Time to process 600 images: 58.53 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 19200\n",
      "Total time: 1884.10 sec\n",
      "Time to process 600 images: 58.44 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 19800\n",
      "Total time: 1942.57 sec\n",
      "Time to process 600 images: 58.47 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 20400\n",
      "Total time: 2000.99 sec\n",
      "Time to process 600 images: 58.43 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 21000\n",
      "Total time: 2059.49 sec\n",
      "Time to process 600 images: 58.49 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 21600\n",
      "Total time: 2118.34 sec\n",
      "Time to process 600 images: 58.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 22200\n",
      "Total time: 2177.49 sec\n",
      "Time to process 600 images: 59.14 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 22800\n",
      "Total time: 2235.84 sec\n",
      "Time to process 600 images: 58.35 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 23400\n",
      "Total time: 2294.24 sec\n",
      "Time to process 600 images: 58.40 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 24000\n",
      "Total time: 2352.59 sec\n",
      "Time to process 600 images: 58.36 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 24600\n",
      "Total time: 2410.99 sec\n",
      "Time to process 600 images: 58.40 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 25200\n",
      "Total time: 2469.34 sec\n",
      "Time to process 600 images: 58.35 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 25800\n",
      "Total time: 2527.46 sec\n",
      "Time to process 600 images: 58.12 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 26400\n",
      "Total time: 2585.60 sec\n",
      "Time to process 600 images: 58.14 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 27000\n",
      "Total time: 2643.78 sec\n",
      "Time to process 600 images: 58.18 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 27600\n",
      "Total time: 2701.89 sec\n",
      "Time to process 600 images: 58.11 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 28200\n",
      "Total time: 2760.04 sec\n",
      "Time to process 600 images: 58.14 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 28800\n",
      "Total time: 2818.13 sec\n",
      "Time to process 600 images: 58.09 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 29400\n",
      "Total time: 2876.30 sec\n",
      "Time to process 600 images: 58.17 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 30000\n",
      "Total time: 2934.34 sec\n",
      "Time to process 600 images: 58.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 30600\n",
      "Total time: 2992.41 sec\n",
      "Time to process 600 images: 58.08 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 31200\n",
      "Total time: 3050.40 sec\n",
      "Time to process 600 images: 57.98 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 31800\n",
      "Total time: 3108.47 sec\n",
      "Time to process 600 images: 58.07 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 32400\n",
      "Total time: 3166.44 sec\n",
      "Time to process 600 images: 57.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 33000\n",
      "Total time: 3224.46 sec\n",
      "Time to process 600 images: 58.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 33600\n",
      "Total time: 3282.45 sec\n",
      "Time to process 600 images: 58.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 34200\n",
      "Total time: 3340.51 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 34800\n",
      "Total time: 3398.51 sec\n",
      "Time to process 600 images: 58.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 35400\n",
      "Total time: 3456.56 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 36000\n",
      "Total time: 3514.61 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 36600\n",
      "Total time: 3572.71 sec\n",
      "Time to process 600 images: 58.10 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 37200\n",
      "Total time: 3630.70 sec\n",
      "Time to process 600 images: 57.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 37800\n",
      "Total time: 3688.70 sec\n",
      "Time to process 600 images: 57.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 38400\n",
      "Total time: 3746.73 sec\n",
      "Time to process 600 images: 58.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 39000\n",
      "Total time: 3804.76 sec\n",
      "Time to process 600 images: 58.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 39600\n",
      "Total time: 3862.85 sec\n",
      "Time to process 600 images: 58.09 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 40200\n",
      "Total time: 3920.89 sec\n",
      "Time to process 600 images: 58.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 40800\n",
      "Total time: 3978.92 sec\n",
      "Time to process 600 images: 58.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 41400\n",
      "Total time: 4036.93 sec\n",
      "Time to process 600 images: 58.01 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 42000\n",
      "Total time: 4095.01 sec\n",
      "Time to process 600 images: 58.08 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 42600\n",
      "Total time: 4153.05 sec\n",
      "Time to process 600 images: 58.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 43200\n",
      "Total time: 4211.11 sec\n",
      "Time to process 600 images: 58.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 43800\n",
      "Total time: 4269.12 sec\n",
      "Time to process 600 images: 58.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 44400\n",
      "Total time: 4327.08 sec\n",
      "Time to process 600 images: 57.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 45000\n",
      "Total time: 4385.04 sec\n",
      "Time to process 600 images: 57.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 45600\n",
      "Total time: 4443.08 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 46200\n",
      "Total time: 4501.00 sec\n",
      "Time to process 600 images: 57.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 46800\n",
      "Total time: 4559.00 sec\n",
      "Time to process 600 images: 58.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 47400\n",
      "Total time: 4616.99 sec\n",
      "Time to process 600 images: 57.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 48000\n",
      "Total time: 4675.08 sec\n",
      "Time to process 600 images: 58.10 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 48600\n",
      "Total time: 4733.59 sec\n",
      "Time to process 600 images: 58.51 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 49200\n",
      "Total time: 4791.97 sec\n",
      "Time to process 600 images: 58.37 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 49800\n",
      "Total time: 4850.35 sec\n",
      "Time to process 600 images: 58.38 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 50400\n",
      "Total time: 4908.75 sec\n",
      "Time to process 600 images: 58.40 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 51000\n",
      "Total time: 4967.12 sec\n",
      "Time to process 600 images: 58.37 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 51600\n",
      "Total time: 5025.61 sec\n",
      "Time to process 600 images: 58.49 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 52200\n",
      "Total time: 5083.72 sec\n",
      "Time to process 600 images: 58.11 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 52800\n",
      "Total time: 5141.75 sec\n",
      "Time to process 600 images: 58.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 53400\n",
      "Total time: 5199.78 sec\n",
      "Time to process 600 images: 58.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 54000\n",
      "Total time: 5257.80 sec\n",
      "Time to process 600 images: 58.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 54600\n",
      "Total time: 5315.80 sec\n",
      "Time to process 600 images: 58.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 55200\n",
      "Total time: 5373.87 sec\n",
      "Time to process 600 images: 58.07 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 55800\n",
      "Total time: 5431.93 sec\n",
      "Time to process 600 images: 58.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 56400\n",
      "Total time: 5489.99 sec\n",
      "Time to process 600 images: 58.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 57000\n",
      "Total time: 5548.15 sec\n",
      "Time to process 600 images: 58.16 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 57600\n",
      "Total time: 5606.21 sec\n",
      "Time to process 600 images: 58.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 58200\n",
      "Total time: 5664.29 sec\n",
      "Time to process 600 images: 58.08 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 58800\n",
      "Total time: 5722.32 sec\n",
      "Time to process 600 images: 58.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 59400\n",
      "Total time: 5780.47 sec\n",
      "Time to process 600 images: 58.15 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 60000\n",
      "Total time: 5838.58 sec\n",
      "Time to process 600 images: 58.10 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 60600\n",
      "Total time: 5896.68 sec\n",
      "Time to process 600 images: 58.11 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 61200\n",
      "Total time: 5954.81 sec\n",
      "Time to process 600 images: 58.12 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 61800\n",
      "Total time: 6013.09 sec\n",
      "Time to process 600 images: 58.29 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 62400\n",
      "Total time: 6071.12 sec\n",
      "Time to process 600 images: 58.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 63000\n",
      "Total time: 6129.13 sec\n",
      "Time to process 600 images: 58.01 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 63600\n",
      "Total time: 6187.10 sec\n",
      "Time to process 600 images: 57.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 64200\n",
      "Total time: 6245.10 sec\n",
      "Time to process 600 images: 58.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 64800\n",
      "Total time: 6303.29 sec\n",
      "Time to process 600 images: 58.19 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 65400\n",
      "Total time: 6361.71 sec\n",
      "Time to process 600 images: 58.42 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 66000\n",
      "Total time: 6420.16 sec\n",
      "Time to process 600 images: 58.45 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 66600\n",
      "Total time: 6478.67 sec\n",
      "Time to process 600 images: 58.51 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 67200\n",
      "Total time: 6537.06 sec\n",
      "Time to process 600 images: 58.39 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 67800\n",
      "Total time: 6595.39 sec\n",
      "Time to process 600 images: 58.33 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 68400\n",
      "Total time: 6653.76 sec\n",
      "Time to process 600 images: 58.37 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 69000\n",
      "Total time: 6711.86 sec\n",
      "Time to process 600 images: 58.10 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 69600\n",
      "Total time: 6769.80 sec\n",
      "Time to process 600 images: 57.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 70200\n",
      "Total time: 6827.78 sec\n",
      "Time to process 600 images: 57.98 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 70800\n",
      "Total time: 6885.67 sec\n",
      "Time to process 600 images: 57.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 71400\n",
      "Total time: 6943.62 sec\n",
      "Time to process 600 images: 57.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 72000\n",
      "Total time: 7001.53 sec\n",
      "Time to process 600 images: 57.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 72600\n",
      "Total time: 7059.49 sec\n",
      "Time to process 600 images: 57.96 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 73200\n",
      "Total time: 7117.54 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 73800\n",
      "Total time: 7175.68 sec\n",
      "Time to process 600 images: 58.13 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 74400\n",
      "Total time: 7233.62 sec\n",
      "Time to process 600 images: 57.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 75000\n",
      "Total time: 7291.53 sec\n",
      "Time to process 600 images: 57.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 75600\n",
      "Total time: 7349.49 sec\n",
      "Time to process 600 images: 57.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 76200\n",
      "Total time: 7407.35 sec\n",
      "Time to process 600 images: 57.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 76800\n",
      "Total time: 7465.41 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 77400\n",
      "Total time: 7523.46 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 78000\n",
      "Total time: 7581.53 sec\n",
      "Time to process 600 images: 58.07 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 78600\n",
      "Total time: 7639.46 sec\n",
      "Time to process 600 images: 57.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 79200\n",
      "Total time: 7697.49 sec\n",
      "Time to process 600 images: 58.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 79800\n",
      "Total time: 7755.48 sec\n",
      "Time to process 600 images: 58.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 80400\n",
      "Total time: 7813.47 sec\n",
      "Time to process 600 images: 57.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 81000\n",
      "Total time: 7871.39 sec\n",
      "Time to process 600 images: 57.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 81600\n",
      "Total time: 7929.44 sec\n",
      "Time to process 600 images: 58.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 82200\n",
      "Total time: 7987.43 sec\n",
      "Time to process 600 images: 58.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 82800\n",
      "Total time: 8045.46 sec\n",
      "Time to process 600 images: 58.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 83400\n",
      "Total time: 8103.47 sec\n",
      "Time to process 600 images: 58.01 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 84000\n",
      "Total time: 8161.48 sec\n",
      "Time to process 600 images: 58.01 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 84600\n",
      "Total time: 8219.44 sec\n",
      "Time to process 600 images: 57.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 85200\n",
      "Total time: 8277.41 sec\n",
      "Time to process 600 images: 57.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 85800\n",
      "Total time: 8335.34 sec\n",
      "Time to process 600 images: 57.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 86400\n",
      "Total time: 8393.36 sec\n",
      "Time to process 600 images: 58.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 87000\n",
      "Total time: 8451.30 sec\n",
      "Time to process 600 images: 57.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 87600\n",
      "Total time: 8509.27 sec\n",
      "Time to process 600 images: 57.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 88200\n",
      "Total time: 8567.45 sec\n",
      "Time to process 600 images: 58.18 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 88800\n",
      "Total time: 8625.89 sec\n",
      "Time to process 600 images: 58.44 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 89400\n",
      "Total time: 8684.34 sec\n",
      "Time to process 600 images: 58.46 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 90000\n",
      "Total time: 8744.31 sec\n",
      "Time to process 600 images: 59.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 90600\n",
      "Total time: 8806.35 sec\n",
      "Time to process 600 images: 62.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 91200\n",
      "Total time: 8867.51 sec\n",
      "Time to process 600 images: 61.16 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 91800\n",
      "Total time: 8927.94 sec\n",
      "Time to process 600 images: 60.44 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 92400\n",
      "Total time: 8987.84 sec\n",
      "Time to process 600 images: 59.90 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 93000\n",
      "Total time: 9048.69 sec\n",
      "Time to process 600 images: 60.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 93600\n",
      "Total time: 9109.00 sec\n",
      "Time to process 600 images: 60.31 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 94200\n",
      "Total time: 9169.35 sec\n",
      "Time to process 600 images: 60.35 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 94800\n",
      "Total time: 9230.92 sec\n",
      "Time to process 600 images: 61.56 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 95400\n",
      "Total time: 9291.89 sec\n",
      "Time to process 600 images: 60.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 96000\n",
      "Total time: 9353.14 sec\n",
      "Time to process 600 images: 61.25 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 96600\n",
      "Total time: 9414.76 sec\n",
      "Time to process 600 images: 61.62 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 97200\n",
      "Total time: 9476.02 sec\n",
      "Time to process 600 images: 61.26 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 97800\n",
      "Total time: 9537.60 sec\n",
      "Time to process 600 images: 61.58 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 98400\n",
      "Total time: 9598.46 sec\n",
      "Time to process 600 images: 60.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 99000\n",
      "Total time: 9659.23 sec\n",
      "Time to process 600 images: 60.76 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 99600\n",
      "Total time: 9720.56 sec\n",
      "Time to process 600 images: 61.34 sec\n"
     ]
    }
   ],
   "source": [
    "# Без сохранения сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad9e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем датафрейм с результатом для заливки на kaggle\n",
    "rle_dataframe.to_csv('rle_dataframe.csv', index=True)\n",
    "sample_submission = pd.read_csv('/home/dima/carvana_dataset/sample_submission.csv')\n",
    "sample_submission = sample_submission.merge(rle_dataframe, how='left', left_on='img', right_on='img_name')\n",
    "sample_submission.drop(columns=['rle_mask', 'img_name'], inplace=True)\n",
    "sample_submission.rename(columns={'img_rle': 'rle_mask'}, inplace=True)\n",
    "sample_submission.to_csv('submission_13_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81effd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6cde352f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100064, 2)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43041e5d",
   "metadata": {},
   "source": [
    "## Сравнение интерполяций исходных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edb4e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33b9e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводим модель по известной архитектуре и сохраненным весам\n",
    "model = smp.DeepLabV3Plus(encoder_name='mobilenet_v2', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model_deeplab_30epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ccf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81425d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform_area = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_AREA),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform_linear = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_LINEAR),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5073c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c50436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "valid_data_area = CustomDatasetForTrain(valid_df, device, valid_transform_area, skip_mask=True)\n",
    "valid_data_loader_area = DataLoader(valid_data_area, batch_size=2, shuffle=False)\n",
    "\n",
    "valid_data_linear = CustomDatasetForTrain(valid_df, device, valid_transform_linear, skip_mask=True)\n",
    "valid_data_loader_linear = DataLoader(valid_data_linear, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09853f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_area = []\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader_area):\n",
    "\n",
    "    out_area = my_model(data)\n",
    "    out_area = F.interpolate(input=out_area, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    dice_area = dice(out_area, labels)\n",
    "    dices_area.append(dice_area.item())\n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71da6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_area = np.mean(dices_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3219f5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958455577492714"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "167a98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_linear = []\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader_linear):\n",
    "\n",
    "    out_linear = my_model(data)\n",
    "    out_linear = F.interpolate(input=out_linear, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    dice_linear = dice(out_linear, labels)\n",
    "    dices_linear.append(dice_linear.item())\n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b90446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_linear = np.mean(dices_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72f3b6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958532294258475"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad227b0",
   "metadata": {},
   "source": [
    "### Вывод: INTER_AREA дает примерно такой же результат как и INTER_LINEAR, но работает быстрее, поэтому используем INTER_AREA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d32e87",
   "metadata": {},
   "source": [
    "## Сравнение интерполяций результата"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c9422",
   "metadata": {},
   "source": [
    "### Для 1000 батчей с усреднением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f6118d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5efc0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_nearest = []\n",
    "dices_bilinear_align = []\n",
    "dices_bicubic_align = []\n",
    "dices_bilinear = []\n",
    "dices_bicubic = []\n",
    "\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "\n",
    "    out = my_model(data)\n",
    "    \n",
    "    output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "    output_bilinear_align = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=True)\n",
    "    output_bicubic_align = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=True)\n",
    "    output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=False)\n",
    "\n",
    "    dice_nearest = dice(output_nearest, labels)\n",
    "    dice_bilinear_align = dice(output_bilinear_align, labels)\n",
    "    dice_bicubic_align = dice(output_bicubic_align, labels)\n",
    "    dice_bilinear = dice(output_bilinear, labels)\n",
    "    dice_bicubic = dice(output_bicubic, labels)\n",
    "    \n",
    "    \n",
    "    dices_nearest.append(dice_nearest.item())\n",
    "    dices_bilinear_align.append(dice_bilinear_align.item())\n",
    "    dices_bicubic_align.append(dice_bicubic_align.item())\n",
    "    dices_bilinear.append(dice_bilinear.item())\n",
    "    dices_bicubic.append(dice_bicubic.item())\n",
    "    \n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f80c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dices_nearest: 0.9950660129077733\n",
      "dices_bilinear_align: 0.9958146193996071\n",
      "dices_bicubic_align: 0.9958100168034434\n",
      "dices_bilinear: 0.9958423018455506\n",
      "dices_bicubic: 0.9958409286104143\n"
     ]
    }
   ],
   "source": [
    "print(f'dices_nearest: {np.mean(dices_nearest)}')\n",
    "print(f'dices_bilinear_align: {np.mean(dices_bilinear_align)}')\n",
    "print(f'dices_bicubic_align: {np.mean(dices_bicubic_align)}')\n",
    "print(f'dices_bilinear: {np.mean(dices_bilinear)}')\n",
    "print(f'dices_bicubic: {np.mean(dices_bicubic)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b4d08",
   "metadata": {},
   "source": [
    "### Для одного батча с сохранением картинок для просмотра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ecbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "iterator = iter(valid_data_loader)\n",
    "input_tensor = iterator.next()\n",
    "out = my_model.model(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "311eb4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice_nearest: 0.9952350854873657\n",
      "dice_bilinear: 0.9960499405860901\n",
      "dice_bicubic: 0.9960504770278931\n"
     ]
    }
   ],
   "source": [
    "output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=True)\n",
    "output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=True)\n",
    "\n",
    "\n",
    "dice_nearest = dice(output_nearest, input_tensor[2])\n",
    "dice_bilinear = dice(output_bilinear, input_tensor[2])\n",
    "dice_bicubic = dice(output_bicubic, input_tensor[2])\n",
    "\n",
    "print(f'dice_nearest: {dice_nearest}')\n",
    "print(f'dice_bilinear: {dice_bilinear}')\n",
    "print(f'dice_bicubic: {dice_bicubic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56be51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice_nearest: 0.9952350854873657\n",
      "dice_bilinear: 0.9961531162261963\n",
      "dice_bicubic: 0.9961585998535156\n"
     ]
    }
   ],
   "source": [
    "output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=False)\n",
    "\n",
    "dice = DiceMetric()\n",
    "dice_nearest = dice(output_nearest, input_tensor[2])\n",
    "dice_bilinear = dice(output_bilinear, input_tensor[2])\n",
    "dice_bicubic = dice(output_bicubic, input_tensor[2])\n",
    "\n",
    "print(f'dice_nearest: {dice_nearest}')\n",
    "print(f'dice_bilinear: {dice_bilinear}')\n",
    "print(f'dice_bicubic: {dice_bicubic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cdad3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.sigmoid(output_nearest)\n",
    "output_bilinear = torch.sigmoid(output_bilinear)\n",
    "output_bicubic = torch.sigmoid(output_bicubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff4426c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.where(output_nearest > 0.5, 1, 0)\n",
    "output_bilinear = torch.where(output_bilinear > 0.5, 1, 0)\n",
    "output_bicubic = torch.where(output_bicubic > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74349240",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = (output_nearest[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_nearest = Image.fromarray(output_nearest.astype('uint8'), 'L')\n",
    "output_nearest.save((predict_directory+'111').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e05a6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bilinear = (output_bilinear[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_bilinear = Image.fromarray(output_bilinear.astype('uint8'), 'L')\n",
    "output_bilinear.save((predict_directory+'222').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33ccdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bicubic = (output_bicubic[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_bicubic = Image.fromarray(output_bicubic.astype('uint8'), 'L')\n",
    "output_bicubic.save((predict_directory+'333').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a30d2a",
   "metadata": {},
   "source": [
    "### Вывод: лучше использовать bilinear с align_corners = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536c1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d00ac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ee793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
