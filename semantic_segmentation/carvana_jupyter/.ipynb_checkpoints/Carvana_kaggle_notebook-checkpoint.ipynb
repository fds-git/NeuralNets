{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ca4aaf",
   "metadata": {},
   "source": [
    "## Подключение библиотек и загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15971ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f662c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.onnx\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6386d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Выполнять, если датасет не загружен\n",
    "#!pip install -q kaggle\n",
    "#!mkdir ~/.kaggle\n",
    "#!cp ~/kaggle.json ~/.kaggle/\n",
    "#!chmod 600 ~/.kaggle/kaggle.json\n",
    "#!kaggle competitions download -c carvana-image-masking-challenge\n",
    "#!unzip ~/carvana-image-masking-challenge.zip ~/carvana_dataset/\n",
    "\n",
    "#!unzip ~/carvana_dataset/train.zip -d ~/carvana_dataset/train\n",
    "#!unzip ~/carvana_dataset/test.zip -d ~/carvana_dataset/test\n",
    "#!unzip ~/carvana_dataset/train_masks.zip -d ~/carvana_dataset/train_masks\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_hq.zip -d ~/carvana_dataset/train_hq\n",
    "#!unzip ~/carvana_dataset/test_hq.zip -d ~/carvana_dataset/test_hq\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_masks.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/sample_submission.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/metadata.csv.zip  ~/carvana_dataset/\n",
    "\n",
    "#!rm ~/carvana-image-masking-challenge.zip\n",
    "#!rm ~/carvana_dataset/test.zip\n",
    "#!rm ~/carvana_dataset/train_masks.zip\n",
    "#!rm ~/carvana_dataset/train.zip\n",
    "#!rm ~/carvana_dataset/test_hq.zip\n",
    "#!rm ~/carvana_dataset/train_hq.zip\n",
    "#!rm ~/carvana_dataset/train_masks.csv.zip\n",
    "#!rm ~/carvana_dataset/sample_submission.csv.zip\n",
    "#!rm ~/carvana_dataset/metadata.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8be7cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4005c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 15 16:36:11 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   39C    P8     9W / 170W |    382MiB / 12045MiB |      2%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       954      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1742      G   /usr/lib/xorg/Xorg                151MiB |\r\n",
      "|    0   N/A  N/A      1882      G   /usr/bin/gnome-shell               36MiB |\r\n",
      "|    0   N/A  N/A      5793      G   telegram-desktop                    2MiB |\r\n",
      "|    0   N/A  N/A      6158      G   /usr/lib/firefox/firefox          136MiB |\r\n",
      "|    0   N/A  N/A      7043      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "|    0   N/A  N/A      7074      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3608d7",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec78c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(imgs_path: str = None, masks_path: str = None) -> pd.DataFrame:\n",
    "    '''Функция получает на вход пути к директориям с изображениями и масками\n",
    "    и генерирует датафрейм, содержащий имя изображений, их адреса и адреса\n",
    "    соответствующих им масок\n",
    "    Входные параметры:\n",
    "    imgs_path: str - путь к директории с изображениями,\n",
    "    masks_path: str - путь к директории с масками\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data - dataframe, содержащий адреса изображений и соответствующих им масок'''\n",
    "\n",
    "    assert (imgs_path != None) & (masks_path != None)\n",
    "\n",
    "    data_img = {}\n",
    "    data_mask = {}\n",
    "    data_img['imgs_path'] = []\n",
    "    data_mask['masks_path'] = []\n",
    "    data_img['imgs_path'] = list(glob.glob(imgs_path + \"/*\"))\n",
    "    data_mask['masks_path'] = list(glob.glob(masks_path + \"/*\"))\n",
    "\n",
    "    data_img = pd.DataFrame(data_img)\n",
    "    data_mask = pd.DataFrame(data_mask)\n",
    "\n",
    "    def file_name(x):\n",
    "        return x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    data_img[\"file_name\"] = data_img[\"imgs_path\"].apply(lambda x: file_name(x))\n",
    "    data_mask[\"file_name\"] = data_mask[\"masks_path\"].apply(lambda x: file_name(x)[:-5])\n",
    "\n",
    "    data = pd.merge(data_img, data_mask, on = \"file_name\", how = \"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5613d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(source_df: pd.DataFrame, separate_feature: str = None, test_size: float = 0.25) -> pd.DataFrame:\n",
    "    '''Функция разделяет source_df на две части с коэффициентом test_size\n",
    "    по уникальным значениям separate_feature так, чтобы в новых датафреймах\n",
    "    не было строк с одинаковыми значениями из separate_feature\n",
    "    Входные параметры:\n",
    "    source_df: pd.DataFrame - датафрейм для разделения на train и test\n",
    "    separate_feature: str - поле, по которому датафрейм будет разделен\n",
    "    test_size: float - коэффициент разделения дтафрейма\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data_train - датафрейм для тренировки\n",
    "    pd.DataFrame: data_valid - датафрейм для валидации'''\n",
    "  \n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_cars, valid_cars = train_test_split(source_df[separate_feature].unique(), test_size=test_size, random_state=42)\n",
    "        data_valid = source_df[np.isin(source_df[separate_feature].values, valid_cars)]\n",
    "        data_train = source_df[np.isin(source_df[separate_feature].values, train_cars)]\n",
    "        assert source_df.shape[0] == (data_valid.shape[0] + data_train.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "    else:\n",
    "        data_train, data_valid = train_test_split(source_df, test_size=test_size)\n",
    "\n",
    "    return data_train, data_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712baa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(logits: torch.Tensor, targets: torch.Tensor, treashold: float) -> float:\n",
    "    '''Функция для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    Входные параметры:\n",
    "    logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "    targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "    treashold: float - порог для определения класса точки в предсказанной точке\n",
    "    Возвращаемые значения:\n",
    "    score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "    \n",
    "    smooth = 1\n",
    "    num = targets.size(0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    outputs = torch.where(probs > treashold, 1, 0)\n",
    "    m1 = outputs.view(num, -1)\n",
    "    m2 = targets.view(num, -1)\n",
    "    intersection = (m1 * m2)\n",
    "\n",
    "    score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "    score = score.sum() / num\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd108c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "    '''Функция принимает одну маску в тензорном формате, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    tensor: torch.Tensor - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строком виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    tensor = tensor.view(1, -1)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor[0] = 0\n",
    "    tensor[-1] = 0\n",
    "    rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    rle = rle.cpu().detach().numpy()\n",
    "    rle_str = rle_to_string(rle)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dba4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_rle(mask_image: np.ndarray) -> str:\n",
    "    '''Функция принимает одну маску в формате массива numpy, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    mask_image: numpy.ndarray - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    pixels = mask_image.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle_str = rle_to_string(runs)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf01f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_string(runs: torch.Tensor) -> str:\n",
    "    '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "    в строковое представление этой последовательности\n",
    "    Входные параметры:\n",
    "    runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - строковое представление последовательности чисел'''\n",
    "    \n",
    "    rle_str = ' '.join(str(x) for x in runs)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd732aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(mask_addr: str) -> str:\n",
    "    '''Функция преобразует маску, имеющую адрес mask_addr и сохраненную в\n",
    "    формате .gif, элементы которой имеют значения 0 и 1 в rle представление\n",
    "    в строковом виде\n",
    "    Входные параметры:\n",
    "    mask_addr: str - адрес маски\n",
    "    Возвращаемые значения:\n",
    "    mask_rle: str - rle представление маски в строком виде'''\n",
    "    \n",
    "    mask = Image.open(mask_addr).convert('LA') # преобразование в серый\n",
    "    mask = np.asarray(mask).astype('float')[:,:,0]\n",
    "    mask = mask/255.0\n",
    "    mask_rle = numpy_to_rle(mask)\n",
    "    return mask_rle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dd14a",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b539ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceMetric(nn.Module):\n",
    "    '''Класс для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    с заданным порогом для определния класса каждой точки изображения'''\n",
    "    \n",
    "    def __init__(self, treashold: float=0.5):\n",
    "        '''Входные параметры:\n",
    "        treashold: float - порог для определения класса точки в предсказанной точке'''\n",
    "        \n",
    "        super(DiceMetric, self).__init__()\n",
    "        self.treashold = treashold\n",
    "\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            smooth = 1\n",
    "            num = targets.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            outputs = torch.where(probs > self.treashold, 1., 0.)\n",
    "            m1 = outputs.view(num, -1)\n",
    "            m2 = targets.view(num, -1)\n",
    "            intersection = (m1 * m2)\n",
    "\n",
    "            score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "            score = score.sum() / num\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e9aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    '''Класс для вычисления DICE loss для набора изображенй в формате torch.Tensor'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE loss для набора предсказанных масок'''\n",
    "        \n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e380bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCESoftDiceLoss(nn.Module):\n",
    "    '''Класс для вычисления BCESoftDice Loss для набора изображенй в формате torch.Tensor'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BCESoftDiceLoss, self).__init__()\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss()\n",
    "        self.soft_dice = SoftDiceLoss()\n",
    "\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        bce_dice: float - значение BCESoftDice loss для набора предсказанных масок'''\n",
    "        \n",
    "        bce_dice = self.bce(logits, targets) + self.soft_dice(logits, targets)\n",
    "        return bce_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4045a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTrain(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    \n",
    "    def __init__(self, data_info: pd.DataFrame, device: str, transform: object, skip_mask: bool=False):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список преобразований, которым будут подвергнуты изображения и маски\n",
    "        skip_mask: bool - флаг, нужно ли генерировать исходную маску (без изменения размерности)'''\n",
    "        \n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Разделяем датафрейм на rgb картинки \n",
    "        self.image_arr = self.data_info.iloc[:,0]\n",
    "        # и на сегментированные картинки\n",
    "        self.mask_arr = self.data_info.iloc[:,2]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Нужно ли пробрасывать маску изображения на выход без изменений\n",
    "        self.skip_mask = skip_mask\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        index: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        tr_image: torch.Tensor - тензорное представление изображения\n",
    "        tr_mask: torch.Tensor - тензорное представление маски\n",
    "        mask: torch.Tensor - тензорное представление маски без преобразований\n",
    "        (возвращается если значение skip_mask равно True - необходимо при валидации)'''\n",
    "        \n",
    "        image = cv2.imread(self.image_arr[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        # gif не открывается через open cv, поэтому используем для чтения PIL Image\n",
    "        mask = Image.open(self.mask_arr[index])\n",
    "        mask = np.asarray(mask)#.astype('float')\n",
    "        \n",
    "        transformed = self.transform(image=image, mask=mask)\n",
    "        tr_image = transformed['image']\n",
    "        tr_mask = transformed['mask']\n",
    "        \n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        tr_mask = tr_mask.to(self.device).float().unsqueeze(0)\n",
    "        \n",
    "        # Если необходима исходная маска, то дополнительно возвращаем ее\n",
    "        if self.skip_mask == True:\n",
    "            mask = (torch.as_tensor(mask)).to(self.device).float().unsqueeze(0)\n",
    "            return (tr_image, tr_mask, mask)\n",
    "        else:\n",
    "            return (tr_image, tr_mask)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15588a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTest(Dataset):\n",
    "    '''Класс для создания тестовых датасетов'''\n",
    "    \n",
    "    def __init__(self, data_info, device: str, transform: object):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список преобразований, которым будут подвергнуты изображения и маски\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTest'''\n",
    "        \n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Получаем адреса RGB изображений \n",
    "        self.image_addresses = self.data_info.iloc[:,0]\n",
    "        # Получаем имена RGB изображений \n",
    "        self.image_names = self.data_info.iloc[:,1]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        '''Входные параметры:\n",
    "        index: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        index: int - индекс для обращения к элементам датафрейма data_info\n",
    "        tr_image: torch.Tensor - тензорное представление изображения\n",
    "        image_name: str - имя изображения'''\n",
    "        \n",
    "        image = cv2.imread(self.image_addresses[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        transformed = self.transform(image=image)\n",
    "        tr_image = transformed['image']\n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        image_name = self.image_names[index]\n",
    "    \n",
    "        return (index, tr_image, image_name)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0a9bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''Класс для работы с нейронной сетью для семантической сегментации Carvana'''\n",
    "    \n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: object - последовательность слоев или модель, через которую будут проходить данные'''\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "    def forward(self, input_data: torch.Tensor) -> torch.Tensor:\n",
    "        '''Функция прямого прохода через объект класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения\n",
    "        Возвращаемые значения: \n",
    "        output_data: torch.Tensor - тензорное представление маски изображения'''\n",
    "        \n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "        '''Статический метод принимает одну маску в тензорном формате, элементы которой\n",
    "        имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "        Входные параметры:\n",
    "        tensor: torch.Tensor - маска в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "        # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "        # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "        # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "        # не сильно упадет)\n",
    "        with torch.no_grad():\n",
    "            tensor = tensor.view(1, -1)\n",
    "            tensor = tensor.squeeze(0)\n",
    "            tensor[0] = 0\n",
    "            tensor[-1] = 0\n",
    "            rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "            rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "            rle = rle.cpu().detach().numpy()\n",
    "            rle_str = NeuralNetwork.rle_to_string(rle)\n",
    "            return rle_str\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def rle_to_string(runs: torch.Tensor) -> str:\n",
    "        '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "        в строковое представление этой последовательности\n",
    "        Входные параметры:\n",
    "        runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - строковое представление последовательности чисел'''\n",
    "        \n",
    "        rle_str = ' '.join(str(x) for x in runs)\n",
    "        return rle_str\n",
    "    \n",
    "    \n",
    "    def fit(self, criterion: object, metric: object, optimizer: object, \n",
    "                  train_data_loader: DataLoader, valid_data_loader: DataLoader=None, epochs: int=1):\n",
    "        '''Метод для обучения объекта класса\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        optimizer: object - оптимизатор\n",
    "        train_data_loader: DataLoader - загрузчик данных для обучения\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        epochs: int - количество эпох обучени\n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        result = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss =0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, (data, labels) in enumerate(train_data_loader):\n",
    "                data, labels = Variable(data), Variable(labels)        \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % 300 == 0:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {(running_loss/300):.6f}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spend time for {300*data.shape[0]} images: {(time2-time1):.6f} sec')\n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)        \n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                result = self.valid(criterion, metric, valid_data_loader)\n",
    "                valid_loss = result['valid_loss']\n",
    "                valid_metric = result['valid_metric']\n",
    "                print(f\"Epoch {epoch+1}, train loss: {(train_loss):.6f}, valid_loss: {(valid_loss):.6f}, valid_metric: {(valid_metric):.6f}\")\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}')\n",
    "                valid_loss = None\n",
    "                valid_metric = None\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        result['epoch_train_losses'] = epoch_train_losses\n",
    "        result['epoch_valid_losses'] = epoch_valid_losses\n",
    "        result['epoch_valid_metrics'] = epoch_valid_metrics\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def valid(self, criterion: object, metric: object, valid_data_loader: DataLoader):\n",
    "        '''Метод для валидации модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.model.eval()\n",
    "        valid_metrics = []\n",
    "        valid_losses = []\n",
    "        result = {}\n",
    "        for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "            data, labels, labels_small = Variable(data), Variable(labels), Variable(labels_small)\n",
    "            outputs = self.model(data)\n",
    "            # loss вычисляется для сжатых масок для правильной валидации (обучались на сжатых)\n",
    "            # чтобы вовремя определить переобучение\n",
    "            loss = criterion(outputs, labels_small)\n",
    "            valid_losses.append(loss.item())\n",
    "            #Преобразуем выход модели к размеру соответствующей маски\n",
    "            outputs = F.interpolate(input=outputs, size=(labels.shape[2], \n",
    "                                                         labels.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # метрика считается для исходных размеров потому что именно так итоговое качество\n",
    "            # определяется алгоритмом kaggle \n",
    "            metric_value = metric(outputs, labels)\n",
    "            valid_metrics.append(metric_value.item())\n",
    "                    \n",
    "        valid_loss    = np.mean(valid_losses)\n",
    "        valid_metric  = np.mean(valid_metrics)\n",
    "        result['valid_loss'] = valid_loss\n",
    "        result['valid_metric'] = valid_metric\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def predict(self, test_data_loader: DataLoader, predict_directory: str=None, output_size: tuple=(1280, 1918), \n",
    "                mask_treashold: float=0.5, generate_rle_dataframe: bool=True) -> pd.DataFrame:\n",
    "        '''Метод предсказания масок для тестового набора изображений\n",
    "        Входные параметры:\n",
    "        test_data_loader: DataLoader - загрузчик данных для предсказания\n",
    "        predict_directory: str - директория, в которую будут сохраняться сгенерированные маски (если None,\n",
    "        то маски сохраняться не будут)\n",
    "        output_size: tuple - пространственная размерность выходных масок\n",
    "        mask_treashold: float - порог, по которому будет определяться класс каждой точки для масок\n",
    "        generate_rle_dataframe: bool - флаг, нужна ли генерация rle представлений масок\n",
    "        Возвращаемые значения:\n",
    "        rle_dataframe: pd.DataFrame - датафрейм с rle представлениями для масок (если \n",
    "        generate_rle_dataframe==True)\n",
    "        Маски в формате .gif для изображений с соответствующими именами, находятся в директории predict_directory'''\n",
    "        \n",
    "        self.model.eval()\n",
    "        img_names = []\n",
    "        img_rles = []\n",
    "        time1 = time.time()\n",
    "        time2 = time.time()\n",
    "        for batch_idx, (index, img, img_name)  in enumerate(test_data_loader):\n",
    "\n",
    "            img = Variable(img)        \n",
    "            pred_mask_logit = self.model(img)\n",
    "            pred_mask_logit = F.interpolate(input=pred_mask_logit, size=output_size, mode='bilinear', align_corners=False)\n",
    "            pred_mask_logit_prob = torch.sigmoid(pred_mask_logit)\n",
    "            pred_mask = torch.where(pred_mask_logit_prob > mask_treashold, 1, 0)\n",
    "            \n",
    "            # Каждое изображение в тензоре преобразуем в картинку и сохраняем\n",
    "            for i in range(pred_mask.shape[0]):\n",
    "                if predict_directory != None:\n",
    "                    mask = (pred_mask[i].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "                    PIL_image = Image.fromarray(mask.astype('uint8'), 'L')\n",
    "                    PIL_image.save((predict_directory+img_name[i]).split('.')[0]+'.gif')\n",
    "                \n",
    "                # Если требуется, получаем значения rle для каждой картинки\n",
    "                if generate_rle_dataframe == True:\n",
    "                    img_names.append(img_name[i])\n",
    "                    img_rles.append(NeuralNetwork.tensor_to_rle(pred_mask[i]))\n",
    "            \n",
    "            if (batch_idx+1) % 300 == 0:\n",
    "                    print('-'*50)\n",
    "                    print(f'Processed images: {(batch_idx+1)*img.shape[0]}')\n",
    "                    time3 = time.time()\n",
    "                    print(f'Total time: {(time3-time1):.2f} sec')\n",
    "                    print(f'Time to process {300*img.shape[0]} images: {(time3-time2):.2f} sec')\n",
    "                    time2 = time.time()\n",
    "                \n",
    "        if generate_rle_dataframe == True:\n",
    "            rle_dataframe = pd.DataFrame(list(zip(img_names, img_rles)), columns =['img_name', 'img_rle'])\n",
    "            return rle_dataframe\n",
    "    \n",
    "    \n",
    "    def save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения весов модели\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения состояния модели'''\n",
    "        \n",
    "        torch.save(self.model.state_dict(), path_to_save)\n",
    "    \n",
    "    \n",
    "    def trace_save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения модели через torchscript\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        \n",
    "        example_forward_input = torch.rand(1, 3, 512, 512).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "            \n",
    "        traced_model = torch.jit.trace((self.model).eval(), example_forward_input)\n",
    "        torch.jit.save(traced_model, path_to_save)\n",
    "    \n",
    "    \n",
    "    def onnx_save(self, path_to_save: str='./carvana_model.onnx'):\n",
    "        '''Метод сохранения модели в формате ONNX\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        \n",
    "        example_forward_input = torch.randn(1, 3, 1024, 1024, requires_grad=True).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "\n",
    "        torch.onnx.export(self.model,\n",
    "                          example_forward_input,\n",
    "                          path_to_save,\n",
    "                          export_params=True,\n",
    "                          opset_version=10,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'],\n",
    "                          dynamic_axes={'input' : {0 : 'batch_size'},    # Модель будет работать с произвольным\n",
    "                                        'output' : {0 : 'batch_size'}})  # размером батча\n",
    "    \n",
    "    \n",
    "    def load(self, path_to_model: str='./model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e07ce",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21937687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/dima/datasets/carvana_dataset'\n",
    "imgs_path  = dataset_path + '/train/train'\n",
    "masks_path = dataset_path + '/train_masks/train_masks'\n",
    "\n",
    "batch_size = 2\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 30\n",
    "mask_treashold = 0.5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1d58730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function preprocess_input at 0x7f7429fc85e0>, input_space='RGB', input_range=[0, 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "preprocess_input = get_preprocessing_fn('timm-mobilenetv3_small_100', pretrained='imagenet')\n",
    "preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "888ec35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(1024, 2048, cv2.INTER_AREA), # для масок автоматически будет применяться своя интерполяция, \n",
    "                                          # поэтому на выходе значения маски останутся 0 и 1\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(1024, 2048, cv2.INTER_AREA), # INTER_AREA как правило лучше осуществляет переход к меньшему разрешению\n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d28b75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e54a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение с валидацией\n",
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_data = CustomDatasetForTrain(train_df, device, train_transform, skip_mask=False)\n",
    "valid_data = CustomDatasetForTrain(valid_df, device, valid_transform, skip_mask=True)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "94c06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение без валидации\n",
    "#train_data = CustomDatasetForTrain(data, device, train_transform, skip_mask=False)\n",
    "#train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4abf005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(encoder_name='timm-mobilenetv3_small_100', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84e40c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNetwork(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "961486dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "DeepLabV3Plus                                           --                        --\n",
       "├─MobileNetV3Encoder: 1-1                               [2, 3, 1024, 2048]        --\n",
       "│    └─MobileNetV3Features: 2                           --                        --\n",
       "│    │    └─Conv2dSame: 3-1                             [2, 16, 512, 1024]        432\n",
       "│    │    └─BatchNorm2d: 3-2                            [2, 16, 512, 1024]        32\n",
       "│    │    └─Hardswish: 3-3                              [2, 16, 512, 1024]        --\n",
       "├─DeepLabV3PlusDecoder: 1-2                             [2, 256, 256, 512]        --\n",
       "│    └─Sequential: 2-1                                  [2, 256, 64, 128]         --\n",
       "│    │    └─ASPP: 3-4                                   [2, 256, 64, 128]         1,083,584\n",
       "│    │    └─SeparableConv2d: 3-5                        [2, 256, 64, 128]         67,840\n",
       "│    │    └─BatchNorm2d: 3-6                            [2, 256, 64, 128]         512\n",
       "│    │    └─ReLU: 3-7                                   [2, 256, 64, 128]         --\n",
       "│    └─UpsamplingBilinear2d: 2-2                        [2, 256, 256, 512]        --\n",
       "│    └─Sequential: 2-3                                  [2, 48, 256, 512]         --\n",
       "│    │    └─Conv2d: 3-8                                 [2, 48, 256, 512]         768\n",
       "│    │    └─BatchNorm2d: 3-9                            [2, 48, 256, 512]         96\n",
       "│    │    └─ReLU: 3-10                                  [2, 48, 256, 512]         --\n",
       "│    └─Sequential: 2-4                                  [2, 256, 256, 512]        --\n",
       "│    │    └─SeparableConv2d: 3-11                       [2, 256, 256, 512]        80,560\n",
       "│    │    └─BatchNorm2d: 3-12                           [2, 256, 256, 512]        512\n",
       "│    │    └─ReLU: 3-13                                  [2, 256, 256, 512]        --\n",
       "├─SegmentationHead: 1-3                                 [2, 1, 1024, 2048]        --\n",
       "│    └─Conv2d: 2-5                                      [2, 1, 256, 512]          257\n",
       "│    └─UpsamplingBilinear2d: 2-6                        [2, 1, 1024, 2048]        --\n",
       "│    └─Activation: 2-7                                  [2, 1, 1024, 2048]        --\n",
       "│    │    └─Identity: 3-14                              [2, 1, 1024, 2048]        --\n",
       "=========================================================================================================\n",
       "Total params: 2,161,137\n",
       "Trainable params: 2,161,137\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 46.52\n",
       "=========================================================================================================\n",
       "Input size (MB): 50.33\n",
       "Forward/backward pass size (MB): 5146.47\n",
       "Params size (MB): 8.64\n",
       "Estimated Total Size (MB): 5205.44\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(2, 3, 1024, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0fb86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCESoftDiceLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "metric = DiceMetric(treashold=mask_treashold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca111f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = my_model.fit(criterion,\n",
    "             metric,\n",
    "             optimizer,\n",
    "             train_data_loader,\n",
    "             valid_data_loader,\n",
    "             epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2d15ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 30, train loss: 0.002916, valid_loss: 0.003709, valid_metric: 0.996013 - model_lab_v2.pth - no aug, \n",
    "# no nomalize\n",
    "# Сохраняем веса обученной модели\n",
    "my_model.save(path_to_save = './model_lab_v3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем оттрассированную модель\n",
    "my_model.trace_save(path_to_save = './model_lab_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт модели в onnx\n",
    "# my_model.onnx_save(path_to_save = './model_lab_v3.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700bfa",
   "metadata": {},
   "source": [
    "## Загрузка сохраненной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94d8df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(encoder_name='timm-mobilenetv3_small_100', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model_lab_v3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb8d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем оттрассированную модель\n",
    "my_model = torch.jit.load('./model_lab_v3.pt')\n",
    "my_model = NeuralNetwork(model=my_model)\n",
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988cd6",
   "metadata": {},
   "source": [
    "## Предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a31d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/datasets/carvana_dataset/test/predict_small/'\n",
    "test_dataset = '/home/dima/datasets/carvana_dataset/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6cc16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}\n",
    "test_dataframe['img_addr'] = list(glob.glob(test_dataset + \"/*\"))\n",
    "test_dataframe = pd.DataFrame(test_dataframe)\n",
    "test_dataframe['img_name'] = test_dataframe['img_addr'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b8aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDatasetForTest(test_dataframe, device, valid_transform)\n",
    "test_data_loader = DataLoader(test_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51507db2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# С сохранением сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "acf87e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Без сохранения сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad9e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем датафрейм с результатом для заливки на kaggle\n",
    "rle_dataframe.to_csv('rle_dataframe.csv', index=True)\n",
    "sample_submission = pd.read_csv('/home/dima/datasets/carvana_dataset/sample_submission.csv')\n",
    "sample_submission = sample_submission.merge(rle_dataframe, how='left', left_on='img', right_on='img_name')\n",
    "sample_submission.drop(columns=['rle_mask', 'img_name'], inplace=True)\n",
    "sample_submission.rename(columns={'img_rle': 'rle_mask'}, inplace=True)\n",
    "sample_submission.to_csv('submission_13_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81effd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6cde352f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100064, 2)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43041e5d",
   "metadata": {},
   "source": [
    "## Сравнение интерполяций исходных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edb4e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/datasets/carvana_dataset/test/predict_small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33b9e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводим модель по известной архитектуре и сохраненным весам\n",
    "model = smp.DeepLabV3Plus(encoder_name='mobilenet_v2', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model_deeplab_30epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ccf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81425d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform_area = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_AREA),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform_linear = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_LINEAR),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5073c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c50436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "valid_data_area = CustomDatasetForTrain(valid_df, device, valid_transform_area, skip_mask=True)\n",
    "valid_data_loader_area = DataLoader(valid_data_area, batch_size=2, shuffle=False)\n",
    "\n",
    "valid_data_linear = CustomDatasetForTrain(valid_df, device, valid_transform_linear, skip_mask=True)\n",
    "valid_data_loader_linear = DataLoader(valid_data_linear, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09853f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_area = []\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader_area):\n",
    "\n",
    "    out_area = my_model(data)\n",
    "    out_area = F.interpolate(input=out_area, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    dice_area = dice(out_area, labels)\n",
    "    dices_area.append(dice_area.item())\n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71da6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_area = np.mean(dices_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3219f5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958455577492714"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "167a98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_linear = []\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader_linear):\n",
    "\n",
    "    out_linear = my_model(data)\n",
    "    out_linear = F.interpolate(input=out_linear, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    dice_linear = dice(out_linear, labels)\n",
    "    dices_linear.append(dice_linear.item())\n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b90446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_linear = np.mean(dices_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72f3b6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958532294258475"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad227b0",
   "metadata": {},
   "source": [
    "### Вывод: INTER_AREA дает примерно такой же результат как и INTER_LINEAR, но работает быстрее, поэтому используем INTER_AREA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d32e87",
   "metadata": {},
   "source": [
    "## Сравнение интерполяций результата"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c9422",
   "metadata": {},
   "source": [
    "### Для 1000 батчей с усреднением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f6118d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5efc0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_nearest = []\n",
    "dices_bilinear_align = []\n",
    "dices_bicubic_align = []\n",
    "dices_bilinear = []\n",
    "dices_bicubic = []\n",
    "\n",
    "for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "\n",
    "    out = my_model(data)\n",
    "    \n",
    "    output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "    output_bilinear_align = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=True)\n",
    "    output_bicubic_align = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=True)\n",
    "    output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "    output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=False)\n",
    "\n",
    "    dice_nearest = dice(output_nearest, labels)\n",
    "    dice_bilinear_align = dice(output_bilinear_align, labels)\n",
    "    dice_bicubic_align = dice(output_bicubic_align, labels)\n",
    "    dice_bilinear = dice(output_bilinear, labels)\n",
    "    dice_bicubic = dice(output_bicubic, labels)\n",
    "    \n",
    "    \n",
    "    dices_nearest.append(dice_nearest.item())\n",
    "    dices_bilinear_align.append(dice_bilinear_align.item())\n",
    "    dices_bicubic_align.append(dice_bicubic_align.item())\n",
    "    dices_bilinear.append(dice_bilinear.item())\n",
    "    dices_bicubic.append(dice_bicubic.item())\n",
    "    \n",
    "    if batch_idx == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f80c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dices_nearest: 0.9950660129077733\n",
      "dices_bilinear_align: 0.9958146193996071\n",
      "dices_bicubic_align: 0.9958100168034434\n",
      "dices_bilinear: 0.9958423018455506\n",
      "dices_bicubic: 0.9958409286104143\n"
     ]
    }
   ],
   "source": [
    "print(f'dices_nearest: {np.mean(dices_nearest)}')\n",
    "print(f'dices_bilinear_align: {np.mean(dices_bilinear_align)}')\n",
    "print(f'dices_bicubic_align: {np.mean(dices_bicubic_align)}')\n",
    "print(f'dices_bilinear: {np.mean(dices_bilinear)}')\n",
    "print(f'dices_bicubic: {np.mean(dices_bicubic)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b4d08",
   "metadata": {},
   "source": [
    "### Для одного батча с сохранением картинок для просмотра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ecbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "iterator = iter(valid_data_loader)\n",
    "input_tensor = iterator.next()\n",
    "out = my_model.model(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "311eb4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice_nearest: 0.9952350854873657\n",
      "dice_bilinear: 0.9960499405860901\n",
      "dice_bicubic: 0.9960504770278931\n"
     ]
    }
   ],
   "source": [
    "output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=True)\n",
    "output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=True)\n",
    "\n",
    "\n",
    "dice_nearest = dice(output_nearest, input_tensor[2])\n",
    "dice_bilinear = dice(output_bilinear, input_tensor[2])\n",
    "dice_bicubic = dice(output_bicubic, input_tensor[2])\n",
    "\n",
    "print(f'dice_nearest: {dice_nearest}')\n",
    "print(f'dice_bilinear: {dice_bilinear}')\n",
    "print(f'dice_bicubic: {dice_bicubic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56be51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dice_nearest: 0.9952350854873657\n",
      "dice_bilinear: 0.9961531162261963\n",
      "dice_bicubic: 0.9961585998535156\n"
     ]
    }
   ],
   "source": [
    "output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=False)\n",
    "output_bicubic = F.interpolate(input=out, size=(1280, 1918), mode='bicubic', align_corners=False)\n",
    "\n",
    "dice = DiceMetric()\n",
    "dice_nearest = dice(output_nearest, input_tensor[2])\n",
    "dice_bilinear = dice(output_bilinear, input_tensor[2])\n",
    "dice_bicubic = dice(output_bicubic, input_tensor[2])\n",
    "\n",
    "print(f'dice_nearest: {dice_nearest}')\n",
    "print(f'dice_bilinear: {dice_bilinear}')\n",
    "print(f'dice_bicubic: {dice_bicubic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cdad3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.sigmoid(output_nearest)\n",
    "output_bilinear = torch.sigmoid(output_bilinear)\n",
    "output_bicubic = torch.sigmoid(output_bicubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff4426c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.where(output_nearest > 0.5, 1, 0)\n",
    "output_bilinear = torch.where(output_bilinear > 0.5, 1, 0)\n",
    "output_bicubic = torch.where(output_bicubic > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74349240",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = (output_nearest[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_nearest = Image.fromarray(output_nearest.astype('uint8'), 'L')\n",
    "output_nearest.save((predict_directory+'111').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e05a6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bilinear = (output_bilinear[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_bilinear = Image.fromarray(output_bilinear.astype('uint8'), 'L')\n",
    "output_bilinear.save((predict_directory+'222').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33ccdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bicubic = (output_bicubic[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_bicubic = Image.fromarray(output_bicubic.astype('uint8'), 'L')\n",
    "output_bicubic.save((predict_directory+'333').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a30d2a",
   "metadata": {},
   "source": [
    "### Вывод: лучше использовать bilinear с align_corners = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536c1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d00ac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ee793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
