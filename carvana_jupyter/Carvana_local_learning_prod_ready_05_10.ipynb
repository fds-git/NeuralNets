{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b5928f",
   "metadata": {},
   "source": [
    "# Improvement Carvana_local_learning_prod_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4aaf",
   "metadata": {},
   "source": [
    "## Подключение библиотек и загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Улучшения: \n",
    "# Убрал .cuda() из класса модели и добавил в даталоадеры перенос на gpu\n",
    "# Добавил трэйсинг моделии и ее сохранение, загрузку модели\n",
    "# Заменил лишний ресайз масок на передачу оригинолов масок\n",
    "# Сделал предсказания с разным размером батча\n",
    "# Нейтрализовал проявления хардкода\n",
    "# Написал комментарии и пояснения\n",
    "# Сделал submission при обучении на всем трерировочном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eadeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# переписать pil на cv2 и сравнить производительность\n",
    "# Реализовать аугментацию через albumintation\n",
    "# Попробовать softdice loss + bce (как в dlcource.ai)\n",
    "# Сделать нормализацию через albumintation (нужны скрипты для определения средего и дисперсии по каждому каналу)\n",
    "# и сравнить точность (параметры можно взять из get_preprocessing_fn из smp если используем их модели)\n",
    "# разобраться, заморожены ли веса энкодера у smp моделей\n",
    "# реализовать модель из https://github.com/lyakaap/Kaggle-Carvana-3rd-Place-Solution/blob/master/model_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15971ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f662c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6386d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполнять, если датасет не загружен\n",
    "!pip install -q kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!cp ~/kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c carvana-image-masking-challenge\n",
    "!unzip ~/carvana-image-masking-challenge.zip ~/carvana_dataset/\n",
    "\n",
    "!unzip ~/carvana_dataset/train.zip -d ~/carvana_dataset/train\n",
    "!unzip ~/carvana_dataset/test.zip -d ~/carvana_dataset/test\n",
    "!unzip ~/carvana_dataset/train_masks.zip -d ~/carvana_dataset/train_masks\n",
    "\n",
    "!unzip ~/carvana_dataset/train_hq.zip -d ~/carvana_dataset/train_hq\n",
    "!unzip ~/carvana_dataset/test_hq.zip -d ~/carvana_dataset/test_hq\n",
    "\n",
    "!unzip ~/carvana_dataset/train_masks.csv.zip  ~/carvana_dataset/\n",
    "!unzip ~/carvana_dataset/sample_submission.csv.zip  ~/carvana_dataset/\n",
    "!unzip ~/carvana_dataset/metadata.csv.zip  ~/carvana_dataset/\n",
    "\n",
    "!rm ~/carvana-image-masking-challenge.zip\n",
    "!rm ~/carvana_dataset/test.zip\n",
    "!rm ~/carvana_dataset/train_masks.zip\n",
    "!rm ~/carvana_dataset/train.zip\n",
    "!rm ~/carvana_dataset/test_hq.zip\n",
    "!rm ~/carvana_dataset/train_hq.zip\n",
    "!rm ~/carvana_dataset/train_masks.csv.zip\n",
    "!rm ~/carvana_dataset/sample_submission.csv.zip\n",
    "!rm ~/carvana_dataset/metadata.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8be7cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3608d7",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ec78c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(imgs_path: str = None, masks_path: str = None) -> pd.DataFrame:\n",
    "    '''Функция получает на вход пути к директориям с изображениями и масками\n",
    "    и генерирует датафрейм, содержащий имя изображений, их адреса и адреса\n",
    "    соответствующих им масок\n",
    "  \n",
    "    Входные параметры:\n",
    "    imgs_path: str - путь к директории с изображениями,\n",
    "    masks_path: str - путь к директории с масками\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data - dataframe, содержащий адреса изображений и соответствующих им масок'''\n",
    "\n",
    "    assert (imgs_path != None) & (masks_path != None)\n",
    "    # imgs_path or masks_path is equal None\n",
    "\n",
    "    data_img = {}\n",
    "    data_mask = {}\n",
    "    data_img['imgs_path'] = []\n",
    "    data_mask['masks_path'] = []\n",
    "    data_img['imgs_path'] = list(glob.glob(imgs_path + \"/*\"))\n",
    "    data_mask['masks_path'] = list(glob.glob(masks_path + \"/*\"))\n",
    "\n",
    "    data_img = pd.DataFrame(data_img)\n",
    "    data_mask = pd.DataFrame(data_mask)\n",
    "\n",
    "    def file_name(x):\n",
    "        return x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    data_img[\"file_name\"] = data_img[\"imgs_path\"].apply(lambda x: file_name(x))\n",
    "    data_mask[\"file_name\"] = data_mask[\"masks_path\"].apply(lambda x: file_name(x)[:-5])\n",
    "\n",
    "    data = pd.merge(data_img, data_mask, on = \"file_name\", how = \"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5613d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(source_df: pd.DataFrame, separate_feature: str = None, test_size: float = 0.25) -> pd.DataFrame:\n",
    "    '''Функция разделяет source_df на две части с коэффициентом test_size\n",
    "    по уникальным значениям separate_feature так, чтобы в новых датафреймах\n",
    "    не было строк с одинаковыми значенияти из separate_feature\n",
    "\n",
    "    Входные параметры:\n",
    "    source_df: pd.DataFrame - датафрейм для разделения на train и test\n",
    "    separate_feature: str - поле, по которому датафрейм будет разделен\n",
    "    test_size: float - коэффициент разделения дтафрейма\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data_train - датафрейм для тренировки\n",
    "    pd.DataFrame: data_valid - датафрейм для валидации'''\n",
    "  \n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_cars, valid_cars = train_test_split(source_df[separate_feature].unique(), test_size=test_size, random_state=42)\n",
    "        data_valid = source_df[np.isin(source_df[separate_feature].values, valid_cars)]\n",
    "        data_train = source_df[np.isin(source_df[separate_feature].values, train_cars)]\n",
    "        assert source_df.shape[0] == (data_valid.shape[0] + data_train.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "    else:\n",
    "        data_train, data_valid = train_test_split(source_df, test_size=test_size)\n",
    "\n",
    "    return data_train, data_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712baa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(logits: torch.Tensor, targets: torch.Tensor, treashold: float) -> float:\n",
    "    '''Функция для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    Входные параметры:\n",
    "    logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "    targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "    treashold: float - порог для определения класса точки в предсказанной точке\n",
    "    Возвращаемые значения:\n",
    "    score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "    \n",
    "    smooth = 1\n",
    "    num = targets.size(0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    outputs = torch.where(probs > treashold, 1, 0)\n",
    "    m1 = outputs.view(num, -1)\n",
    "    m2 = targets.view(num, -1)\n",
    "    intersection = (m1 * m2)\n",
    "\n",
    "    score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "    score = score.sum() / num\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fd108c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "    '''Функция принимает одну маску в тензорном формате, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    tensor: torch.Tensor - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строком виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    tensor = tensor.view(1, -1)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor[0] = 0\n",
    "    tensor[-1] = 0\n",
    "    rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    rle = rle.cpu().detach().numpy()\n",
    "    rle_str = rle_to_string(rle)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dba4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_rle(mask_image: np.ndarray) -> str:\n",
    "    '''Функция принимает одну маску в формате массива numpy, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    mask_image: numpy.ndarray - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    pixels = mask_image.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle_str = rle_to_string(runs)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf01f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_string(runs: torch.Tensor) -> str:\n",
    "    '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "    в строковое представление этой последовательности\n",
    "    Входные параметры:\n",
    "    runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - строковое представление последовательности чисел'''\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd732aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(mask_addr: str) -> str:\n",
    "    '''Функция преобразует маску, имеющую адрес mask_addr и сохраненную в\n",
    "    формате .gif, элементы которой имеют значения 0 и 1 в rle представление\n",
    "    в строковом виде\n",
    "    Входные параметры:\n",
    "    mask_addr: str - адрес маски\n",
    "    Возвращаемые значения:\n",
    "    mask_rle: str - rle представление маски в строком виде\n",
    "    '''\n",
    "    \n",
    "    mask = Image.open(mask_addr).convert('LA') # преобразование в серый\n",
    "    mask = np.asarray(mask).astype('float')[:,:,0]\n",
    "    mask = mask/255.0\n",
    "    mask_rle = numpy_to_rle(mask)\n",
    "    return mask_rle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dd14a",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b539ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceMetric(nn.Module):\n",
    "    '''Класс для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    с заданным порогом для определния класса каждой точки изображения'''\n",
    "    \n",
    "    def __init__(self, treashold: float=0.5):\n",
    "        '''treashold: float - порог для определения класса точки в предсказанной точке'''\n",
    "        super(DiceMetric, self).__init__()\n",
    "        self.treashold = treashold\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "        with torch.no_grad():\n",
    "            smooth = 1\n",
    "            num = targets.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            outputs = torch.where(probs > self.treashold, 1., 0.)\n",
    "            m1 = outputs.view(num, -1)\n",
    "            m2 = targets.view(num, -1)\n",
    "            intersection = (m1 * m2)\n",
    "\n",
    "            score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "            score = score.sum() / num\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e9aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    '''Класс для вычисления DICE loss для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE loss для набора предсказанных масок'''\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4045a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTrain(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str, out_shape: tuple=(512, 512), \n",
    "                                                             skip_mask: bool=False):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        out_shape: tuple - пространственная размерность тензоров, к которой будут приводиться изображения и маски\n",
    "        skip_mask: bool - флаг, нужно ли генерировать исходную маску (без изменения размерности)\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTrain'''\n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Разделяем датафрейм на rgb картинки \n",
    "        self.image_arr = self.data_info.iloc[:,0]\n",
    "        # и на сегментированные картинки\n",
    "        self.mask_arr = self.data_info.iloc[:,2]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Пространственные размеры тензоров на выходе объекта\n",
    "        self.out_shape = out_shape\n",
    "        # Нужно ли пробрасывать маску изображения на выход без изменений\n",
    "        self.skip_mask = skip_mask\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        mask: torch.Tensor - тензорное представление изображения с размерностью out_shape \n",
    "        (возвращается если значение skip_mask равно True)'''\n",
    "        img = np.asarray(Image.open(self.image_arr[index])).astype('float')\n",
    "        img = (torch.as_tensor(img)/255.0).to(self.device)\n",
    "        # unsqueeze - чтобы interpolate работало\n",
    "        # permute - переставляем измерение каналов на 2-е место\n",
    "        img = img.unsqueeze(0).permute(0,3,1,2)\n",
    "        # clamp не позволяет выйти за границы значений\n",
    "        img = F.interpolate(input=img, size=self.out_shape, align_corners=False, mode='bicubic').clamp(min=0, max=1)\n",
    "        img = img.squeeze(0)\n",
    "        # Маски - одноканальные изображения со значениями 0 и 1\n",
    "        mask = Image.open(self.mask_arr[index])\n",
    "        mask = np.asarray(mask).astype('float')\n",
    "        # unsqueeze - добавляем измерение каналов\n",
    "        mask = (torch.as_tensor(mask)).to(self.device).unsqueeze(0) \n",
    "        # unsqueeze - чтобы interpolate работало\n",
    "        mask_small = mask.unsqueeze(0)\n",
    "        mask_small = F.interpolate(input=mask_small, size=self.out_shape, mode='nearest')\n",
    "        mask_small = mask_small.squeeze(0)\n",
    "        \n",
    "        # Если необходима исходная маска, то дополнительно возвращаем ее\n",
    "        if self.skip_mask == True:\n",
    "            return (img.float(), mask_small.float(), mask.float())\n",
    "        else:\n",
    "            return (img.float(), mask_small.float())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15588a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTest(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    def __init__(self, data_info, device: str, out_shape: tuple=(512, 512)):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        out_shape: tuple - пространственная размерность тензоров, к которой будут приводиться изображения\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTest'''\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Получаем адреса RGB изображений \n",
    "        self.image_addresses = self.data_info.iloc[:,0]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Пространственные размеры тензоров на выходе объекта\n",
    "        self.out_shape = out_shape\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        image_name: str - имя изображения'''\n",
    "        img = np.asarray(Image.open(self.image_addresses[index])).astype('float')\n",
    "        img = (torch.as_tensor(img)/255).to(self.device)    \n",
    "        # unsqueeze - чтобы interpolate работало\n",
    "        # permute - переставляем измерение каналов на 2-е место\n",
    "        img = img.unsqueeze(0).permute(0,3,1,2)\n",
    "        # clamp не позволяет выйти за границы значений\n",
    "        img = F.interpolate(input=img, size=self.out_shape, align_corners=False, mode='bicubic').clamp(min=0, max=1)\n",
    "        img = img.squeeze(0)\n",
    "        image_address = self.image_addresses[index]\n",
    "        image_name = image_address.split('/')[-1]\n",
    "    \n",
    "        return (index, img.float(), image_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0a9bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''Класс для создания работы с нейронной сетью для семантической сегментации Carvana'''\n",
    "    def __init__(self, model: nn.Module):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: nn.Module - последовательность слоев или модель, через которую будут проходить данные\n",
    "        Возвращаемые значения: \n",
    "        объект класса NeuralNetwork'''\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        '''Функция прямого прохода через объкт класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения\n",
    "        Возвращаемые значения: \n",
    "        input_data: torch.Tensor - тензорное представление маски изображения'''\n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "        '''Статический метод принимает одну маску в тензорном формате, элементы которой\n",
    "        имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "        Входные параметры:\n",
    "        tensor: torch.Tensor - маска в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "        # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "        # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "        # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "        # не сильно упадет)\n",
    "        with torch.no_grad():\n",
    "            tensor = tensor.view(1, -1)\n",
    "            tensor = tensor.squeeze(0)\n",
    "            tensor[0] = 0\n",
    "            tensor[-1] = 0\n",
    "            rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "            rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "            rle = rle.cpu().detach().numpy()\n",
    "            rle_str = NeuralNetwork.rle_to_string(rle)\n",
    "            return rle_str\n",
    "    \n",
    "    @staticmethod\n",
    "    def rle_to_string(runs: torch.Tensor) -> str:\n",
    "        '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "        в строковое представление этой последовательности\n",
    "        Входные параметры:\n",
    "        runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - строковое представление последовательности чисел'''\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "    \n",
    "    \n",
    "    def fit(self, criterion: object, metric: object, optimizer: object, \n",
    "                  train_data_loader: DataLoader, valid_data_loader: DataLoader=None, epochs: int=1):\n",
    "        '''Метод для обучения объекта класса\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        optimizer: object - оптимизатор\n",
    "        train_data_loader: DataLoader - загрузчик данных для обучения\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        epochs: int - количество эпох обучения\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        result = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss =0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, (data, labels) in enumerate(train_data_loader):\n",
    "                data, labels = Variable(data), Variable(labels)        \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % 300 == 299:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {running_loss/300}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spend time for 300 batches: {time2-time1} sec')\n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)        \n",
    "            \n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                self.model.eval()\n",
    "                valid_metrics = []\n",
    "                valid_losses = []\n",
    "                for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "                    data, labels, labels_small = Variable(data), Variable(labels), Variable(labels_small)\n",
    "                    outputs = self.model(data)\n",
    "                    # loss вычисляется для сжатых масок для правильной валидации (обучались на сжатых)\n",
    "                    # чтобы вовремя определить переобучение\n",
    "                    loss = criterion(outputs, labels_small)\n",
    "                    valid_losses.append(loss.item())\n",
    "                    #Преобразуем выход модели к размеру соответствующей маски\n",
    "                    outputs = F.interpolate(input=outputs, size=(labels.shape[2], labels.shape[3]), mode='nearest')\n",
    "\n",
    "                    # метрика считается для исходных размеров потому что именно так итоговое качество\n",
    "                    # определяется алгоритмом kaggle \n",
    "                    metric_value = metric(outputs, labels)\n",
    "                    valid_metrics.append(metric_value.item())\n",
    "                    \n",
    "                valid_loss    = np.mean(valid_losses)\n",
    "                valid_metric  = np.mean(valid_metrics)\n",
    "                print(f'Epoch {epoch+1}, train loss: {train_loss}, valid_loss: {valid_loss}, valid_metric: {valid_metric}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}, train loss: {train_loss}')\n",
    "                valid_loss = None\n",
    "                valid_metric = None\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        result['epoch_train_losses'] = epoch_train_losses\n",
    "        result['epoch_valid_losses'] = epoch_valid_losses\n",
    "        result['epoch_valid_metrics'] = epoch_valid_metrics\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, test_data_loader: DataLoader, predict_directory: str, output_size: tuple=(1280, 1918), \n",
    "                mask_treashold: float=0.5, generate_rle_dataframe: bool=True) -> pd.DataFrame:\n",
    "        '''Метод для предсказания масок для набора изображения\n",
    "        Входные параметры:\n",
    "        test_data_loader: DataLoader - загрузчик данных для предсказания\n",
    "        predict_directory: str - директория, в которую будут сохраняться сгенерированные маски\n",
    "        output_size: tuple - пространственная размерность выходных масок\n",
    "        mask_treashold: float - порог, по которому будет определяться класс каждой точки для масок\n",
    "        generate_rle_dataframe: bool - флаг, нужна ли генерация rle представлений масок\n",
    "        Возвращаемые значения:\n",
    "        rle_dataframe: pd.DataFrame - датафрейм с rle представлениями для масок (если \n",
    "        generate_rle_dataframe==True)\n",
    "        Маски в формате .gif для изображений с соответствующими именами, находятся в директории predict_directory'''\n",
    "        self.model.eval()\n",
    "        img_names = []\n",
    "        img_rles = []\n",
    "        \n",
    "        for batch_idx, (index, img, img_name)  in enumerate(test_data_loader):\n",
    "\n",
    "            img = Variable(img)        \n",
    "            pred_mask_logit = self.model(img)\n",
    "            pred_mask_logit = F.interpolate(input=pred_mask_logit, size=output_size, mode='nearest')\n",
    "            pred_mask_logit_prob = torch.sigmoid(pred_mask_logit)\n",
    "            pred_mask = torch.where(pred_mask_logit_prob > mask_treashold, 1, 0)\n",
    "            \n",
    "            # Каждое изображение в тензоре преобразуем в картинку и сохраняем\n",
    "            for i in range(pred_mask.shape[0]):\n",
    "                mask = (pred_mask[i].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "                PIL_image = Image.fromarray(mask.astype('uint8'), 'L')\n",
    "                PIL_image.save((predict_directory+img_name[i]).split('.')[0]+'.gif')\n",
    "                \n",
    "                # Если требуется, получаем значения rle для каждой картинки\n",
    "                if generate_rle_dataframe == True:\n",
    "                    img_names.append(img_name[i])\n",
    "                    img_rles.append(NeuralNetwork.tensor_to_rle(pred_mask[i]))\n",
    "                \n",
    "        if generate_rle_dataframe == True:\n",
    "            rle_dataframe = pd.DataFrame(list(zip(img_names, img_rles)), columns =['img_name', 'img_rle'])\n",
    "            return rle_dataframe\n",
    "    \n",
    "    def save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения весов модели\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения состояния модели'''\n",
    "        torch.save(self.model.state_dict(), path_to_save)\n",
    "    \n",
    "    def trace_save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения модели через torchscript\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.rand(1, 3, 512, 512).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "            \n",
    "        traced_model = torch.jit.trace(self.model, example_forward_input)\n",
    "        torch.jit.save(traced_model, 'model.pt')\n",
    "    \n",
    "    def load(self, path_to_model: str='./model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e07ce",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21937687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/dima/carvana_dataset'\n",
    "imgs_path  = dataset_path + '/train/train'\n",
    "masks_path = dataset_path + '/train_masks/train_masks'\n",
    "\n",
    "nn_image_shape = (512, 512)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "mask_treashold = 0.5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d28b75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e54a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение с валидацией\n",
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_data = CustomDatasetForTrain(train_df, device, out_shape=nn_image_shape)\n",
    "valid_data = CustomDatasetForTrain(valid_df, device, out_shape=nn_image_shape, skip_mask=True)\n",
    "\n",
    "train_data_loader = DataLoader(train_data,batch_size=4, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data,batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94c06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение без валидации\n",
    "train_data = CustomDatasetForTrain(data, device, out_shape=nn_image_shape)\n",
    "train_data_loader = DataLoader(train_data,batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca4b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель на основе предложенной архитектуры\n",
    "model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "                 encoder_weights='imagenet', decoder_channels = [256, 128, 64, 32, 16]).to(device)\n",
    "\n",
    "#model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "#                 encoder_weights='imagenet').to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fb86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SoftDiceLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "metric = DiceMetric(treashold=mask_treashold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca111f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.0499671345949173\n",
      "Spend time for 300 batches: 103.78062438964844 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8631/1744615802.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m my_model.fit(criterion,\n\u001b[0m\u001b[1;32m      2\u001b[0m              \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8631/2808794667.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, criterion, metric, optimizer, train_data_loader, valid_data_loader, epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8631/1013084175.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mтензорное\u001b[0m \u001b[0mпредставление\u001b[0m \u001b[0mизображения\u001b[0m \u001b[0mс\u001b[0m \u001b[0mразмерностью\u001b[0m \u001b[0mout_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         (возвращается если значение skip_mask равно True)'''\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# unsqueeze - чтобы interpolate работало\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mclass\u001b[0m \u001b[0mArrayData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_model.fit(criterion,\n",
    "             metric,\n",
    "             optimizer,\n",
    "             train_data_loader,\n",
    "             valid_data_loader,\n",
    "             epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "e2d15ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем веса обученной модели\n",
    "my_model.save(path_to_save = './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "9e78f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем оттрассированную модель\n",
    "my_model.trace_save(path_to_save = './model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700bfa",
   "metadata": {},
   "source": [
    "## Загрузка сохраненной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a384ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводим модель по известной архитектуре и сохраненным весам\n",
    "model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "                 encoder_weights='imagenet', decoder_channels = [256, 128, 64, 32, 16]).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "9fb8d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем оттрассированную модель\n",
    "my_model = torch.jit.load('./model.pt')\n",
    "my_model = NeuralNetwork(model=my_model)\n",
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988cd6",
   "metadata": {},
   "source": [
    "## Предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a31d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "test_dataset = '/home/dima/carvana_dataset/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6cc16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}\n",
    "test_dataframe['img_addr'] = list(glob.glob(test_dataset + \"/*\"))\n",
    "test_dataframe = pd.DataFrame(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b8aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDatasetForTest(test_dataframe)\n",
    "test_data_loader = DataLoader(test_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51507db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ad9e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем датафрейм с результатом для заливки на kaggle\n",
    "rle_dataframe.to_csv('rle_dataframe.csv', index=True)\n",
    "sample_submission = pd.read_csv('/home/dima/carvana_dataset/sample_submission.csv')\n",
    "sample_submission = sample_submission.merge(rle_dataframe, how='left', left_on='img', right_on='img_name')\n",
    "sample_submission.drop(columns=['rle_mask', 'img_name'], inplace=True)\n",
    "sample_submission.rename(columns={'img_rle': 'rle_mask'}, inplace=True)\n",
    "sample_submission.to_csv('submission_02_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81effd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde352f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
