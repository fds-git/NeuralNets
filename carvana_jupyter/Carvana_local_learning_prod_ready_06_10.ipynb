{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b5928f",
   "metadata": {},
   "source": [
    "# Improvement Carvana_local_learning_prod_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4aaf",
   "metadata": {},
   "source": [
    "## Подключение библиотек и загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Улучшения: \n",
    "# Убрал .cuda() из класса модели и добавил в даталоадеры перенос на gpu\n",
    "# Добавил трэйсинг моделии и ее сохранение, загрузку модели\n",
    "# Заменил лишний ресайз масок на передачу оригинолов масок\n",
    "# Сделал предсказания с разным размером батча\n",
    "# Нейтрализовал проявления хардкода\n",
    "# Написал комментарии и пояснения\n",
    "# Сделал submission при обучении на всем трерировочном датасете\n",
    "# переписал pil на cv2 где это можно - скорость увеличилась\n",
    "# Реализовал ресайз через albumintation - скорость увеличилась\n",
    "# Сделал нормализацию через albumintation улучшений нет, только время увеличилось\n",
    "# Проверил, что при нормализации маска не изменяется\n",
    "# Изменил формат вывода при обучении и предсказании\n",
    "# сделал predict на torchscript - скорость не выросла\n",
    "# заменил при валидации и в predict интерполяцию масок с nearest на bilinear\n",
    "# с onnx получилось медленне, видимо нужно отдельно устанавливать cuda и cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eadeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эксперименты с unet(mobilenet), unet(efficientnetb5), deeplabv3plus(mobilenet), deeplabv3plus(efficientnetb5)\n",
    "# в transform попробовать другое кроме cv2.INTER_LINEAR и сравнить точность\n",
    "# попробовать в predict не nearest для масок и сравнить точность (написать код и посмотреть как маска меняется\n",
    "# при интерполяции nearest, linear, bilinear и т.д. и раcсчитать ошибки)\n",
    "# Попробовать softdice loss + bce (как в dlcource.ai)\n",
    "# разобраться, заморожены ли веса энкодера у smp моделей\n",
    "# реализовать модель из https://github.com/lyakaap/Kaggle-Carvana-3rd-Place-Solution/blob/master/model_pytorch.py\n",
    "# наиболее предпочтительным методом интерполяции является cv.INTER_AREA - попробовать при аугментации\n",
    "# Взять deeplab обученный из torchvision.models.segmentation.segmentation и протестировать вместо предыдущих моделей\n",
    "# с интерполяцией и на полном изображении - сравнить точность с 30 - эпоховой моделью. Быть может, уперлись в точность\n",
    "# именно из-за интерполяции\n",
    "# сделать predict 30 - эпоховой модели при linear и bilinear интерполяции, сравнить точность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15971ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f662c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.onnx\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c6386d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Выполнять, если датасет не загружен\n",
    "#!pip install -q kaggle\n",
    "#!mkdir ~/.kaggle\n",
    "#!cp ~/kaggle.json ~/.kaggle/\n",
    "#!chmod 600 ~/.kaggle/kaggle.json\n",
    "#!kaggle competitions download -c carvana-image-masking-challenge\n",
    "#!unzip ~/carvana-image-masking-challenge.zip ~/carvana_dataset/\n",
    "\n",
    "#!unzip ~/carvana_dataset/train.zip -d ~/carvana_dataset/train\n",
    "#!unzip ~/carvana_dataset/test.zip -d ~/carvana_dataset/test\n",
    "#!unzip ~/carvana_dataset/train_masks.zip -d ~/carvana_dataset/train_masks\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_hq.zip -d ~/carvana_dataset/train_hq\n",
    "#!unzip ~/carvana_dataset/test_hq.zip -d ~/carvana_dataset/test_hq\n",
    "\n",
    "#!unzip ~/carvana_dataset/train_masks.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/sample_submission.csv.zip  ~/carvana_dataset/\n",
    "#!unzip ~/carvana_dataset/metadata.csv.zip  ~/carvana_dataset/\n",
    "\n",
    "#!rm ~/carvana-image-masking-challenge.zip\n",
    "#!rm ~/carvana_dataset/test.zip\n",
    "#!rm ~/carvana_dataset/train_masks.zip\n",
    "#!rm ~/carvana_dataset/train.zip\n",
    "#!rm ~/carvana_dataset/test_hq.zip\n",
    "#!rm ~/carvana_dataset/train_hq.zip\n",
    "#!rm ~/carvana_dataset/train_masks.csv.zip\n",
    "#!rm ~/carvana_dataset/sample_submission.csv.zip\n",
    "#!rm ~/carvana_dataset/metadata.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8be7cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4005c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct  7 15:03:51 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   51C    P8    14W / 170W |    363MiB / 12045MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       958      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1653      G   /usr/lib/xorg/Xorg                142MiB |\r\n",
      "|    0   N/A  N/A      1783      G   /usr/bin/gnome-shell               44MiB |\r\n",
      "|    0   N/A  N/A      8102      G   /usr/bin/nvidia-settings            0MiB |\r\n",
      "|    0   N/A  N/A     15067      G   /usr/lib/firefox/firefox          124MiB |\r\n",
      "|    0   N/A  N/A     15188      G   /usr/lib/firefox/firefox            2MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3608d7",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec78c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv(imgs_path: str = None, masks_path: str = None) -> pd.DataFrame:\n",
    "    '''Функция получает на вход пути к директориям с изображениями и масками\n",
    "    и генерирует датафрейм, содержащий имя изображений, их адреса и адреса\n",
    "    соответствующих им масок\n",
    "  \n",
    "    Входные параметры:\n",
    "    imgs_path: str - путь к директории с изображениями,\n",
    "    masks_path: str - путь к директории с масками\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data - dataframe, содержащий адреса изображений и соответствующих им масок'''\n",
    "\n",
    "    assert (imgs_path != None) & (masks_path != None)\n",
    "    # imgs_path or masks_path is equal None\n",
    "\n",
    "    data_img = {}\n",
    "    data_mask = {}\n",
    "    data_img['imgs_path'] = []\n",
    "    data_mask['masks_path'] = []\n",
    "    data_img['imgs_path'] = list(glob.glob(imgs_path + \"/*\"))\n",
    "    data_mask['masks_path'] = list(glob.glob(masks_path + \"/*\"))\n",
    "\n",
    "    data_img = pd.DataFrame(data_img)\n",
    "    data_mask = pd.DataFrame(data_mask)\n",
    "\n",
    "    def file_name(x):\n",
    "        return x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    data_img[\"file_name\"] = data_img[\"imgs_path\"].apply(lambda x: file_name(x))\n",
    "    data_mask[\"file_name\"] = data_mask[\"masks_path\"].apply(lambda x: file_name(x)[:-5])\n",
    "\n",
    "    data = pd.merge(data_img, data_mask, on = \"file_name\", how = \"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5613d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(source_df: pd.DataFrame, separate_feature: str = None, test_size: float = 0.25) -> pd.DataFrame:\n",
    "    '''Функция разделяет source_df на две части с коэффициентом test_size\n",
    "    по уникальным значениям separate_feature так, чтобы в новых датафреймах\n",
    "    не было строк с одинаковыми значенияти из separate_feature\n",
    "\n",
    "    Входные параметры:\n",
    "    source_df: pd.DataFrame - датафрейм для разделения на train и test\n",
    "    separate_feature: str - поле, по которому датафрейм будет разделен\n",
    "    test_size: float - коэффициент разделения дтафрейма\n",
    "    Возвращаемые значения:\n",
    "    pd.DataFrame: data_train - датафрейм для тренировки\n",
    "    pd.DataFrame: data_valid - датафрейм для валидации'''\n",
    "  \n",
    "    if (separate_feature != None) & (separate_feature in source_df.columns):\n",
    "        train_cars, valid_cars = train_test_split(source_df[separate_feature].unique(), test_size=test_size, random_state=42)\n",
    "        data_valid = source_df[np.isin(source_df[separate_feature].values, valid_cars)]\n",
    "        data_train = source_df[np.isin(source_df[separate_feature].values, train_cars)]\n",
    "        assert source_df.shape[0] == (data_valid.shape[0] + data_train.shape[0])\n",
    "        assert np.isin(data_train[separate_feature].values, data_valid[separate_feature].values).sum() == 0\n",
    "    else:\n",
    "        data_train, data_valid = train_test_split(source_df, test_size=test_size)\n",
    "\n",
    "    return data_train, data_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712baa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(logits: torch.Tensor, targets: torch.Tensor, treashold: float) -> float:\n",
    "    '''Функция для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    Входные параметры:\n",
    "    logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "    targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "    treashold: float - порог для определения класса точки в предсказанной точке\n",
    "    Возвращаемые значения:\n",
    "    score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "    \n",
    "    smooth = 1\n",
    "    num = targets.size(0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    outputs = torch.where(probs > treashold, 1, 0)\n",
    "    m1 = outputs.view(num, -1)\n",
    "    m2 = targets.view(num, -1)\n",
    "    intersection = (m1 * m2)\n",
    "\n",
    "    score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "    score = score.sum() / num\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd108c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "    '''Функция принимает одну маску в тензорном формате, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    tensor: torch.Tensor - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строком виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    tensor = tensor.view(1, -1)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor[0] = 0\n",
    "    tensor[-1] = 0\n",
    "    rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    rle = rle.cpu().detach().numpy()\n",
    "    rle_str = rle_to_string(rle)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dba4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_rle(mask_image: np.ndarray) -> str:\n",
    "    '''Функция принимает одну маску в формате массива numpy, элементы которой\n",
    "    имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "    Входные параметры:\n",
    "    mask_image: numpy.ndarray - маска в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "    # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "    # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "    # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "    # не сильно упадет)\n",
    "    pixels = mask_image.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle_str = rle_to_string(runs)\n",
    "    return rle_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf01f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_string(runs: torch.Tensor) -> str:\n",
    "    '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "    в строковое представление этой последовательности\n",
    "    Входные параметры:\n",
    "    runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "    Возвращаемые значения:\n",
    "    rle_str: str - строковое представление последовательности чисел'''\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd732aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(mask_addr: str) -> str:\n",
    "    '''Функция преобразует маску, имеющую адрес mask_addr и сохраненную в\n",
    "    формате .gif, элементы которой имеют значения 0 и 1 в rle представление\n",
    "    в строковом виде\n",
    "    Входные параметры:\n",
    "    mask_addr: str - адрес маски\n",
    "    Возвращаемые значения:\n",
    "    mask_rle: str - rle представление маски в строком виде\n",
    "    '''\n",
    "    \n",
    "    mask = Image.open(mask_addr).convert('LA') # преобразование в серый\n",
    "    mask = np.asarray(mask).astype('float')[:,:,0]\n",
    "    mask = mask/255.0\n",
    "    mask_rle = numpy_to_rle(mask)\n",
    "    return mask_rle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dd14a",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b539ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceMetric(nn.Module):\n",
    "    '''Класс для вычисления DICE коэффициента для набора изображенй в формате torch.Tensor\n",
    "    с заданным порогом для определния класса каждой точки изображения'''\n",
    "    \n",
    "    def __init__(self, treashold: float=0.5):\n",
    "        '''treashold: float - порог для определения класса точки в предсказанной точке'''\n",
    "        super(DiceMetric, self).__init__()\n",
    "        self.treashold = treashold\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE коэффициента для набора предсказанных масок'''\n",
    "        with torch.no_grad():\n",
    "            smooth = 1\n",
    "            num = targets.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            outputs = torch.where(probs > self.treashold, 1., 0.)\n",
    "            m1 = outputs.view(num, -1)\n",
    "            m2 = targets.view(num, -1)\n",
    "            intersection = (m1 * m2)\n",
    "\n",
    "            score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "            score = score.sum() / num\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e9aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    '''Класс для вычисления DICE loss для набора изображенй в формате torch.Tensor'''\n",
    "    def __init__(self):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        '''Входные параметры:\n",
    "        logits: torch.Tensor - тензор из предсказанных масок в logit масштабе\n",
    "        targets: torch.Tensor - тензор из целевых целевых значений масок\n",
    "        Возвращаемые значения:\n",
    "        score: float - значение DICE loss для набора предсказанных масок'''\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4045a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTrain(Dataset):\n",
    "    '''Класс для создания тренировочных и валидационных датасетов'''\n",
    "    def __init__(self, data_info: pd.DataFrame, device: str, transform: object, skip_mask: bool=False):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами изображений и масок\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения и маски\n",
    "        skip_mask: bool - флаг, нужно ли генерировать исходную маску (без изменения размерности)\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTrain'''\n",
    "        # Подаем подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Разделяем датафрейм на rgb картинки \n",
    "        self.image_arr = self.data_info.iloc[:,0]\n",
    "        # и на сегментированные картинки\n",
    "        self.mask_arr = self.data_info.iloc[:,2]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Нужно ли пробрасывать маску изображения на выход без изменений\n",
    "        self.skip_mask = skip_mask\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        mask: torch.Tensor - тензорное представление изображения с размерностью out_shape \n",
    "        (возвращается если значение skip_mask равно True)'''\n",
    "        image = cv2.imread(self.image_arr[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        # gif не открывается через open cv, поэтому используем для чтения PIL Image\n",
    "        mask = Image.open(self.mask_arr[index])\n",
    "        mask = np.asarray(mask)#.astype('float')\n",
    "        \n",
    "        transformed = self.transform(image=image, mask=mask)\n",
    "        tr_image = transformed['image']\n",
    "        tr_mask = transformed['mask']\n",
    "        \n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        tr_mask = tr_mask.to(self.device).float().unsqueeze(0)\n",
    "\n",
    "        \n",
    "        # Если необходима исходная маска, то дополнительно возвращаем ее\n",
    "        if self.skip_mask == True:\n",
    "            mask = (torch.as_tensor(mask)).to(self.device).float().unsqueeze(0)\n",
    "            return (tr_image, tr_mask, mask)\n",
    "        else:\n",
    "            return (tr_image, tr_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15588a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForTest(Dataset):\n",
    "    '''Класс для создания тестовых датасетов'''\n",
    "    def __init__(self, data_info, device: str, transform: object):\n",
    "        '''Входные параметры:\n",
    "        data_info: pd.DataFrame - датафрейм с адресами и именами изображений\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список трансформации, которым будут подвергнуты изображения\n",
    "        Возвращаемые значения:\n",
    "        объект класса CustomDatasetForTest'''\n",
    "        # Подаем наш подготовленный датафрейм\n",
    "        self.data_info = data_info\n",
    "        # Получаем адреса RGB изображений \n",
    "        self.image_addresses = self.data_info.iloc[:,0]\n",
    "        # Получаем имена RGB изображений \n",
    "        self.image_names = self.data_info.iloc[:,1]\n",
    "        # Количество пар картинка-сегментация\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Устройство, на котором будут находиться выходные тензоры\n",
    "        self.device = device\n",
    "        # Сохраняем преобразования данных\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Входные параметры:\n",
    "        img: int - индекс для обращения к элементам датафрейма data_info\n",
    "        Возвращаемые значения:\n",
    "        img: torch.Tensor - тензорное представление изображения с размерностью out_shape\n",
    "        mask_small: torch.Tensor - тензорное представление маски с исходной размерностью\n",
    "        image_name: str - имя изображения'''\n",
    "        image = cv2.imread(self.image_addresses[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "        \n",
    "        transformed = self.transform(image=image)\n",
    "        tr_image = transformed['image']\n",
    "        tr_image = tr_image.to(self.device).float()\n",
    "        image_name = self.image_names[index]\n",
    "    \n",
    "        return (index, tr_image, image_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a9bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''Класс для создания работы с нейронной сетью для семантической сегментации Carvana'''\n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: nn.Module - последовательность слоев или модель, через которую будут проходить данные\n",
    "        Возвращаемые значения: \n",
    "        объект класса NeuralNetwork'''\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        '''Функция прямого прохода через объкт класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения\n",
    "        Возвращаемые значения: \n",
    "        input_data: torch.Tensor - тензорное представление маски изображения'''\n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def tensor_to_rle(tensor: torch.Tensor) -> str:\n",
    "        '''Статический метод принимает одну маску в тензорном формате, элементы которой\n",
    "        имеют значения 0. и 1. и генерирует rle представление маски в строковом формате\n",
    "        Входные параметры:\n",
    "        tensor: torch.Tensor - маска в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - rle представление маски в строковом виде'''\n",
    "    \n",
    "        # Для правильной работы алгоритма необходимо, чтобы первое и последнее значения выпрямленной маски\n",
    "        # (что соответствует двум углам изображения) были равны 0. Это не должно повлиять на качество работы\n",
    "        # алгоритма, так как мы не ожидаем наличие объекта в этих точках (но даже если он там будет, качество\n",
    "        # не сильно упадет)\n",
    "        with torch.no_grad():\n",
    "            tensor = tensor.view(1, -1)\n",
    "            tensor = tensor.squeeze(0)\n",
    "            tensor[0] = 0\n",
    "            tensor[-1] = 0\n",
    "            rle = torch.where(tensor[1:] != tensor[:-1])[0] + 2\n",
    "            rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "            rle = rle.cpu().detach().numpy()\n",
    "            rle_str = NeuralNetwork.rle_to_string(rle)\n",
    "            return rle_str\n",
    "    \n",
    "    @staticmethod\n",
    "    def rle_to_string(runs: torch.Tensor) -> str:\n",
    "        '''Функция преобразует последовательноть чисел в тензоре runs\n",
    "        в строковое представление этой последовательности\n",
    "        Входные параметры:\n",
    "        runs: torch.Tensor - последовательность чисел в тензорном формате\n",
    "        Возвращаемые значения:\n",
    "        rle_str: str - строковое представление последовательности чисел'''\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "    \n",
    "    \n",
    "    def fit(self, criterion: object, metric: object, optimizer: object, \n",
    "                  train_data_loader: DataLoader, valid_data_loader: DataLoader=None, epochs: int=1):\n",
    "        '''Метод для обучения объекта класса\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        optimizer: object - оптимизатор\n",
    "        train_data_loader: DataLoader - загрузчик данных для обучения\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        epochs: int - количество эпох обучения\n",
    "        \n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss при тренировке, валидации и метрики при валидации \n",
    "        для каждой эпохи'''\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        epoch_train_losses = []\n",
    "        epoch_valid_losses = []\n",
    "        epoch_valid_metrics = []\n",
    "        result = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            time1 = time.time()\n",
    "            running_loss =0.0\n",
    "            train_losses = []\n",
    "            for batch_idx, (data, labels) in enumerate(train_data_loader):\n",
    "                data, labels = Variable(data), Variable(labels)        \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                if (batch_idx+1) % 300 == 0:\n",
    "                    print(f'Train Epoch: {epoch+1}, Loss: {(running_loss/300):.6f}')\n",
    "                    time2 = time.time()\n",
    "                    print(f'Spend time for {300*data.shape[0]} images: {(time2-time1):.6f} sec')\n",
    "                    time1 = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            train_loss = np.mean(train_losses)        \n",
    "            \n",
    "            \n",
    "            if valid_data_loader != None:\n",
    "                self.model.eval()\n",
    "                valid_metrics = []\n",
    "                valid_losses = []\n",
    "                for batch_idx, (data, labels_small, labels) in enumerate(valid_data_loader):\n",
    "                    data, labels, labels_small = Variable(data), Variable(labels), Variable(labels_small)\n",
    "                    outputs = self.model(data)\n",
    "                    # loss вычисляется для сжатых масок для правильной валидации (обучались на сжатых)\n",
    "                    # чтобы вовремя определить переобучение\n",
    "                    loss = criterion(outputs, labels_small)\n",
    "                    valid_losses.append(loss.item())\n",
    "                    #Преобразуем выход модели к размеру соответствующей маски\n",
    "                    outputs = F.interpolate(input=outputs, size=(labels.shape[2], labels.shape[3]), mode='bilinear')\n",
    "\n",
    "                    # метрика считается для исходных размеров потому что именно так итоговое качество\n",
    "                    # определяется алгоритмом kaggle \n",
    "                    metric_value = metric(outputs, labels)\n",
    "                    valid_metrics.append(metric_value.item())\n",
    "                    \n",
    "                valid_loss    = np.mean(valid_losses)\n",
    "                valid_metric  = np.mean(valid_metrics)\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}, valid_loss: {(valid_loss):.6f}, valid_metric: {(valid_metric):.6f}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}, train loss: {(train_loss):.6f}')\n",
    "                valid_loss = None\n",
    "                valid_metric = None\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_valid_losses.append(valid_loss)\n",
    "            epoch_valid_metrics.append(valid_metric)\n",
    "        \n",
    "        result['epoch_train_losses'] = epoch_train_losses\n",
    "        result['epoch_valid_losses'] = epoch_valid_losses\n",
    "        result['epoch_valid_metrics'] = epoch_valid_metrics\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, test_data_loader: DataLoader, predict_directory: str=None, output_size: tuple=(1280, 1918), \n",
    "                mask_treashold: float=0.5, generate_rle_dataframe: bool=True) -> pd.DataFrame:\n",
    "        '''Метод для предсказания масок для набора изображения\n",
    "        Входные параметры:\n",
    "        test_data_loader: DataLoader - загрузчик данных для предсказания\n",
    "        predict_directory: str - директория, в которую будут сохраняться сгенерированные маски (если None,\n",
    "        то маски сохраняться не будут)\n",
    "        output_size: tuple - пространственная размерность выходных масок\n",
    "        mask_treashold: float - порог, по которому будет определяться класс каждой точки для масок\n",
    "        generate_rle_dataframe: bool - флаг, нужна ли генерация rle представлений масок\n",
    "        Возвращаемые значения:\n",
    "        rle_dataframe: pd.DataFrame - датафрейм с rle представлениями для масок (если \n",
    "        generate_rle_dataframe==True)\n",
    "        Маски в формате .gif для изображений с соответствующими именами, находятся в директории predict_directory'''\n",
    "        self.model.eval()\n",
    "        img_names = []\n",
    "        img_rles = []\n",
    "        time1 = time.time()\n",
    "        time2 = time.time()\n",
    "        for batch_idx, (index, img, img_name)  in enumerate(test_data_loader):\n",
    "\n",
    "            img = Variable(img)        \n",
    "            pred_mask_logit = self.model(img)\n",
    "            pred_mask_logit = F.interpolate(input=pred_mask_logit, size=output_size, mode='bilinear')\n",
    "            pred_mask_logit_prob = torch.sigmoid(pred_mask_logit)\n",
    "            pred_mask = torch.where(pred_mask_logit_prob > mask_treashold, 1, 0)\n",
    "            \n",
    "            # Каждое изображение в тензоре преобразуем в картинку и сохраняем\n",
    "            for i in range(pred_mask.shape[0]):\n",
    "                if predict_directory != None:\n",
    "                    mask = (pred_mask[i].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "                    PIL_image = Image.fromarray(mask.astype('uint8'), 'L')\n",
    "                    PIL_image.save((predict_directory+img_name[i]).split('.')[0]+'.gif')\n",
    "                \n",
    "                # Если требуется, получаем значения rle для каждой картинки\n",
    "                if generate_rle_dataframe == True:\n",
    "                    img_names.append(img_name[i])\n",
    "                    img_rles.append(NeuralNetwork.tensor_to_rle(pred_mask[i]))\n",
    "            \n",
    "            if (batch_idx+1) % 300 == 0:\n",
    "                    print('-'*50)\n",
    "                    print(f'Processed images: {(batch_idx+1)*img.shape[0]}')\n",
    "                    time3 = time.time()\n",
    "                    print(f'Total time: {(time3-time1):.2f} sec')\n",
    "                    print(f'Time to process {300*img.shape[0]} images: {(time3-time2):.2f} sec')\n",
    "                    time2 = time.time()\n",
    "                \n",
    "        if generate_rle_dataframe == True:\n",
    "            rle_dataframe = pd.DataFrame(list(zip(img_names, img_rles)), columns =['img_name', 'img_rle'])\n",
    "            return rle_dataframe\n",
    "    \n",
    "    def save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения весов модели\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения состояния модели'''\n",
    "        torch.save(self.model.state_dict(), path_to_save)\n",
    "    \n",
    "    def trace_save(self, path_to_save: str='./model.pth'):\n",
    "        '''Метод сохранения модели через torchscript\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.rand(1, 3, 512, 512).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "            \n",
    "        traced_model = torch.jit.trace((self.model).eval(), example_forward_input)\n",
    "        torch.jit.save(traced_model, path_to_save)\n",
    "    \n",
    "    def onnx_save(self, path_to_save: str='./carvana_model.onnx'):\n",
    "        '''Метод сохранения модели в формате ONNX\n",
    "        Входные параметры:\n",
    "        path_to_save: str - директория для сохранения модели'''\n",
    "        example_forward_input = torch.randn(1, 3, 1024, 1024, requires_grad=True).to('cpu')\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            example_forward_input= example_forward_input.to('cuda:0')\n",
    "\n",
    "        torch.onnx.export(self.model,\n",
    "                          example_forward_input,\n",
    "                          path_to_save,\n",
    "                          export_params=True,\n",
    "                          opset_version=10,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'],\n",
    "                          dynamic_axes={'input' : {0 : 'batch_size'},    # Модель будет работать с произвольным\n",
    "                                        'output' : {0 : 'batch_size'}})  # размером батча\n",
    "    \n",
    "    def load(self, path_to_model: str='./model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e07ce",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21937687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/dima/carvana_dataset'\n",
    "imgs_path  = dataset_path + '/train/train'\n",
    "masks_path = dataset_path + '/train_masks/train_masks'\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "mask_treashold = 0.5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d58730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function preprocess_input at 0x7f17a12ca790>, input_space='RGB', input_range=[0, 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "preprocess_input = get_preprocessing_fn('mobilenet_v2', pretrained='imagenet')\n",
    "preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "888ec35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_LINEAR), # для масок автоматически будет применяться своя интерполяция, \n",
    "                                          # поэтому на выходе значения маски останутся 0 и 1\n",
    "    #С нормализацией хуже почему то\n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(1024, 1024, cv2.INTER_LINEAR),\n",
    "    #С нормализацией хуже почему то\n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=1.0), # согласно imagenet\n",
    "    #A.Normalize(mean=(0.696, 0.689, 0.684), std=(0.239, 0.243, 0.240), max_pixel_value=1.0), # согласно carvana\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28b75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_csv(imgs_path=imgs_path, masks_path=masks_path)\n",
    "    \n",
    "# Добавляем признак, по которому будем разбивать датасет на train и test,\n",
    "# чтобы не было разных фотографий одной и той же машины в двух датасетах\n",
    "data[\"car\"] = data[\"file_name\"].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e54a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение с валидацией\n",
    "train_df, valid_df = get_train_test(data, separate_feature='car', test_size=0.25)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "valid_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_data = CustomDatasetForTrain(train_df, device, train_transform, skip_mask=False)\n",
    "valid_data = CustomDatasetForTrain(valid_df, device, valid_transform, skip_mask=True)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94c06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение без валидации\n",
    "train_data = CustomDatasetForTrain(data, device, train_transform, skip_mask=False)\n",
    "train_data_loader = DataLoader(train_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ca4b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель на основе предложенной архитектуры\n",
    "model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "                 encoder_weights='imagenet', decoder_channels = [256, 128, 64, 32, 16]).to(device)\n",
    "\n",
    "#model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "#                 encoder_weights='imagenet').to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ccff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#efficientnet-b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccfc2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(encoder_name='mobilenet_v2', encoder_depth=5, encoder_weights='imagenet', \n",
    "                          encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), \n",
    "                          in_channels=3, classes=1, activation=None, upsampling=4, aux_params=None).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "#summary(model, input_size=(1, 3, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SoftDiceLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "metric = DiceMetric(treashold=mask_treashold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca111f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16032/3846411862.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = my_model.fit(criterion,\n\u001b[0m\u001b[1;32m      2\u001b[0m              \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16032/1186113508.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, criterion, metric, optimizer, train_data_loader, valid_data_loader, epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1105\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1105\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/segmentation_models_pytorch/encoders/mobilenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/segmentation_models_pytorch/encoders/mobilenet.py\u001b[0m in \u001b[0;36mget_stages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sequential'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36madd_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         r\"\"\"Adds a child module to the current module.\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = my_model.fit(criterion,\n",
    "             metric,\n",
    "             optimizer,\n",
    "             train_data_loader,\n",
    "             valid_data_loader,\n",
    "             epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2d15ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем веса обученной модели\n",
    "my_model.save(path_to_save = './model_lab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e78f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем оттрассированную модель\n",
    "my_model.trace_save(path_to_save = './model_lab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9bc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/lib/python3.8/site-packages/torch/onnx/utils.py:88: UserWarning: `enable_onnx_checker' is deprecated and ignored. It will be removed inthe next PyTorch release. To proceed despite ONNX checker failures, youcan catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"`enable_onnx_checker' is deprecated and ignored. It will be removed in\"\n",
      "/home/dima/anaconda3/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:381: UserWarning: You are trying to export the model with onnx:Resize for ONNX opset version 10. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator.\n",
      "  warnings.warn(\"You are trying to export the model with \" + onnx_op + \" for ONNX opset version \"\n"
     ]
    }
   ],
   "source": [
    "# Экспорт модели в onnx\n",
    "my_model.onnx_save(path_to_save = './carvana_model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700bfa",
   "metadata": {},
   "source": [
    "## Загрузка сохраненной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6a384ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводим модель по известной архитектуре и сохраненным весам\n",
    "model = smp.Unet('mobilenet_v2', classes=1, encoder_depth=5, \n",
    "                 encoder_weights='imagenet', decoder_channels = [256, 128, 64, 32, 16]).to(device)\n",
    "\n",
    "my_model = NeuralNetwork(model=model)\n",
    "my_model.load(path_to_model = './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb8d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем оттрассированную модель\n",
    "my_model = torch.jit.load('./model.pt')\n",
    "my_model = NeuralNetwork(model=my_model)\n",
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988cd6",
   "metadata": {},
   "source": [
    "## Предсказание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a31d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'\n",
    "test_dataset = '/home/dima/carvana_dataset/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6cc16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}\n",
    "test_dataframe['img_addr'] = list(glob.glob(test_dataset + \"/*\"))\n",
    "test_dataframe = pd.DataFrame(test_dataframe)\n",
    "test_dataframe['img_name'] = test_dataframe['img_addr'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b8aa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDatasetForTest(test_dataframe, device, valid_transform)\n",
    "test_data_loader = DataLoader(test_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51507db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15565/2166412596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# С сохранением сгенерированных масок в predict_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                  mask_treashold=mask_treashold, generate_rle_dataframe=True)\n",
      "\u001b[0;32m/tmp/ipykernel_15565/953016608.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_data_loader, predict_directory, output_size, mask_treashold, generate_rle_dataframe)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpredict_directory\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# [0] - избавляемся от батч размерности\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                     \u001b[0mPIL_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mPIL_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_directory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.gif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# С сохранением сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, predict_directory, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf87e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 600\n",
      "Total time: 45.61 sec\n",
      "Time to process 600 images: 45.61 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 1200\n",
      "Total time: 90.61 sec\n",
      "Time to process 600 images: 44.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 1800\n",
      "Total time: 135.49 sec\n",
      "Time to process 600 images: 44.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 2400\n",
      "Total time: 180.41 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 3000\n",
      "Total time: 225.76 sec\n",
      "Time to process 600 images: 45.35 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 3600\n",
      "Total time: 270.78 sec\n",
      "Time to process 600 images: 45.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 4200\n",
      "Total time: 316.20 sec\n",
      "Time to process 600 images: 45.42 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 4800\n",
      "Total time: 361.58 sec\n",
      "Time to process 600 images: 45.38 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 5400\n",
      "Total time: 406.52 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 6000\n",
      "Total time: 451.61 sec\n",
      "Time to process 600 images: 45.08 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 6600\n",
      "Total time: 496.42 sec\n",
      "Time to process 600 images: 44.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 7200\n",
      "Total time: 541.12 sec\n",
      "Time to process 600 images: 44.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 7800\n",
      "Total time: 585.83 sec\n",
      "Time to process 600 images: 44.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 8400\n",
      "Total time: 630.67 sec\n",
      "Time to process 600 images: 44.84 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 9000\n",
      "Total time: 675.69 sec\n",
      "Time to process 600 images: 45.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 9600\n",
      "Total time: 720.55 sec\n",
      "Time to process 600 images: 44.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 10200\n",
      "Total time: 765.59 sec\n",
      "Time to process 600 images: 45.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 10800\n",
      "Total time: 810.50 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 11400\n",
      "Total time: 855.36 sec\n",
      "Time to process 600 images: 44.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 12000\n",
      "Total time: 900.22 sec\n",
      "Time to process 600 images: 44.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 12600\n",
      "Total time: 945.05 sec\n",
      "Time to process 600 images: 44.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 13200\n",
      "Total time: 989.84 sec\n",
      "Time to process 600 images: 44.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 13800\n",
      "Total time: 1034.65 sec\n",
      "Time to process 600 images: 44.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 14400\n",
      "Total time: 1079.48 sec\n",
      "Time to process 600 images: 44.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 15000\n",
      "Total time: 1124.35 sec\n",
      "Time to process 600 images: 44.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 15600\n",
      "Total time: 1169.27 sec\n",
      "Time to process 600 images: 44.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 16200\n",
      "Total time: 1214.17 sec\n",
      "Time to process 600 images: 44.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 16800\n",
      "Total time: 1258.98 sec\n",
      "Time to process 600 images: 44.81 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 17400\n",
      "Total time: 1303.77 sec\n",
      "Time to process 600 images: 44.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 18000\n",
      "Total time: 1348.54 sec\n",
      "Time to process 600 images: 44.77 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 18600\n",
      "Total time: 1393.34 sec\n",
      "Time to process 600 images: 44.80 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 19200\n",
      "Total time: 1438.09 sec\n",
      "Time to process 600 images: 44.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 19800\n",
      "Total time: 1482.84 sec\n",
      "Time to process 600 images: 44.74 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 20400\n",
      "Total time: 1527.63 sec\n",
      "Time to process 600 images: 44.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 21000\n",
      "Total time: 1572.41 sec\n",
      "Time to process 600 images: 44.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 21600\n",
      "Total time: 1617.19 sec\n",
      "Time to process 600 images: 44.78 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 22200\n",
      "Total time: 1662.01 sec\n",
      "Time to process 600 images: 44.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 22800\n",
      "Total time: 1706.95 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 23400\n",
      "Total time: 1752.05 sec\n",
      "Time to process 600 images: 45.09 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 24000\n",
      "Total time: 1797.09 sec\n",
      "Time to process 600 images: 45.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 24600\n",
      "Total time: 1842.18 sec\n",
      "Time to process 600 images: 45.10 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 25200\n",
      "Total time: 1887.32 sec\n",
      "Time to process 600 images: 45.14 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 25800\n",
      "Total time: 1932.38 sec\n",
      "Time to process 600 images: 45.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 26400\n",
      "Total time: 1977.45 sec\n",
      "Time to process 600 images: 45.07 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 27000\n",
      "Total time: 2022.49 sec\n",
      "Time to process 600 images: 45.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 27600\n",
      "Total time: 2067.58 sec\n",
      "Time to process 600 images: 45.09 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 28200\n",
      "Total time: 2112.66 sec\n",
      "Time to process 600 images: 45.08 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 28800\n",
      "Total time: 2157.73 sec\n",
      "Time to process 600 images: 45.07 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 29400\n",
      "Total time: 2202.71 sec\n",
      "Time to process 600 images: 44.98 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 30000\n",
      "Total time: 2247.66 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 30600\n",
      "Total time: 2292.59 sec\n",
      "Time to process 600 images: 44.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 31200\n",
      "Total time: 2337.52 sec\n",
      "Time to process 600 images: 44.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 31800\n",
      "Total time: 2382.28 sec\n",
      "Time to process 600 images: 44.76 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 32400\n",
      "Total time: 2427.04 sec\n",
      "Time to process 600 images: 44.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 33000\n",
      "Total time: 2471.83 sec\n",
      "Time to process 600 images: 44.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 33600\n",
      "Total time: 2516.63 sec\n",
      "Time to process 600 images: 44.80 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 34200\n",
      "Total time: 2561.38 sec\n",
      "Time to process 600 images: 44.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 34800\n",
      "Total time: 2606.50 sec\n",
      "Time to process 600 images: 45.12 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 35400\n",
      "Total time: 2651.35 sec\n",
      "Time to process 600 images: 44.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 36000\n",
      "Total time: 2696.21 sec\n",
      "Time to process 600 images: 44.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 36600\n",
      "Total time: 2741.05 sec\n",
      "Time to process 600 images: 44.84 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 37200\n",
      "Total time: 2785.90 sec\n",
      "Time to process 600 images: 44.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 37800\n",
      "Total time: 2830.82 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 38400\n",
      "Total time: 2875.73 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 39000\n",
      "Total time: 2920.89 sec\n",
      "Time to process 600 images: 45.17 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 39600\n",
      "Total time: 2965.87 sec\n",
      "Time to process 600 images: 44.98 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 40200\n",
      "Total time: 3010.87 sec\n",
      "Time to process 600 images: 45.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 40800\n",
      "Total time: 3055.90 sec\n",
      "Time to process 600 images: 45.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 41400\n",
      "Total time: 3100.79 sec\n",
      "Time to process 600 images: 44.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 42000\n",
      "Total time: 3145.82 sec\n",
      "Time to process 600 images: 45.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 42600\n",
      "Total time: 3190.81 sec\n",
      "Time to process 600 images: 44.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 43200\n",
      "Total time: 3235.67 sec\n",
      "Time to process 600 images: 44.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 43800\n",
      "Total time: 3280.52 sec\n",
      "Time to process 600 images: 44.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 44400\n",
      "Total time: 3325.42 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 45000\n",
      "Total time: 3370.38 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 45600\n",
      "Total time: 3415.30 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 46200\n",
      "Total time: 3460.22 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 46800\n",
      "Total time: 3505.14 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 47400\n",
      "Total time: 3550.01 sec\n",
      "Time to process 600 images: 44.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 48000\n",
      "Total time: 3594.86 sec\n",
      "Time to process 600 images: 44.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 48600\n",
      "Total time: 3639.73 sec\n",
      "Time to process 600 images: 44.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 49200\n",
      "Total time: 3684.60 sec\n",
      "Time to process 600 images: 44.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 49800\n",
      "Total time: 3729.52 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 50400\n",
      "Total time: 3774.33 sec\n",
      "Time to process 600 images: 44.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 51000\n",
      "Total time: 3819.21 sec\n",
      "Time to process 600 images: 44.88 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 51600\n",
      "Total time: 3864.18 sec\n",
      "Time to process 600 images: 44.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 52200\n",
      "Total time: 3909.14 sec\n",
      "Time to process 600 images: 44.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 52800\n",
      "Total time: 3954.11 sec\n",
      "Time to process 600 images: 44.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 53400\n",
      "Total time: 3999.05 sec\n",
      "Time to process 600 images: 44.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 54000\n",
      "Total time: 4044.03 sec\n",
      "Time to process 600 images: 44.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 54600\n",
      "Total time: 4088.95 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 55200\n",
      "Total time: 4133.94 sec\n",
      "Time to process 600 images: 44.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 55800\n",
      "Total time: 4178.87 sec\n",
      "Time to process 600 images: 44.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 56400\n",
      "Total time: 4223.86 sec\n",
      "Time to process 600 images: 44.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 57000\n",
      "Total time: 4268.95 sec\n",
      "Time to process 600 images: 45.09 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 57600\n",
      "Total time: 4313.89 sec\n",
      "Time to process 600 images: 44.94 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 58200\n",
      "Total time: 4358.80 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 58800\n",
      "Total time: 4403.75 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 59400\n",
      "Total time: 4448.67 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 60000\n",
      "Total time: 4493.67 sec\n",
      "Time to process 600 images: 45.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 60600\n",
      "Total time: 4538.65 sec\n",
      "Time to process 600 images: 44.98 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 61200\n",
      "Total time: 4583.67 sec\n",
      "Time to process 600 images: 45.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 61800\n",
      "Total time: 4628.69 sec\n",
      "Time to process 600 images: 45.01 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 62400\n",
      "Total time: 4673.63 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 63000\n",
      "Total time: 4718.56 sec\n",
      "Time to process 600 images: 44.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 63600\n",
      "Total time: 4763.56 sec\n",
      "Time to process 600 images: 45.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 64200\n",
      "Total time: 4808.47 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 64800\n",
      "Total time: 4853.45 sec\n",
      "Time to process 600 images: 44.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 65400\n",
      "Total time: 4898.37 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 66000\n",
      "Total time: 4943.28 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 66600\n",
      "Total time: 4988.14 sec\n",
      "Time to process 600 images: 44.86 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 67200\n",
      "Total time: 5032.98 sec\n",
      "Time to process 600 images: 44.85 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 67800\n",
      "Total time: 5077.77 sec\n",
      "Time to process 600 images: 44.79 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 68400\n",
      "Total time: 5122.60 sec\n",
      "Time to process 600 images: 44.83 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 69000\n",
      "Total time: 5167.58 sec\n",
      "Time to process 600 images: 44.98 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 69600\n",
      "Total time: 5212.40 sec\n",
      "Time to process 600 images: 44.82 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 70200\n",
      "Total time: 5257.33 sec\n",
      "Time to process 600 images: 44.93 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 70800\n",
      "Total time: 5302.25 sec\n",
      "Time to process 600 images: 44.92 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 71400\n",
      "Total time: 5347.20 sec\n",
      "Time to process 600 images: 44.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 72000\n",
      "Total time: 5392.11 sec\n",
      "Time to process 600 images: 44.91 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 72600\n",
      "Total time: 5437.04 sec\n",
      "Time to process 600 images: 44.93 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processed images: 73200\n",
      "Total time: 5482.15 sec\n",
      "Time to process 600 images: 45.10 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 73800\n",
      "Total time: 5527.02 sec\n",
      "Time to process 600 images: 44.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 74400\n",
      "Total time: 5571.85 sec\n",
      "Time to process 600 images: 44.84 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 75000\n",
      "Total time: 5616.82 sec\n",
      "Time to process 600 images: 44.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 75600\n",
      "Total time: 5661.76 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 76200\n",
      "Total time: 5706.65 sec\n",
      "Time to process 600 images: 44.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 76800\n",
      "Total time: 5751.62 sec\n",
      "Time to process 600 images: 44.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 77400\n",
      "Total time: 5796.49 sec\n",
      "Time to process 600 images: 44.87 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 78000\n",
      "Total time: 5841.44 sec\n",
      "Time to process 600 images: 44.95 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 78600\n",
      "Total time: 5886.33 sec\n",
      "Time to process 600 images: 44.89 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 79200\n",
      "Total time: 5931.36 sec\n",
      "Time to process 600 images: 45.03 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 79800\n",
      "Total time: 5976.83 sec\n",
      "Time to process 600 images: 45.47 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 80400\n",
      "Total time: 6022.63 sec\n",
      "Time to process 600 images: 45.80 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 81000\n",
      "Total time: 6068.96 sec\n",
      "Time to process 600 images: 46.34 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 81600\n",
      "Total time: 6115.98 sec\n",
      "Time to process 600 images: 47.01 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 82200\n",
      "Total time: 6162.68 sec\n",
      "Time to process 600 images: 46.71 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 82800\n",
      "Total time: 6208.06 sec\n",
      "Time to process 600 images: 45.38 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 83400\n",
      "Total time: 6253.08 sec\n",
      "Time to process 600 images: 45.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 84000\n",
      "Total time: 6298.17 sec\n",
      "Time to process 600 images: 45.09 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 84600\n",
      "Total time: 6343.92 sec\n",
      "Time to process 600 images: 45.75 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 85200\n",
      "Total time: 6389.48 sec\n",
      "Time to process 600 images: 45.56 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 85800\n",
      "Total time: 6434.80 sec\n",
      "Time to process 600 images: 45.32 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 86400\n",
      "Total time: 6480.86 sec\n",
      "Time to process 600 images: 46.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 87000\n",
      "Total time: 6526.00 sec\n",
      "Time to process 600 images: 45.14 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 87600\n",
      "Total time: 6571.00 sec\n",
      "Time to process 600 images: 45.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 88200\n",
      "Total time: 6615.98 sec\n",
      "Time to process 600 images: 44.98 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 88800\n",
      "Total time: 6661.08 sec\n",
      "Time to process 600 images: 45.10 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 89400\n",
      "Total time: 6706.14 sec\n",
      "Time to process 600 images: 45.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 90000\n",
      "Total time: 6751.30 sec\n",
      "Time to process 600 images: 45.16 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 90600\n",
      "Total time: 6796.34 sec\n",
      "Time to process 600 images: 45.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 91200\n",
      "Total time: 6841.60 sec\n",
      "Time to process 600 images: 45.26 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 91800\n",
      "Total time: 6886.58 sec\n",
      "Time to process 600 images: 44.97 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 92400\n",
      "Total time: 6931.58 sec\n",
      "Time to process 600 images: 45.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 93000\n",
      "Total time: 6976.54 sec\n",
      "Time to process 600 images: 44.96 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 93600\n",
      "Total time: 7021.53 sec\n",
      "Time to process 600 images: 44.99 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 94200\n",
      "Total time: 7066.55 sec\n",
      "Time to process 600 images: 45.02 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 94800\n",
      "Total time: 7111.61 sec\n",
      "Time to process 600 images: 45.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 95400\n",
      "Total time: 7156.67 sec\n",
      "Time to process 600 images: 45.07 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 96000\n",
      "Total time: 7201.72 sec\n",
      "Time to process 600 images: 45.05 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 96600\n",
      "Total time: 7246.78 sec\n",
      "Time to process 600 images: 45.06 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 97200\n",
      "Total time: 7291.82 sec\n",
      "Time to process 600 images: 45.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 97800\n",
      "Total time: 7336.82 sec\n",
      "Time to process 600 images: 45.00 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 98400\n",
      "Total time: 7381.92 sec\n",
      "Time to process 600 images: 45.10 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 99000\n",
      "Total time: 7426.96 sec\n",
      "Time to process 600 images: 45.04 sec\n",
      "--------------------------------------------------\n",
      "Processed images: 99600\n",
      "Total time: 7471.93 sec\n",
      "Time to process 600 images: 44.97 sec\n"
     ]
    }
   ],
   "source": [
    "# Без сохранения сгенерированных масок в predict_directory\n",
    "rle_dataframe = my_model.predict(test_data_loader, \n",
    "                                 mask_treashold=mask_treashold, generate_rle_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ad9e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем датафрейм с результатом для заливки на kaggle\n",
    "rle_dataframe.to_csv('rle_dataframe.csv', index=True)\n",
    "sample_submission = pd.read_csv('/home/dima/carvana_dataset/sample_submission.csv')\n",
    "sample_submission = sample_submission.merge(rle_dataframe, how='left', left_on='img', right_on='img_name')\n",
    "sample_submission.drop(columns=['rle_mask', 'img_name'], inplace=True)\n",
    "sample_submission.rename(columns={'img_rle': 'rle_mask'}, inplace=True)\n",
    "sample_submission.to_csv('submission_07_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81effd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6cde352f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100064, 2)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d32e87",
   "metadata": {},
   "source": [
    "## Сравнение интерполяций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846bd59",
   "metadata": {},
   "source": [
    "### Попробовать bicubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ef5f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50fb86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28798169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ce9f5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024, 1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f36dfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1280, 1918])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eccfacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = my_model.model(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57f4ab1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024, 1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97808ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = F.interpolate(input=out, size=(1280, 1918), mode='nearest')\n",
    "output_bilinear = F.interpolate(input=out, size=(1280, 1918), mode='bilinear', align_corners=True)\n",
    "#output_linear = F.interpolate(input=out, size=(1280, 1918), mode='linear')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "311eb4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9945, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice = DiceMetric()\n",
    "dice_nearest = dice(output_nearest, input_tensor[2])\n",
    "dice_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83246bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9955, device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_bilinear = dice(output_bilinear, input_tensor[2])\n",
    "dice_bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdad3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.sigmoid(output_nearest)\n",
    "output_bilinear = torch.sigmoid(output_bilinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff4426c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = torch.where(output_nearest > 0.5, 1, 0)\n",
    "output_bilinear = torch.where(output_bilinear > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20797e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1280, 1918])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_nearest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14844c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_directory = '/home/dima/carvana_dataset/test/predict_small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74349240",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nearest = (output_nearest[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_nearest = Image.fromarray(output_nearest.astype('uint8'), 'L')\n",
    "output_nearest.save((predict_directory+'111').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e05a6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bilinear = (output_bilinear[0].cpu().numpy() * 255.0)[0] # [0] - избавляемся от батч размерности\n",
    "output_bilinear = Image.fromarray(output_bilinear.astype('uint8'), 'L')\n",
    "output_bilinear.save((predict_directory+'222').split('.')[0]+'.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33ccdecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3894c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
